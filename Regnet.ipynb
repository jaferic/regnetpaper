{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9688fa72-540f-4d55-85e4-4f38e56772f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f51a39ac-8589-4515-b8c4-8a159ce88e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegNetBlock(nn.Module):\n",
    "    expansion =1\n",
    "    def __init__(self, in_planes, out_planes, stride=1, groups=1, kernel_size=3, padding=1):\n",
    "        super(RegNetBlock, self).__init__()\n",
    "        self.groups = min(in_planes, groups, out_planes)  # Adjust groups to not exceed in_planes\n",
    "       # print(\"group number:\",groups)\n",
    "        while in_planes % groups != 0 or out_planes % groups != 0:\n",
    "            groups //= 2  # Reduce groups to ensure divisibility\n",
    "       # if in_planes % groups !=0:\n",
    "        #groups = in_planes\n",
    "        if in_planes > 0:\n",
    "            # Example condition: adjust kernel size for small inputs\n",
    "            if in_planes <= 32:\n",
    "                kernel_size = 3\n",
    "                padding = 1\n",
    "            else:\n",
    "                kernel_size = 7\n",
    "                padding = 3\n",
    "        else:\n",
    "            # Default to a standard kernel size if no input size info is available\n",
    "            kernel_size = 3\n",
    "            padding = 1\n",
    "        #print(f\"Config regnetblock: out_planes={out_planes}, stride={stride}, groups={groups}, kernel_size={kernel_size}, padding={padding}\")\n",
    "        groups = int(groups)\n",
    "        #print(f\"groups_regblock={groups}\")\n",
    "        #print(f\"Out_planes_regnetblock={out_planes}\")\n",
    "\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size,stride=stride, padding=padding, groups=groups, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * out_planes:\n",
    "           # print(\"SHORTCUT DETECTED!!!!!!!!!!\")\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * out_planes, kernel_size=1, groups=groups, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * out_planes),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.relu(self.bn(self.conv(x)))\n",
    "        residual = self.shortcut(residual)\n",
    "        if out.shape != residual.shape:\n",
    "            out = F.interpolate(out, size=residual.shape[2:], mode='nearest')\n",
    "        #out += self.shortcut(x)\n",
    "        out = out + residual\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e20d7366-9025-4494-ae6c-38f648395f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnyNetX(nn.Module):\n",
    "    def __init__(self, config, num_classes=10):\n",
    "        super(AnyNetX, self).__init__()\n",
    "        self.in_planes = 64\n",
    "        input_channels = 1\n",
    "        out_planes = config[0][0]\n",
    "        print(f\"Configanynetx initial: out_planes={out_planes}, inplane={self.in_planes}\")\n",
    "\n",
    "        # Initial conv layer\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, self.in_planes, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(self.in_planes),\n",
    "            nn.ReLU(inplace=True)        \n",
    "        )\n",
    "\n",
    "        self.layers = self._make_layers(config)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(self.in_planes, num_classes)\n",
    "        self.in_planes = out_planes\n",
    "\n",
    "\n",
    "    def _make_layers(self, config):\n",
    "        layers = []\n",
    "        print(config)\n",
    "        for idx,(out_planes, num_blocks, stride, groups, kernel_size, padding) in enumerate(config):\n",
    "            #print(f\"Configanynetx: in_planes={self.in_planes}, out_planes={out_planes}, num_blocks={num_blocks}, stride={stride}, groups={groups}, kernel_size={kernel_size}, padding={padding}\")\n",
    "            #print(f\"Initial in_planes={self.in_planes}\")\n",
    "            blocks = nn.Sequential()\n",
    "            for b in range(num_blocks):\n",
    "                #print(f\"b is: {b}\")\n",
    "                #print(f\"  Block: in_planes={self.in_planes}, out_planes={out_planes}, groups={groups}\")\n",
    "                block = RegNetBlock(in_planes=self.in_planes,out_planes=out_planes, stride=stride, groups=groups, kernel_size=kernel_size, padding=padding)\n",
    "                blocks.add_module(f\"block_{idx}_{b}\", block)\n",
    "               # if b == 0:\n",
    "                self.in_planes =out_planes# Update in_planes for the next layer\n",
    "            \n",
    "            #else:self.in_planes = groups\n",
    "                #print(f\"  Updated in_planes={self.in_planes}\")\n",
    "\n",
    "                #stride = 1  # Only the first block may have a stride > 1\n",
    "            layers.append(blocks)\n",
    "          #  print(blocks)\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "       # print(nn.Sequential)\n",
    "        x = self.initial(x)\n",
    "        x = self.layers(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64a07a20-6d8b-48e5-8865-a35eecbb1cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e391ad5c-9601-49d3-8fbd-fa7ba237ae99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_configuration():\n",
    "    depth_range = [2, 6, 12]\n",
    "   # width_range = (64, 512)\n",
    "   # group_width_range = (1, 64)\n",
    "    width_multiplier_range = [1, 64, 128]  # Changed to a multiplier range\n",
    "    group_width_choices = np.array([1, 2, 4, 8, 16, 32])\n",
    "    kernel_size_choices = [3, 5, 7]\n",
    "    stride_choices = [1, 2,3,4]\n",
    "    padding_choices = [1, 2,3,4]\n",
    "\n",
    "    num_stages = 4\n",
    "    config = []\n",
    "    #config.append((64,1,1,1,1,1))\n",
    "    for _ in range(num_stages):\n",
    "        depth = np.random.choice(depth_range)\n",
    "        group_width = np.random.choice(group_width_choices)\n",
    "        width_multiplier = np.random.choice(width_multiplier_range)\n",
    "        out_planes = group_width * width_multiplier\n",
    "        kernel_size = np.random.choice(kernel_size_choices)\n",
    "        stride = np.random.choice(stride_choices)\n",
    "        padding = np.random.choice(padding_choices)\n",
    "        # Ensure group width is a divisor of width\n",
    "       # group_width = width if group_width > width else group_width\n",
    "        #if out_planes % group_width != 0:\n",
    "         #   out_planes = (out_planes // group_width) * group_width\n",
    "        config.append((out_planes,depth,stride, group_width, kernel_size, padding))\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27d32ce4-1102-4cfe-90fe-ec47cd1defac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model):\n",
    "    # Example dataset and dataloader setup\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Lambda(lambda x: x.expand(-1, 3, -1, -1)),  # Expand single-channel images to three channels\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "    # Load training and validation datasets\n",
    "    trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    valset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "    valloader = DataLoader(valset, batch_size=64, shuffle=False)\n",
    "\n",
    "    # Example loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    epochs = 10\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for images, labels in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(images)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(trainloader)}\")\n",
    "\n",
    "    # Evaluate accuracy on the validation dataset\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in valloader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy on the validation set: {accuracy}%')\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90963659-0ebe-4d91-a6f4-758a11b6a83e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec91af66-d006-4b40-9c05-59ac6dddf947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configanynetx initial: out_planes=8, inplane=64\n",
      "[(8, 12, 3, 8, 5, 2), (256, 2, 1, 2, 3, 3), (256, 2, 2, 2, 3, 1), (512, 6, 2, 8, 3, 2)]\n",
      "Epoch 1, Loss: 1.8438198982017127\n",
      "Epoch 2, Loss: 1.6367234592752924\n",
      "Epoch 3, Loss: 1.5725938144014842\n",
      "Epoch 4, Loss: 1.5523676764227943\n",
      "Epoch 5, Loss: 1.5432295442135857\n",
      "Epoch 6, Loss: 1.4766400476762735\n",
      "Epoch 7, Loss: 1.505238528953178\n",
      "Epoch 8, Loss: 1.5060469232388396\n",
      "Epoch 9, Loss: 1.47072295860441\n",
      "Epoch 10, Loss: 1.461047604012845\n",
      "Accuracy on the validation set: 25.35%\n",
      "Configanynetx initial: out_planes=256, inplane=64\n",
      "[(256, 12, 4, 2, 3, 3), (512, 6, 1, 4, 7, 2), (4096, 2, 4, 32, 5, 2), (256, 6, 2, 4, 7, 3)]\n",
      "Epoch 1, Loss: 0.5162335887773714\n",
      "Epoch 2, Loss: 0.2195756041260162\n",
      "Epoch 3, Loss: 0.1781825624537398\n",
      "Epoch 4, Loss: 0.16247200904223424\n",
      "Epoch 5, Loss: 0.127944096993369\n",
      "Epoch 6, Loss: 0.10994036703793479\n",
      "Epoch 7, Loss: 0.09915870188825777\n",
      "Epoch 8, Loss: 0.07938810202158066\n",
      "Epoch 9, Loss: 0.07631755222666707\n",
      "Epoch 10, Loss: 0.07423029947955943\n",
      "Accuracy on the validation set: 96.42%\n",
      "Configanynetx initial: out_planes=256, inplane=64\n",
      "[(256, 6, 3, 4, 3, 3), (16, 6, 4, 16, 3, 2), (2048, 2, 1, 16, 3, 1), (64, 12, 1, 1, 3, 4)]\n",
      "Epoch 1, Loss: 1.03882980076616\n",
      "Epoch 2, Loss: 0.5478769682331889\n",
      "Epoch 3, Loss: 0.33063775326794526\n",
      "Epoch 4, Loss: 0.27204709120396614\n",
      "Epoch 5, Loss: 0.21535545659622848\n",
      "Epoch 6, Loss: 0.17097156394972032\n",
      "Epoch 7, Loss: 0.14816507046271774\n",
      "Epoch 8, Loss: 0.13894903142231582\n",
      "Epoch 9, Loss: 0.10743007211358166\n",
      "Epoch 10, Loss: 0.09176359598218069\n",
      "Accuracy on the validation set: 89.26%\n",
      "Configanynetx initial: out_planes=1024, inplane=64\n",
      "[(1024, 6, 3, 16, 5, 1), (1024, 2, 3, 16, 5, 1), (128, 12, 2, 2, 5, 4), (32, 2, 4, 32, 3, 4)]\n",
      "Epoch 1, Loss: 0.6804978425568863\n",
      "Epoch 2, Loss: 0.2711146637987989\n",
      "Epoch 3, Loss: 0.20230932059739506\n",
      "Epoch 4, Loss: 0.16312773374399778\n",
      "Epoch 5, Loss: 0.13908566786909599\n",
      "Epoch 6, Loss: 0.13141084498842992\n",
      "Epoch 7, Loss: 0.11248698289583582\n",
      "Epoch 8, Loss: 0.10796771452341204\n",
      "Epoch 9, Loss: 0.09925091206264902\n",
      "Epoch 10, Loss: 0.08296882544707722\n",
      "Accuracy on the validation set: 97.42%\n",
      "Configanynetx initial: out_planes=128, inplane=64\n",
      "[(128, 12, 4, 2, 5, 1), (256, 6, 2, 4, 7, 3), (4, 6, 3, 4, 5, 4), (4096, 6, 3, 32, 3, 1)]\n",
      "Epoch 1, Loss: 2.4378725175918547\n",
      "Epoch 2, Loss: 2.2947950848638374\n",
      "Epoch 3, Loss: 2.212741058645472\n",
      "Epoch 4, Loss: 2.1981733236739887\n",
      "Epoch 5, Loss: 2.2136397853589007\n",
      "Epoch 6, Loss: 2.0309473170654604\n",
      "Epoch 7, Loss: 2.110254061755849\n",
      "Epoch 8, Loss: 2.0750951511519298\n",
      "Epoch 9, Loss: 1.9538516398431904\n",
      "Epoch 10, Loss: 2.1393239425697814\n",
      "Accuracy on the validation set: 10.09%\n",
      "Configanynetx initial: out_planes=4096, inplane=64\n",
      "[(4096, 12, 2, 32, 7, 1), (2048, 12, 3, 32, 7, 3), (2048, 2, 3, 32, 7, 1), (2048, 6, 3, 16, 5, 3)]\n",
      "Epoch 1, Loss: 0.7408722658623764\n",
      "Epoch 2, Loss: 0.47502911638126954\n",
      "Epoch 3, Loss: 0.332653161039008\n",
      "Epoch 4, Loss: 0.24031306070480138\n",
      "Epoch 5, Loss: 0.3003436012157817\n",
      "Epoch 6, Loss: 0.2453443823175739\n",
      "Epoch 7, Loss: 0.18372872343529134\n",
      "Epoch 8, Loss: 0.15669624875786145\n",
      "Epoch 9, Loss: 0.17489732027069718\n",
      "Epoch 10, Loss: 0.15029058489364697\n",
      "Accuracy on the validation set: 41.73%\n",
      "Configanynetx initial: out_planes=4096, inplane=64\n",
      "[(4096, 6, 2, 32, 7, 3), (4, 6, 2, 4, 7, 3), (256, 12, 4, 2, 7, 4), (32, 2, 3, 32, 3, 2)]\n",
      "Epoch 1, Loss: 1.9266660487982257\n",
      "Epoch 2, Loss: 1.6911429046059467\n",
      "Epoch 3, Loss: 1.6244760585237923\n",
      "Epoch 4, Loss: 1.6222733423145594\n",
      "Epoch 5, Loss: 1.549925317642277\n",
      "Epoch 6, Loss: 1.4014033917933384\n",
      "Epoch 7, Loss: 1.382934540446633\n",
      "Epoch 8, Loss: 1.255161099342395\n",
      "Epoch 9, Loss: 1.070022477079302\n",
      "Epoch 10, Loss: 1.0460246042020793\n",
      "Accuracy on the validation set: 46.48%\n",
      "Configanynetx initial: out_planes=2048, inplane=64\n",
      "[(2048, 2, 3, 16, 5, 4), (512, 6, 3, 4, 3, 4), (16, 12, 4, 16, 5, 2), (128, 2, 3, 1, 3, 4)]\n",
      "Epoch 1, Loss: 1.8533755829339342\n",
      "Epoch 2, Loss: 1.7703982983062516\n",
      "Epoch 3, Loss: 1.5925106984465869\n",
      "Epoch 4, Loss: 1.5361520232422265\n",
      "Epoch 5, Loss: 1.6909855113609005\n",
      "Epoch 6, Loss: 1.7015206393148345\n",
      "Epoch 7, Loss: 1.6162472254177656\n",
      "Epoch 8, Loss: 1.5932161201800366\n",
      "Epoch 9, Loss: 1.5046882126122905\n",
      "Epoch 10, Loss: 1.4816053510983107\n",
      "Accuracy on the validation set: 22.23%\n",
      "Configanynetx initial: out_planes=4096, inplane=64\n",
      "[(4096, 2, 4, 32, 7, 3), (64, 6, 4, 1, 5, 4), (1024, 12, 2, 16, 3, 4), (512, 12, 4, 4, 3, 3)]\n",
      "Epoch 1, Loss: 0.9435588223402942\n",
      "Epoch 2, Loss: 0.43005707024384154\n",
      "Epoch 3, Loss: 0.36790487483969886\n",
      "Epoch 4, Loss: 0.22444871899042365\n",
      "Epoch 5, Loss: 0.2143957004058304\n",
      "Epoch 6, Loss: 0.15215176387257112\n",
      "Epoch 7, Loss: 0.2606620467625368\n",
      "Epoch 8, Loss: 0.15589167677096402\n",
      "Epoch 9, Loss: 0.15738231050215964\n",
      "Epoch 10, Loss: 0.10966411055967228\n",
      "Accuracy on the validation set: 95.88%\n",
      "Configanynetx initial: out_planes=256, inplane=64\n",
      "[(256, 6, 4, 4, 5, 4), (32, 12, 2, 32, 5, 3), (8, 6, 3, 8, 3, 1), (1024, 6, 4, 16, 7, 3)]\n",
      "Epoch 1, Loss: 2.159874089109872\n",
      "Epoch 2, Loss: 1.9813105219971143\n",
      "Epoch 3, Loss: 1.934917029287261\n",
      "Epoch 4, Loss: 1.8711124081601467\n",
      "Epoch 5, Loss: 1.8619194460321846\n",
      "Epoch 6, Loss: 1.9185167992038767\n",
      "Epoch 7, Loss: 1.9020171283658889\n",
      "Epoch 8, Loss: 1.8654658558017918\n",
      "Epoch 9, Loss: 1.8416653621171328\n",
      "Epoch 10, Loss: 1.869453233950682\n",
      "Accuracy on the validation set: 26.05%\n"
     ]
    }
   ],
   "source": [
    "number_of_samples=10\n",
    "data = []\n",
    "for _ in range(number_of_samples):\n",
    "    config = sample_configuration()\n",
    "    model = AnyNetX(config)\n",
    "    accuracy = train_and_evaluate(model)  # This function is to be defined based on your dataset and training loop\n",
    "    data.append((config, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e92548b1-13f7-49e6-8b80-707687431d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([(8, 12, 3, 8, 5, 2), (256, 2, 1, 2, 3, 3), (256, 2, 2, 2, 3, 1), (512, 6, 2, 8, 3, 2)], 25.35), ([(256, 12, 4, 2, 3, 3), (512, 6, 1, 4, 7, 2), (4096, 2, 4, 32, 5, 2), (256, 6, 2, 4, 7, 3)], 96.42), ([(256, 6, 3, 4, 3, 3), (16, 6, 4, 16, 3, 2), (2048, 2, 1, 16, 3, 1), (64, 12, 1, 1, 3, 4)], 89.26), ([(1024, 6, 3, 16, 5, 1), (1024, 2, 3, 16, 5, 1), (128, 12, 2, 2, 5, 4), (32, 2, 4, 32, 3, 4)], 97.42), ([(128, 12, 4, 2, 5, 1), (256, 6, 2, 4, 7, 3), (4, 6, 3, 4, 5, 4), (4096, 6, 3, 32, 3, 1)], 10.09), ([(4096, 12, 2, 32, 7, 1), (2048, 12, 3, 32, 7, 3), (2048, 2, 3, 32, 7, 1), (2048, 6, 3, 16, 5, 3)], 41.73), ([(4096, 6, 2, 32, 7, 3), (4, 6, 2, 4, 7, 3), (256, 12, 4, 2, 7, 4), (32, 2, 3, 32, 3, 2)], 46.48), ([(2048, 2, 3, 16, 5, 4), (512, 6, 3, 4, 3, 4), (16, 12, 4, 16, 5, 2), (128, 2, 3, 1, 3, 4)], 22.23), ([(4096, 2, 4, 32, 7, 3), (64, 6, 4, 1, 5, 4), (1024, 12, 2, 16, 3, 4), (512, 12, 4, 4, 3, 3)], 95.88), ([(256, 6, 4, 4, 5, 4), (32, 12, 2, 32, 5, 3), (8, 6, 3, 8, 3, 1), (1024, 6, 4, 16, 7, 3)], 26.05)]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "892be179-3e38-4f98-b35b-a2cd821a8324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[25.3500],\n",
      "        [96.4200],\n",
      "        [89.2600],\n",
      "        [97.4200],\n",
      "        [10.0900],\n",
      "        [41.7300],\n",
      "        [46.4800],\n",
      "        [22.2300],\n",
      "        [95.8800],\n",
      "        [26.0500]])\n",
      "tensor([[8.0000e+00, 1.2000e+01, 3.0000e+00, 8.0000e+00, 5.0000e+00, 2.0000e+00,\n",
      "         2.5600e+02, 2.0000e+00, 1.0000e+00, 2.0000e+00, 3.0000e+00, 3.0000e+00,\n",
      "         2.5600e+02, 2.0000e+00, 2.0000e+00, 2.0000e+00, 3.0000e+00, 1.0000e+00,\n",
      "         5.1200e+02, 6.0000e+00, 2.0000e+00, 8.0000e+00, 3.0000e+00, 2.0000e+00],\n",
      "        [2.5600e+02, 1.2000e+01, 4.0000e+00, 2.0000e+00, 3.0000e+00, 3.0000e+00,\n",
      "         5.1200e+02, 6.0000e+00, 1.0000e+00, 4.0000e+00, 7.0000e+00, 2.0000e+00,\n",
      "         4.0960e+03, 2.0000e+00, 4.0000e+00, 3.2000e+01, 5.0000e+00, 2.0000e+00,\n",
      "         2.5600e+02, 6.0000e+00, 2.0000e+00, 4.0000e+00, 7.0000e+00, 3.0000e+00],\n",
      "        [2.5600e+02, 6.0000e+00, 3.0000e+00, 4.0000e+00, 3.0000e+00, 3.0000e+00,\n",
      "         1.6000e+01, 6.0000e+00, 4.0000e+00, 1.6000e+01, 3.0000e+00, 2.0000e+00,\n",
      "         2.0480e+03, 2.0000e+00, 1.0000e+00, 1.6000e+01, 3.0000e+00, 1.0000e+00,\n",
      "         6.4000e+01, 1.2000e+01, 1.0000e+00, 1.0000e+00, 3.0000e+00, 4.0000e+00],\n",
      "        [1.0240e+03, 6.0000e+00, 3.0000e+00, 1.6000e+01, 5.0000e+00, 1.0000e+00,\n",
      "         1.0240e+03, 2.0000e+00, 3.0000e+00, 1.6000e+01, 5.0000e+00, 1.0000e+00,\n",
      "         1.2800e+02, 1.2000e+01, 2.0000e+00, 2.0000e+00, 5.0000e+00, 4.0000e+00,\n",
      "         3.2000e+01, 2.0000e+00, 4.0000e+00, 3.2000e+01, 3.0000e+00, 4.0000e+00],\n",
      "        [1.2800e+02, 1.2000e+01, 4.0000e+00, 2.0000e+00, 5.0000e+00, 1.0000e+00,\n",
      "         2.5600e+02, 6.0000e+00, 2.0000e+00, 4.0000e+00, 7.0000e+00, 3.0000e+00,\n",
      "         4.0000e+00, 6.0000e+00, 3.0000e+00, 4.0000e+00, 5.0000e+00, 4.0000e+00,\n",
      "         4.0960e+03, 6.0000e+00, 3.0000e+00, 3.2000e+01, 3.0000e+00, 1.0000e+00],\n",
      "        [4.0960e+03, 1.2000e+01, 2.0000e+00, 3.2000e+01, 7.0000e+00, 1.0000e+00,\n",
      "         2.0480e+03, 1.2000e+01, 3.0000e+00, 3.2000e+01, 7.0000e+00, 3.0000e+00,\n",
      "         2.0480e+03, 2.0000e+00, 3.0000e+00, 3.2000e+01, 7.0000e+00, 1.0000e+00,\n",
      "         2.0480e+03, 6.0000e+00, 3.0000e+00, 1.6000e+01, 5.0000e+00, 3.0000e+00],\n",
      "        [4.0960e+03, 6.0000e+00, 2.0000e+00, 3.2000e+01, 7.0000e+00, 3.0000e+00,\n",
      "         4.0000e+00, 6.0000e+00, 2.0000e+00, 4.0000e+00, 7.0000e+00, 3.0000e+00,\n",
      "         2.5600e+02, 1.2000e+01, 4.0000e+00, 2.0000e+00, 7.0000e+00, 4.0000e+00,\n",
      "         3.2000e+01, 2.0000e+00, 3.0000e+00, 3.2000e+01, 3.0000e+00, 2.0000e+00],\n",
      "        [2.0480e+03, 2.0000e+00, 3.0000e+00, 1.6000e+01, 5.0000e+00, 4.0000e+00,\n",
      "         5.1200e+02, 6.0000e+00, 3.0000e+00, 4.0000e+00, 3.0000e+00, 4.0000e+00,\n",
      "         1.6000e+01, 1.2000e+01, 4.0000e+00, 1.6000e+01, 5.0000e+00, 2.0000e+00,\n",
      "         1.2800e+02, 2.0000e+00, 3.0000e+00, 1.0000e+00, 3.0000e+00, 4.0000e+00],\n",
      "        [4.0960e+03, 2.0000e+00, 4.0000e+00, 3.2000e+01, 7.0000e+00, 3.0000e+00,\n",
      "         6.4000e+01, 6.0000e+00, 4.0000e+00, 1.0000e+00, 5.0000e+00, 4.0000e+00,\n",
      "         1.0240e+03, 1.2000e+01, 2.0000e+00, 1.6000e+01, 3.0000e+00, 4.0000e+00,\n",
      "         5.1200e+02, 1.2000e+01, 4.0000e+00, 4.0000e+00, 3.0000e+00, 3.0000e+00],\n",
      "        [2.5600e+02, 6.0000e+00, 4.0000e+00, 4.0000e+00, 5.0000e+00, 4.0000e+00,\n",
      "         3.2000e+01, 1.2000e+01, 2.0000e+00, 3.2000e+01, 5.0000e+00, 3.0000e+00,\n",
      "         8.0000e+00, 6.0000e+00, 3.0000e+00, 8.0000e+00, 3.0000e+00, 1.0000e+00,\n",
      "         1.0240e+03, 6.0000e+00, 4.0000e+00, 1.6000e+01, 7.0000e+00, 3.0000e+00]])\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "def config_to_features(config):\n",
    "    # Flatten the configuration tuples into a single list of features\n",
    "    features = [feature for stage in config for feature in stage]\n",
    "    return features\n",
    "\n",
    "X = torch.tensor([config_to_features(config) for config, _ in data], dtype=torch.float32)\n",
    "y = torch.tensor([[accuracy] for _, accuracy in data], dtype=torch.float32)  # Ensure y is 2D\n",
    "print(y)\n",
    "print(X)\n",
    "\n",
    "input_size = len(config_to_features(data[0][0]))  # Number of features\n",
    "print(input_size)\n",
    "output_size = 1  # Predicting a single value, e.g., accuracy\n",
    "\n",
    "model = LinearRegressionModel(input_size, output_size)\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay =1e-5)  # ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97883b26-07a2-41db-a826-58411d4e00dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lqm(model, X, y):\n",
    "    epochs = 30000  # Number of epochs to train for\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss = (loss ** 2).mean()\n",
    "\n",
    "        \n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch+1) % 10 == 0:  # Print loss every 100 epochs\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "966186bd-0c98-45b1-a33c-b35d2a753ad8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m predicted_accuracies \u001b[38;5;241m=\u001b[39m [outputs\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m config, _ \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(predicted_accuracies)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(y)\n",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m predicted_accuracies \u001b[38;5;241m=\u001b[39m [\u001b[43moutputs\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m config, _ \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(predicted_accuracies)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(y)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'outputs' is not defined"
     ]
    }
   ],
   "source": [
    "predicted_accuracies = [outputs.detach().numpy() for config, _ in data]\n",
    "print(predicted_accuracies)\n",
    "print(y)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y, predicted_accuracies, alpha=0.5)\n",
    "plt.title('Actual vs. Predicted Accuracies')\n",
    "plt.xlabel('Actual Accuracy')\n",
    "plt.ylabel('Predicted Accuracy')\n",
    "plt.grid(True)\n",
    "\n",
    "# Optional: plot a line representing perfect predictions for reference\n",
    "#plt.plot([min(actual_accuracies), max(actual_accuracies)], [min(actual_accuracies), max(actual_accuracies)], 'r--')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1547922f-327e-422b-a4f5-0e2846b35ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_performance(model, config):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        features = torch.tensor([config_to_features(config)], dtype=torch.float32)\n",
    "        prediction = model(features)\n",
    "    return prediction.item()  # Return the predicted value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0bfeab0-d959-4e7e-91c9-f468273a3be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/30000], Loss: 1936649.0000\n",
      "Epoch [20/30000], Loss: 1927850.8750\n",
      "Epoch [30/30000], Loss: 1919132.3750\n",
      "Epoch [40/30000], Loss: 1910490.8750\n",
      "Epoch [50/30000], Loss: 1901927.1250\n",
      "Epoch [60/30000], Loss: 1893438.1250\n",
      "Epoch [70/30000], Loss: 1885024.3750\n",
      "Epoch [80/30000], Loss: 1876682.6250\n",
      "Epoch [90/30000], Loss: 1868413.3750\n",
      "Epoch [100/30000], Loss: 1860215.6250\n",
      "Epoch [110/30000], Loss: 1852088.3750\n",
      "Epoch [120/30000], Loss: 1844029.7500\n",
      "Epoch [130/30000], Loss: 1836039.2500\n",
      "Epoch [140/30000], Loss: 1828115.8750\n",
      "Epoch [150/30000], Loss: 1820257.5000\n",
      "Epoch [160/30000], Loss: 1812466.0000\n",
      "Epoch [170/30000], Loss: 1804738.3750\n",
      "Epoch [180/30000], Loss: 1797073.5000\n",
      "Epoch [190/30000], Loss: 1789471.5000\n",
      "Epoch [200/30000], Loss: 1781931.0000\n",
      "Epoch [210/30000], Loss: 1774451.3750\n",
      "Epoch [220/30000], Loss: 1767031.5000\n",
      "Epoch [230/30000], Loss: 1759670.6250\n",
      "Epoch [240/30000], Loss: 1752368.0000\n",
      "Epoch [250/30000], Loss: 1745123.2500\n",
      "Epoch [260/30000], Loss: 1737935.6250\n",
      "Epoch [270/30000], Loss: 1730803.2500\n",
      "Epoch [280/30000], Loss: 1723726.0000\n",
      "Epoch [290/30000], Loss: 1716703.2500\n",
      "Epoch [300/30000], Loss: 1709734.7500\n",
      "Epoch [310/30000], Loss: 1702818.8750\n",
      "Epoch [320/30000], Loss: 1695956.1250\n",
      "Epoch [330/30000], Loss: 1689144.5000\n",
      "Epoch [340/30000], Loss: 1682383.7500\n",
      "Epoch [350/30000], Loss: 1675674.0000\n",
      "Epoch [360/30000], Loss: 1669013.2500\n",
      "Epoch [370/30000], Loss: 1662401.6250\n",
      "Epoch [380/30000], Loss: 1655839.2500\n",
      "Epoch [390/30000], Loss: 1649324.7500\n",
      "Epoch [400/30000], Loss: 1642856.7500\n",
      "Epoch [410/30000], Loss: 1636435.8750\n",
      "Epoch [420/30000], Loss: 1630061.6250\n",
      "Epoch [430/30000], Loss: 1623732.3750\n",
      "Epoch [440/30000], Loss: 1617448.0000\n",
      "Epoch [450/30000], Loss: 1611208.7500\n",
      "Epoch [460/30000], Loss: 1605013.3750\n",
      "Epoch [470/30000], Loss: 1598860.8750\n",
      "Epoch [480/30000], Loss: 1592752.5000\n",
      "Epoch [490/30000], Loss: 1586685.2500\n",
      "Epoch [500/30000], Loss: 1580661.0000\n",
      "Epoch [510/30000], Loss: 1574677.6250\n",
      "Epoch [520/30000], Loss: 1568735.5000\n",
      "Epoch [530/30000], Loss: 1562833.8750\n",
      "Epoch [540/30000], Loss: 1556972.3750\n",
      "Epoch [550/30000], Loss: 1551150.7500\n",
      "Epoch [560/30000], Loss: 1545368.0000\n",
      "Epoch [570/30000], Loss: 1539624.1250\n",
      "Epoch [580/30000], Loss: 1533918.5000\n",
      "Epoch [590/30000], Loss: 1528251.2500\n",
      "Epoch [600/30000], Loss: 1522621.0000\n",
      "Epoch [610/30000], Loss: 1517028.2500\n",
      "Epoch [620/30000], Loss: 1511472.5000\n",
      "Epoch [630/30000], Loss: 1505952.8750\n",
      "Epoch [640/30000], Loss: 1500468.2500\n",
      "Epoch [650/30000], Loss: 1495020.2500\n",
      "Epoch [660/30000], Loss: 1489607.7500\n",
      "Epoch [670/30000], Loss: 1484229.3750\n",
      "Epoch [680/30000], Loss: 1478885.7500\n",
      "Epoch [690/30000], Loss: 1473577.0000\n",
      "Epoch [700/30000], Loss: 1468301.3750\n",
      "Epoch [710/30000], Loss: 1463059.7500\n",
      "Epoch [720/30000], Loss: 1457850.5000\n",
      "Epoch [730/30000], Loss: 1452674.8750\n",
      "Epoch [740/30000], Loss: 1447530.8750\n",
      "Epoch [750/30000], Loss: 1442420.3750\n",
      "Epoch [760/30000], Loss: 1437341.1250\n",
      "Epoch [770/30000], Loss: 1432293.5000\n",
      "Epoch [780/30000], Loss: 1427277.3750\n",
      "Epoch [790/30000], Loss: 1422292.1250\n",
      "Epoch [800/30000], Loss: 1417337.0000\n",
      "Epoch [810/30000], Loss: 1412413.2500\n",
      "Epoch [820/30000], Loss: 1407519.2500\n",
      "Epoch [830/30000], Loss: 1402654.7500\n",
      "Epoch [840/30000], Loss: 1397820.1250\n",
      "Epoch [850/30000], Loss: 1393015.7500\n",
      "Epoch [860/30000], Loss: 1388239.3750\n",
      "Epoch [870/30000], Loss: 1383492.2500\n",
      "Epoch [880/30000], Loss: 1378773.6250\n",
      "Epoch [890/30000], Loss: 1374083.6250\n",
      "Epoch [900/30000], Loss: 1369421.3750\n",
      "Epoch [910/30000], Loss: 1364787.2500\n",
      "Epoch [920/30000], Loss: 1360180.3750\n",
      "Epoch [930/30000], Loss: 1355601.3750\n",
      "Epoch [940/30000], Loss: 1351049.2500\n",
      "Epoch [950/30000], Loss: 1346523.6250\n",
      "Epoch [960/30000], Loss: 1342025.5000\n",
      "Epoch [970/30000], Loss: 1337553.7500\n",
      "Epoch [980/30000], Loss: 1333107.8750\n",
      "Epoch [990/30000], Loss: 1328688.2500\n",
      "Epoch [1000/30000], Loss: 1324294.6250\n",
      "Epoch [1010/30000], Loss: 1319926.1250\n",
      "Epoch [1020/30000], Loss: 1315583.7500\n",
      "Epoch [1030/30000], Loss: 1311266.5000\n",
      "Epoch [1040/30000], Loss: 1306973.6250\n",
      "Epoch [1050/30000], Loss: 1302706.0000\n",
      "Epoch [1060/30000], Loss: 1298463.0000\n",
      "Epoch [1070/30000], Loss: 1294244.7500\n",
      "Epoch [1080/30000], Loss: 1290050.3750\n",
      "Epoch [1090/30000], Loss: 1285880.1250\n",
      "Epoch [1100/30000], Loss: 1281733.8750\n",
      "Epoch [1110/30000], Loss: 1277611.3750\n",
      "Epoch [1120/30000], Loss: 1273512.0000\n",
      "Epoch [1130/30000], Loss: 1269436.0000\n",
      "Epoch [1140/30000], Loss: 1265383.8750\n",
      "Epoch [1150/30000], Loss: 1261354.1250\n",
      "Epoch [1160/30000], Loss: 1257347.5000\n",
      "Epoch [1170/30000], Loss: 1253363.6250\n",
      "Epoch [1180/30000], Loss: 1249401.6250\n",
      "Epoch [1190/30000], Loss: 1245462.5000\n",
      "Epoch [1200/30000], Loss: 1241545.1250\n",
      "Epoch [1210/30000], Loss: 1237650.0000\n",
      "Epoch [1220/30000], Loss: 1233776.3750\n",
      "Epoch [1230/30000], Loss: 1229924.8750\n",
      "Epoch [1240/30000], Loss: 1226095.2500\n",
      "Epoch [1250/30000], Loss: 1222286.0000\n",
      "Epoch [1260/30000], Loss: 1218498.5000\n",
      "Epoch [1270/30000], Loss: 1214732.5000\n",
      "Epoch [1280/30000], Loss: 1210986.8750\n",
      "Epoch [1290/30000], Loss: 1207262.0000\n",
      "Epoch [1300/30000], Loss: 1203557.6250\n",
      "Epoch [1310/30000], Loss: 1199873.8750\n",
      "Epoch [1320/30000], Loss: 1196210.8750\n",
      "Epoch [1330/30000], Loss: 1192568.0000\n",
      "Epoch [1340/30000], Loss: 1188944.5000\n",
      "Epoch [1350/30000], Loss: 1185341.2500\n",
      "Epoch [1360/30000], Loss: 1181757.3750\n",
      "Epoch [1370/30000], Loss: 1178193.6250\n",
      "Epoch [1380/30000], Loss: 1174649.2500\n",
      "Epoch [1390/30000], Loss: 1171124.2500\n",
      "Epoch [1400/30000], Loss: 1167619.0000\n",
      "Epoch [1410/30000], Loss: 1164132.2500\n",
      "Epoch [1420/30000], Loss: 1160663.7500\n",
      "Epoch [1430/30000], Loss: 1157215.0000\n",
      "Epoch [1440/30000], Loss: 1153784.8750\n",
      "Epoch [1450/30000], Loss: 1150373.0000\n",
      "Epoch [1460/30000], Loss: 1146979.8750\n",
      "Epoch [1470/30000], Loss: 1143604.3750\n",
      "Epoch [1480/30000], Loss: 1140248.0000\n",
      "Epoch [1490/30000], Loss: 1136909.1250\n",
      "Epoch [1500/30000], Loss: 1133588.1250\n",
      "Epoch [1510/30000], Loss: 1130285.1250\n",
      "Epoch [1520/30000], Loss: 1126999.3750\n",
      "Epoch [1530/30000], Loss: 1123732.0000\n",
      "Epoch [1540/30000], Loss: 1120481.3750\n",
      "Epoch [1550/30000], Loss: 1117248.2500\n",
      "Epoch [1560/30000], Loss: 1114032.3750\n",
      "Epoch [1570/30000], Loss: 1110833.1250\n",
      "Epoch [1580/30000], Loss: 1107651.1250\n",
      "Epoch [1590/30000], Loss: 1104486.2500\n",
      "Epoch [1600/30000], Loss: 1101338.0000\n",
      "Epoch [1610/30000], Loss: 1098206.2500\n",
      "Epoch [1620/30000], Loss: 1095090.2500\n",
      "Epoch [1630/30000], Loss: 1091991.8750\n",
      "Epoch [1640/30000], Loss: 1088908.7500\n",
      "Epoch [1650/30000], Loss: 1085842.1250\n",
      "Epoch [1660/30000], Loss: 1082791.8750\n",
      "Epoch [1670/30000], Loss: 1079756.8750\n",
      "Epoch [1680/30000], Loss: 1076737.8750\n",
      "Epoch [1690/30000], Loss: 1073734.6250\n",
      "Epoch [1700/30000], Loss: 1070747.2500\n",
      "Epoch [1710/30000], Loss: 1067774.6250\n",
      "Epoch [1720/30000], Loss: 1064817.1250\n",
      "Epoch [1730/30000], Loss: 1061876.1250\n",
      "Epoch [1740/30000], Loss: 1058949.2500\n",
      "Epoch [1750/30000], Loss: 1056037.7500\n",
      "Epoch [1760/30000], Loss: 1053141.0000\n",
      "Epoch [1770/30000], Loss: 1050258.8750\n",
      "Epoch [1780/30000], Loss: 1047391.5625\n",
      "Epoch [1790/30000], Loss: 1044539.1250\n",
      "Epoch [1800/30000], Loss: 1041701.0625\n",
      "Epoch [1810/30000], Loss: 1038876.9375\n",
      "Epoch [1820/30000], Loss: 1036067.8750\n",
      "Epoch [1830/30000], Loss: 1033272.5000\n",
      "Epoch [1840/30000], Loss: 1030490.6875\n",
      "Epoch [1850/30000], Loss: 1027723.0000\n",
      "Epoch [1860/30000], Loss: 1024969.6250\n",
      "Epoch [1870/30000], Loss: 1022229.8750\n",
      "Epoch [1880/30000], Loss: 1019503.3750\n",
      "Epoch [1890/30000], Loss: 1016790.0000\n",
      "Epoch [1900/30000], Loss: 1014091.0625\n",
      "Epoch [1910/30000], Loss: 1011405.0000\n",
      "Epoch [1920/30000], Loss: 1008732.1250\n",
      "Epoch [1930/30000], Loss: 1006072.3125\n",
      "Epoch [1940/30000], Loss: 1003425.4375\n",
      "Epoch [1950/30000], Loss: 1000791.1875\n",
      "Epoch [1960/30000], Loss: 998169.8125\n",
      "Epoch [1970/30000], Loss: 995561.9375\n",
      "Epoch [1980/30000], Loss: 992965.8125\n",
      "Epoch [1990/30000], Loss: 990382.6250\n",
      "Epoch [2000/30000], Loss: 987811.7500\n",
      "Epoch [2010/30000], Loss: 985253.0000\n",
      "Epoch [2020/30000], Loss: 982706.6250\n",
      "Epoch [2030/30000], Loss: 980172.1250\n",
      "Epoch [2040/30000], Loss: 977650.0000\n",
      "Epoch [2050/30000], Loss: 975139.5000\n",
      "Epoch [2060/30000], Loss: 972640.1875\n",
      "Epoch [2070/30000], Loss: 970152.8750\n",
      "Epoch [2080/30000], Loss: 967677.3750\n",
      "Epoch [2090/30000], Loss: 965213.5625\n",
      "Epoch [2100/30000], Loss: 962761.2500\n",
      "Epoch [2110/30000], Loss: 960320.0000\n",
      "Epoch [2120/30000], Loss: 957890.0625\n",
      "Epoch [2130/30000], Loss: 955470.6875\n",
      "Epoch [2140/30000], Loss: 953063.1250\n",
      "Epoch [2150/30000], Loss: 950665.9375\n",
      "Epoch [2160/30000], Loss: 948280.0000\n",
      "Epoch [2170/30000], Loss: 945904.0625\n",
      "Epoch [2180/30000], Loss: 943539.3750\n",
      "Epoch [2190/30000], Loss: 941185.1250\n",
      "Epoch [2200/30000], Loss: 938841.0625\n",
      "Epoch [2210/30000], Loss: 936507.5625\n",
      "Epoch [2220/30000], Loss: 934184.3125\n",
      "Epoch [2230/30000], Loss: 931871.0000\n",
      "Epoch [2240/30000], Loss: 929567.7500\n",
      "Epoch [2250/30000], Loss: 927274.6250\n",
      "Epoch [2260/30000], Loss: 924991.5625\n",
      "Epoch [2270/30000], Loss: 922718.1875\n",
      "Epoch [2280/30000], Loss: 920454.3750\n",
      "Epoch [2290/30000], Loss: 918200.3750\n",
      "Epoch [2300/30000], Loss: 915955.5625\n",
      "Epoch [2310/30000], Loss: 913720.3750\n",
      "Epoch [2320/30000], Loss: 911494.6250\n",
      "Epoch [2330/30000], Loss: 909277.9375\n",
      "Epoch [2340/30000], Loss: 907070.2500\n",
      "Epoch [2350/30000], Loss: 904872.1250\n",
      "Epoch [2360/30000], Loss: 902682.5625\n",
      "Epoch [2370/30000], Loss: 900501.8125\n",
      "Epoch [2380/30000], Loss: 898329.8125\n",
      "Epoch [2390/30000], Loss: 896166.7500\n",
      "Epoch [2400/30000], Loss: 894012.1250\n",
      "Epoch [2410/30000], Loss: 891866.0625\n",
      "Epoch [2420/30000], Loss: 889728.2500\n",
      "Epoch [2430/30000], Loss: 887599.3125\n",
      "Epoch [2440/30000], Loss: 885478.2500\n",
      "Epoch [2450/30000], Loss: 883365.6875\n",
      "Epoch [2460/30000], Loss: 881260.9375\n",
      "Epoch [2470/30000], Loss: 879164.2500\n",
      "Epoch [2480/30000], Loss: 877075.2500\n",
      "Epoch [2490/30000], Loss: 874994.3125\n",
      "Epoch [2500/30000], Loss: 872921.3750\n",
      "Epoch [2510/30000], Loss: 870855.5625\n",
      "Epoch [2520/30000], Loss: 868797.8750\n",
      "Epoch [2530/30000], Loss: 866747.0000\n",
      "Epoch [2540/30000], Loss: 864704.0625\n",
      "Epoch [2550/30000], Loss: 862668.3750\n",
      "Epoch [2560/30000], Loss: 860640.1250\n",
      "Epoch [2570/30000], Loss: 858618.5000\n",
      "Epoch [2580/30000], Loss: 856604.2500\n",
      "Epoch [2590/30000], Loss: 854597.0000\n",
      "Epoch [2600/30000], Loss: 852596.6875\n",
      "Epoch [2610/30000], Loss: 850603.0625\n",
      "Epoch [2620/30000], Loss: 848616.6875\n",
      "Epoch [2630/30000], Loss: 846636.5000\n",
      "Epoch [2640/30000], Loss: 844662.6250\n",
      "Epoch [2650/30000], Loss: 842695.6250\n",
      "Epoch [2660/30000], Loss: 840735.6250\n",
      "Epoch [2670/30000], Loss: 838781.1875\n",
      "Epoch [2680/30000], Loss: 836833.6875\n",
      "Epoch [2690/30000], Loss: 834891.5625\n",
      "Epoch [2700/30000], Loss: 832956.2500\n",
      "Epoch [2710/30000], Loss: 831026.6250\n",
      "Epoch [2720/30000], Loss: 829103.2500\n",
      "Epoch [2730/30000], Loss: 827185.5625\n",
      "Epoch [2740/30000], Loss: 825274.0000\n",
      "Epoch [2750/30000], Loss: 823367.6875\n",
      "Epoch [2760/30000], Loss: 821467.6250\n",
      "Epoch [2770/30000], Loss: 819572.8125\n",
      "Epoch [2780/30000], Loss: 817683.7500\n",
      "Epoch [2790/30000], Loss: 815799.6875\n",
      "Epoch [2800/30000], Loss: 813921.8125\n",
      "Epoch [2810/30000], Loss: 812048.5000\n",
      "Epoch [2820/30000], Loss: 810181.3125\n",
      "Epoch [2830/30000], Loss: 808318.3125\n",
      "Epoch [2840/30000], Loss: 806461.0000\n",
      "Epoch [2850/30000], Loss: 804608.3125\n",
      "Epoch [2860/30000], Loss: 802761.1875\n",
      "Epoch [2870/30000], Loss: 800918.5625\n",
      "Epoch [2880/30000], Loss: 799080.6250\n",
      "Epoch [2890/30000], Loss: 797247.8125\n",
      "Epoch [2900/30000], Loss: 795419.4375\n",
      "Epoch [2910/30000], Loss: 793595.9375\n",
      "Epoch [2920/30000], Loss: 791776.6875\n",
      "Epoch [2930/30000], Loss: 789961.9375\n",
      "Epoch [2940/30000], Loss: 788151.7500\n",
      "Epoch [2950/30000], Loss: 786346.0000\n",
      "Epoch [2960/30000], Loss: 784544.7500\n",
      "Epoch [2970/30000], Loss: 782747.3750\n",
      "Epoch [2980/30000], Loss: 780954.3125\n",
      "Epoch [2990/30000], Loss: 779165.4375\n",
      "Epoch [3000/30000], Loss: 777380.3750\n",
      "Epoch [3010/30000], Loss: 775599.5000\n",
      "Epoch [3020/30000], Loss: 773822.6250\n",
      "Epoch [3030/30000], Loss: 772049.1250\n",
      "Epoch [3040/30000], Loss: 770280.5000\n",
      "Epoch [3050/30000], Loss: 768514.5000\n",
      "Epoch [3060/30000], Loss: 766752.8125\n",
      "Epoch [3070/30000], Loss: 764994.8125\n",
      "Epoch [3080/30000], Loss: 763239.9375\n",
      "Epoch [3090/30000], Loss: 761488.6875\n",
      "Epoch [3100/30000], Loss: 759741.4375\n",
      "Epoch [3110/30000], Loss: 757997.0000\n",
      "Epoch [3120/30000], Loss: 756255.9375\n",
      "Epoch [3130/30000], Loss: 754518.5000\n",
      "Epoch [3140/30000], Loss: 752784.3750\n",
      "Epoch [3150/30000], Loss: 751052.9375\n",
      "Epoch [3160/30000], Loss: 749324.9375\n",
      "Epoch [3170/30000], Loss: 747600.0625\n",
      "Epoch [3180/30000], Loss: 745878.1875\n",
      "Epoch [3190/30000], Loss: 744159.2500\n",
      "Epoch [3200/30000], Loss: 742443.6250\n",
      "Epoch [3210/30000], Loss: 740730.6250\n",
      "Epoch [3220/30000], Loss: 739020.1250\n",
      "Epoch [3230/30000], Loss: 737312.8750\n",
      "Epoch [3240/30000], Loss: 735608.3750\n",
      "Epoch [3250/30000], Loss: 733906.1250\n",
      "Epoch [3260/30000], Loss: 732206.9375\n",
      "Epoch [3270/30000], Loss: 730510.2500\n",
      "Epoch [3280/30000], Loss: 728816.0000\n",
      "Epoch [3290/30000], Loss: 727124.2500\n",
      "Epoch [3300/30000], Loss: 725435.3125\n",
      "Epoch [3310/30000], Loss: 723748.6875\n",
      "Epoch [3320/30000], Loss: 722064.2500\n",
      "Epoch [3330/30000], Loss: 720382.3750\n",
      "Epoch [3340/30000], Loss: 718702.6250\n",
      "Epoch [3350/30000], Loss: 717025.0625\n",
      "Epoch [3360/30000], Loss: 715349.5625\n",
      "Epoch [3370/30000], Loss: 713676.8125\n",
      "Epoch [3380/30000], Loss: 712006.0625\n",
      "Epoch [3390/30000], Loss: 710336.8125\n",
      "Epoch [3400/30000], Loss: 708670.5000\n",
      "Epoch [3410/30000], Loss: 707005.3750\n",
      "Epoch [3420/30000], Loss: 705343.2500\n",
      "Epoch [3430/30000], Loss: 703682.3125\n",
      "Epoch [3440/30000], Loss: 702023.3750\n",
      "Epoch [3450/30000], Loss: 700366.6250\n",
      "Epoch [3460/30000], Loss: 698711.3125\n",
      "Epoch [3470/30000], Loss: 697058.3750\n",
      "Epoch [3480/30000], Loss: 695406.6875\n",
      "Epoch [3490/30000], Loss: 693756.9375\n",
      "Epoch [3500/30000], Loss: 692108.9375\n",
      "Epoch [3510/30000], Loss: 690462.8125\n",
      "Epoch [3520/30000], Loss: 688817.9375\n",
      "Epoch [3530/30000], Loss: 687175.1250\n",
      "Epoch [3540/30000], Loss: 685533.5625\n",
      "Epoch [3550/30000], Loss: 683894.0000\n",
      "Epoch [3560/30000], Loss: 682255.6250\n",
      "Epoch [3570/30000], Loss: 680618.8750\n",
      "Epoch [3580/30000], Loss: 678983.5625\n",
      "Epoch [3590/30000], Loss: 677350.1250\n",
      "Epoch [3600/30000], Loss: 675717.5000\n",
      "Epoch [3610/30000], Loss: 674086.6875\n",
      "Epoch [3620/30000], Loss: 672457.0625\n",
      "Epoch [3630/30000], Loss: 670829.1875\n",
      "Epoch [3640/30000], Loss: 669202.3125\n",
      "Epoch [3650/30000], Loss: 667576.9375\n",
      "Epoch [3660/30000], Loss: 665952.6250\n",
      "Epoch [3670/30000], Loss: 664329.7500\n",
      "Epoch [3680/30000], Loss: 662708.1875\n",
      "Epoch [3690/30000], Loss: 661087.8125\n",
      "Epoch [3700/30000], Loss: 659468.6875\n",
      "Epoch [3710/30000], Loss: 657850.6875\n",
      "Epoch [3720/30000], Loss: 656233.8750\n",
      "Epoch [3730/30000], Loss: 654618.2500\n",
      "Epoch [3740/30000], Loss: 653003.5625\n",
      "Epoch [3750/30000], Loss: 651390.5625\n",
      "Epoch [3760/30000], Loss: 649778.5000\n",
      "Epoch [3770/30000], Loss: 648167.5625\n",
      "Epoch [3780/30000], Loss: 646557.5625\n",
      "Epoch [3790/30000], Loss: 644948.5000\n",
      "Epoch [3800/30000], Loss: 643340.5000\n",
      "Epoch [3810/30000], Loss: 641733.8125\n",
      "Epoch [3820/30000], Loss: 640128.2500\n",
      "Epoch [3830/30000], Loss: 638523.3125\n",
      "Epoch [3840/30000], Loss: 636919.6250\n",
      "Epoch [3850/30000], Loss: 635317.0000\n",
      "Epoch [3860/30000], Loss: 633715.4375\n",
      "Epoch [3870/30000], Loss: 632114.6875\n",
      "Epoch [3880/30000], Loss: 630515.0625\n",
      "Epoch [3890/30000], Loss: 628916.1875\n",
      "Epoch [3900/30000], Loss: 627318.4375\n",
      "Epoch [3910/30000], Loss: 625721.5625\n",
      "Epoch [3920/30000], Loss: 624125.5625\n",
      "Epoch [3930/30000], Loss: 622530.7500\n",
      "Epoch [3940/30000], Loss: 620936.5625\n",
      "Epoch [3950/30000], Loss: 619343.3750\n",
      "Epoch [3960/30000], Loss: 617751.3125\n",
      "Epoch [3970/30000], Loss: 616159.9375\n",
      "Epoch [3980/30000], Loss: 614569.6875\n",
      "Epoch [3990/30000], Loss: 612980.2500\n",
      "Epoch [4000/30000], Loss: 611391.6875\n",
      "Epoch [4010/30000], Loss: 609803.8750\n",
      "Epoch [4020/30000], Loss: 608217.3125\n",
      "Epoch [4030/30000], Loss: 606631.2500\n",
      "Epoch [4040/30000], Loss: 605046.4375\n",
      "Epoch [4050/30000], Loss: 603462.4375\n",
      "Epoch [4060/30000], Loss: 601879.1875\n",
      "Epoch [4070/30000], Loss: 600297.0625\n",
      "Epoch [4080/30000], Loss: 598715.4375\n",
      "Epoch [4090/30000], Loss: 597135.1250\n",
      "Epoch [4100/30000], Loss: 595555.6875\n",
      "Epoch [4110/30000], Loss: 593976.7500\n",
      "Epoch [4120/30000], Loss: 592399.1250\n",
      "Epoch [4130/30000], Loss: 590822.3125\n",
      "Epoch [4140/30000], Loss: 589246.4375\n",
      "Epoch [4150/30000], Loss: 587671.0000\n",
      "Epoch [4160/30000], Loss: 586097.1250\n",
      "Epoch [4170/30000], Loss: 584524.0000\n",
      "Epoch [4180/30000], Loss: 582951.7500\n",
      "Epoch [4190/30000], Loss: 581380.3750\n",
      "Epoch [4200/30000], Loss: 579809.8125\n",
      "Epoch [4210/30000], Loss: 578240.5000\n",
      "Epoch [4220/30000], Loss: 576671.8125\n",
      "Epoch [4230/30000], Loss: 575104.2500\n",
      "Epoch [4240/30000], Loss: 573537.5000\n",
      "Epoch [4250/30000], Loss: 571971.8750\n",
      "Epoch [4260/30000], Loss: 570407.1875\n",
      "Epoch [4270/30000], Loss: 568843.2500\n",
      "Epoch [4280/30000], Loss: 567280.7500\n",
      "Epoch [4290/30000], Loss: 565719.0000\n",
      "Epoch [4300/30000], Loss: 564158.2500\n",
      "Epoch [4310/30000], Loss: 562598.4375\n",
      "Epoch [4320/30000], Loss: 561039.7500\n",
      "Epoch [4330/30000], Loss: 559482.0625\n",
      "Epoch [4340/30000], Loss: 557925.5000\n",
      "Epoch [4350/30000], Loss: 556369.9375\n",
      "Epoch [4360/30000], Loss: 554815.1875\n",
      "Epoch [4370/30000], Loss: 553261.9375\n",
      "Epoch [4380/30000], Loss: 551709.6875\n",
      "Epoch [4390/30000], Loss: 550158.3750\n",
      "Epoch [4400/30000], Loss: 548608.1250\n",
      "Epoch [4410/30000], Loss: 547059.2500\n",
      "Epoch [4420/30000], Loss: 545511.1875\n",
      "Epoch [4430/30000], Loss: 543964.2500\n",
      "Epoch [4440/30000], Loss: 542418.6875\n",
      "Epoch [4450/30000], Loss: 540874.2500\n",
      "Epoch [4460/30000], Loss: 539331.0000\n",
      "Epoch [4470/30000], Loss: 537788.6875\n",
      "Epoch [4480/30000], Loss: 536247.8125\n",
      "Epoch [4490/30000], Loss: 534708.0625\n",
      "Epoch [4500/30000], Loss: 533169.6875\n",
      "Epoch [4510/30000], Loss: 531632.6250\n",
      "Epoch [4520/30000], Loss: 530096.5000\n",
      "Epoch [4530/30000], Loss: 528561.7500\n",
      "Epoch [4540/30000], Loss: 527028.5000\n",
      "Epoch [4550/30000], Loss: 525496.3125\n",
      "Epoch [4560/30000], Loss: 523965.5938\n",
      "Epoch [4570/30000], Loss: 522436.1250\n",
      "Epoch [4580/30000], Loss: 520908.1875\n",
      "Epoch [4590/30000], Loss: 519381.4062\n",
      "Epoch [4600/30000], Loss: 517856.0000\n",
      "Epoch [4610/30000], Loss: 516332.1562\n",
      "Epoch [4620/30000], Loss: 514809.5000\n",
      "Epoch [4630/30000], Loss: 513288.3750\n",
      "Epoch [4640/30000], Loss: 511768.5625\n",
      "Epoch [4650/30000], Loss: 510250.4688\n",
      "Epoch [4660/30000], Loss: 508733.7500\n",
      "Epoch [4670/30000], Loss: 507218.4375\n",
      "Epoch [4680/30000], Loss: 505704.5000\n",
      "Epoch [4690/30000], Loss: 504192.4062\n",
      "Epoch [4700/30000], Loss: 502681.5938\n",
      "Epoch [4710/30000], Loss: 501172.4062\n",
      "Epoch [4720/30000], Loss: 499664.8438\n",
      "Epoch [4730/30000], Loss: 498158.8750\n",
      "Epoch [4740/30000], Loss: 496654.3125\n",
      "Epoch [4750/30000], Loss: 495151.3438\n",
      "Epoch [4760/30000], Loss: 493650.5000\n",
      "Epoch [4770/30000], Loss: 492150.9688\n",
      "Epoch [4780/30000], Loss: 490653.0625\n",
      "Epoch [4790/30000], Loss: 489157.0625\n",
      "Epoch [4800/30000], Loss: 487662.5000\n",
      "Epoch [4810/30000], Loss: 486169.8125\n",
      "Epoch [4820/30000], Loss: 484679.0000\n",
      "Epoch [4830/30000], Loss: 483189.6875\n",
      "Epoch [4840/30000], Loss: 481702.2500\n",
      "Epoch [4850/30000], Loss: 480216.5312\n",
      "Epoch [4860/30000], Loss: 478732.9062\n",
      "Epoch [4870/30000], Loss: 477250.7500\n",
      "Epoch [4880/30000], Loss: 475770.7188\n",
      "Epoch [4890/30000], Loss: 474292.4062\n",
      "Epoch [4900/30000], Loss: 472815.9688\n",
      "Epoch [4910/30000], Loss: 471341.4062\n",
      "Epoch [4920/30000], Loss: 469868.9062\n",
      "Epoch [4930/30000], Loss: 468398.0312\n",
      "Epoch [4940/30000], Loss: 466929.5000\n",
      "Epoch [4950/30000], Loss: 465462.5625\n",
      "Epoch [4960/30000], Loss: 463997.9375\n",
      "Epoch [4970/30000], Loss: 462535.1562\n",
      "Epoch [4980/30000], Loss: 461074.5625\n",
      "Epoch [4990/30000], Loss: 459615.9688\n",
      "Epoch [5000/30000], Loss: 458159.0938\n",
      "Epoch [5010/30000], Loss: 456704.5625\n",
      "Epoch [5020/30000], Loss: 455252.1562\n",
      "Epoch [5030/30000], Loss: 453801.9688\n",
      "Epoch [5040/30000], Loss: 452353.6875\n",
      "Epoch [5050/30000], Loss: 450907.5625\n",
      "Epoch [5060/30000], Loss: 449463.7812\n",
      "Epoch [5070/30000], Loss: 448021.8750\n",
      "Epoch [5080/30000], Loss: 446582.3750\n",
      "Epoch [5090/30000], Loss: 445145.0938\n",
      "Epoch [5100/30000], Loss: 443710.0625\n",
      "Epoch [5110/30000], Loss: 442277.1250\n",
      "Epoch [5120/30000], Loss: 440846.5625\n",
      "Epoch [5130/30000], Loss: 439418.3125\n",
      "Epoch [5140/30000], Loss: 437992.4062\n",
      "Epoch [5150/30000], Loss: 436568.7812\n",
      "Epoch [5160/30000], Loss: 435147.2500\n",
      "Epoch [5170/30000], Loss: 433728.4375\n",
      "Epoch [5180/30000], Loss: 432311.7812\n",
      "Epoch [5190/30000], Loss: 430897.5312\n",
      "Epoch [5200/30000], Loss: 429485.5938\n",
      "Epoch [5210/30000], Loss: 428076.4375\n",
      "Epoch [5220/30000], Loss: 426669.4688\n",
      "Epoch [5230/30000], Loss: 425264.9375\n",
      "Epoch [5240/30000], Loss: 423862.8438\n",
      "Epoch [5250/30000], Loss: 422463.1875\n",
      "Epoch [5260/30000], Loss: 421066.0000\n",
      "Epoch [5270/30000], Loss: 419671.5625\n",
      "Epoch [5280/30000], Loss: 418279.6562\n",
      "Epoch [5290/30000], Loss: 416890.1250\n",
      "Epoch [5300/30000], Loss: 415503.1562\n",
      "Epoch [5310/30000], Loss: 414118.8750\n",
      "Epoch [5320/30000], Loss: 412737.0625\n",
      "Epoch [5330/30000], Loss: 411357.9688\n",
      "Epoch [5340/30000], Loss: 409981.5000\n",
      "Epoch [5350/30000], Loss: 408607.6250\n",
      "Epoch [5360/30000], Loss: 407236.3750\n",
      "Epoch [5370/30000], Loss: 405867.9062\n",
      "Epoch [5380/30000], Loss: 404502.0625\n",
      "Epoch [5390/30000], Loss: 403138.8750\n",
      "Epoch [5400/30000], Loss: 401778.4688\n",
      "Epoch [5410/30000], Loss: 400420.8438\n",
      "Epoch [5420/30000], Loss: 399065.8750\n",
      "Epoch [5430/30000], Loss: 397713.9062\n",
      "Epoch [5440/30000], Loss: 396364.2500\n",
      "Epoch [5450/30000], Loss: 395017.5312\n",
      "Epoch [5460/30000], Loss: 393673.7500\n",
      "Epoch [5470/30000], Loss: 392332.7812\n",
      "Epoch [5480/30000], Loss: 390994.7188\n",
      "Epoch [5490/30000], Loss: 389659.2500\n",
      "Epoch [5500/30000], Loss: 388326.8438\n",
      "Epoch [5510/30000], Loss: 386997.2188\n",
      "Epoch [5520/30000], Loss: 385670.5000\n",
      "Epoch [5530/30000], Loss: 384346.7188\n",
      "Epoch [5540/30000], Loss: 383025.6875\n",
      "Epoch [5550/30000], Loss: 381707.6562\n",
      "Epoch [5560/30000], Loss: 380392.7500\n",
      "Epoch [5570/30000], Loss: 379080.7188\n",
      "Epoch [5580/30000], Loss: 377771.3125\n",
      "Epoch [5590/30000], Loss: 376465.2188\n",
      "Epoch [5600/30000], Loss: 375162.2188\n",
      "Epoch [5610/30000], Loss: 373861.8750\n",
      "Epoch [5620/30000], Loss: 372564.7812\n",
      "Epoch [5630/30000], Loss: 371270.5000\n",
      "Epoch [5640/30000], Loss: 369979.5000\n",
      "Epoch [5650/30000], Loss: 368691.4062\n",
      "Epoch [5660/30000], Loss: 367406.5312\n",
      "Epoch [5670/30000], Loss: 366124.5000\n",
      "Epoch [5680/30000], Loss: 364845.6250\n",
      "Epoch [5690/30000], Loss: 363569.9062\n",
      "Epoch [5700/30000], Loss: 362297.0938\n",
      "Epoch [5710/30000], Loss: 361027.5000\n",
      "Epoch [5720/30000], Loss: 359761.1250\n",
      "Epoch [5730/30000], Loss: 358497.7500\n",
      "Epoch [5740/30000], Loss: 357237.7188\n",
      "Epoch [5750/30000], Loss: 355980.6250\n",
      "Epoch [5760/30000], Loss: 354726.9062\n",
      "Epoch [5770/30000], Loss: 353476.1875\n",
      "Epoch [5780/30000], Loss: 352228.6875\n",
      "Epoch [5790/30000], Loss: 350984.4375\n",
      "Epoch [5800/30000], Loss: 349743.3125\n",
      "Epoch [5810/30000], Loss: 348505.5312\n",
      "Epoch [5820/30000], Loss: 347270.8750\n",
      "Epoch [5830/30000], Loss: 346039.4375\n",
      "Epoch [5840/30000], Loss: 344811.3750\n",
      "Epoch [5850/30000], Loss: 343586.3750\n",
      "Epoch [5860/30000], Loss: 342364.7812\n",
      "Epoch [5870/30000], Loss: 341146.4062\n",
      "Epoch [5880/30000], Loss: 339931.2812\n",
      "Epoch [5890/30000], Loss: 338719.5938\n",
      "Epoch [5900/30000], Loss: 337510.8750\n",
      "Epoch [5910/30000], Loss: 336305.5625\n",
      "Epoch [5920/30000], Loss: 335103.8125\n",
      "Epoch [5930/30000], Loss: 333905.0938\n",
      "Epoch [5940/30000], Loss: 332709.8438\n",
      "Epoch [5950/30000], Loss: 331517.7188\n",
      "Epoch [5960/30000], Loss: 330329.1562\n",
      "Epoch [5970/30000], Loss: 329144.0938\n",
      "Epoch [5980/30000], Loss: 327962.1562\n",
      "Epoch [5990/30000], Loss: 326783.4688\n",
      "Epoch [6000/30000], Loss: 325608.2812\n",
      "Epoch [6010/30000], Loss: 324436.6250\n",
      "Epoch [6020/30000], Loss: 323268.0312\n",
      "Epoch [6030/30000], Loss: 322102.9375\n",
      "Epoch [6040/30000], Loss: 320941.1875\n",
      "Epoch [6050/30000], Loss: 319783.0000\n",
      "Epoch [6060/30000], Loss: 318628.1250\n",
      "Epoch [6070/30000], Loss: 317476.6875\n",
      "Epoch [6080/30000], Loss: 316328.4688\n",
      "Epoch [6090/30000], Loss: 315183.9688\n",
      "Epoch [6100/30000], Loss: 314042.7188\n",
      "Epoch [6110/30000], Loss: 312904.9062\n",
      "Epoch [6120/30000], Loss: 311770.5000\n",
      "Epoch [6130/30000], Loss: 310639.5312\n",
      "Epoch [6140/30000], Loss: 309512.1250\n",
      "Epoch [6150/30000], Loss: 308387.9688\n",
      "Epoch [6160/30000], Loss: 307267.5000\n",
      "Epoch [6170/30000], Loss: 306150.3438\n",
      "Epoch [6180/30000], Loss: 305036.6875\n",
      "Epoch [6190/30000], Loss: 303926.4375\n",
      "Epoch [6200/30000], Loss: 302819.6562\n",
      "Epoch [6210/30000], Loss: 301716.3438\n",
      "Epoch [6220/30000], Loss: 300616.4688\n",
      "Epoch [6230/30000], Loss: 299520.3750\n",
      "Epoch [6240/30000], Loss: 298427.5312\n",
      "Epoch [6250/30000], Loss: 297338.0938\n",
      "Epoch [6260/30000], Loss: 296252.3125\n",
      "Epoch [6270/30000], Loss: 295169.7812\n",
      "Epoch [6280/30000], Loss: 294091.0625\n",
      "Epoch [6290/30000], Loss: 293015.6562\n",
      "Epoch [6300/30000], Loss: 291943.6562\n",
      "Epoch [6310/30000], Loss: 290875.1562\n",
      "Epoch [6320/30000], Loss: 289810.3125\n",
      "Epoch [6330/30000], Loss: 288748.9375\n",
      "Epoch [6340/30000], Loss: 287691.0000\n",
      "Epoch [6350/30000], Loss: 286636.6562\n",
      "Epoch [6360/30000], Loss: 285585.5938\n",
      "Epoch [6370/30000], Loss: 284538.3125\n",
      "Epoch [6380/30000], Loss: 283494.5000\n",
      "Epoch [6390/30000], Loss: 282454.2812\n",
      "Epoch [6400/30000], Loss: 281417.3125\n",
      "Epoch [6410/30000], Loss: 280383.9375\n",
      "Epoch [6420/30000], Loss: 279354.2500\n",
      "Epoch [6430/30000], Loss: 278327.8438\n",
      "Epoch [6440/30000], Loss: 277305.0938\n",
      "Epoch [6450/30000], Loss: 276285.6250\n",
      "Epoch [6460/30000], Loss: 275270.0625\n",
      "Epoch [6470/30000], Loss: 274257.8125\n",
      "Epoch [6480/30000], Loss: 273249.0625\n",
      "Epoch [6490/30000], Loss: 272243.7500\n",
      "Epoch [6500/30000], Loss: 271242.2188\n",
      "Epoch [6510/30000], Loss: 270244.0000\n",
      "Epoch [6520/30000], Loss: 269249.2500\n",
      "Epoch [6530/30000], Loss: 268258.1250\n",
      "Epoch [6540/30000], Loss: 267270.5000\n",
      "Epoch [6550/30000], Loss: 266286.4375\n",
      "Epoch [6560/30000], Loss: 265305.7188\n",
      "Epoch [6570/30000], Loss: 264328.6562\n",
      "Epoch [6580/30000], Loss: 263355.1562\n",
      "Epoch [6590/30000], Loss: 262385.0625\n",
      "Epoch [6600/30000], Loss: 261418.3438\n",
      "Epoch [6610/30000], Loss: 260455.3281\n",
      "Epoch [6620/30000], Loss: 259495.7188\n",
      "Epoch [6630/30000], Loss: 258539.6562\n",
      "Epoch [6640/30000], Loss: 257587.0469\n",
      "Epoch [6650/30000], Loss: 256637.9688\n",
      "Epoch [6660/30000], Loss: 255692.4375\n",
      "Epoch [6670/30000], Loss: 254750.2344\n",
      "Epoch [6680/30000], Loss: 253811.6875\n",
      "Epoch [6690/30000], Loss: 252876.4844\n",
      "Epoch [6700/30000], Loss: 251944.9844\n",
      "Epoch [6710/30000], Loss: 251016.7188\n",
      "Epoch [6720/30000], Loss: 250092.1094\n",
      "Epoch [6730/30000], Loss: 249170.9531\n",
      "Epoch [6740/30000], Loss: 248253.1562\n",
      "Epoch [6750/30000], Loss: 247338.9062\n",
      "Epoch [6760/30000], Loss: 246428.0781\n",
      "Epoch [6770/30000], Loss: 245520.7344\n",
      "Epoch [6780/30000], Loss: 244616.7656\n",
      "Epoch [6790/30000], Loss: 243716.4062\n",
      "Epoch [6800/30000], Loss: 242819.4062\n",
      "Epoch [6810/30000], Loss: 241925.8906\n",
      "Epoch [6820/30000], Loss: 241035.8750\n",
      "Epoch [6830/30000], Loss: 240149.2031\n",
      "Epoch [6840/30000], Loss: 239265.8906\n",
      "Epoch [6850/30000], Loss: 238386.0625\n",
      "Epoch [6860/30000], Loss: 237509.7969\n",
      "Epoch [6870/30000], Loss: 236636.8750\n",
      "Epoch [6880/30000], Loss: 235767.3281\n",
      "Epoch [6890/30000], Loss: 234901.2344\n",
      "Epoch [6900/30000], Loss: 234038.4844\n",
      "Epoch [6910/30000], Loss: 233179.2031\n",
      "Epoch [6920/30000], Loss: 232323.2969\n",
      "Epoch [6930/30000], Loss: 231470.7500\n",
      "Epoch [6940/30000], Loss: 230621.6875\n",
      "Epoch [6950/30000], Loss: 229775.9531\n",
      "Epoch [6960/30000], Loss: 228933.5938\n",
      "Epoch [6970/30000], Loss: 228094.6250\n",
      "Epoch [6980/30000], Loss: 227259.0469\n",
      "Epoch [6990/30000], Loss: 226426.7500\n",
      "Epoch [7000/30000], Loss: 225597.8125\n",
      "Epoch [7010/30000], Loss: 224772.2969\n",
      "Epoch [7020/30000], Loss: 223950.0000\n",
      "Epoch [7030/30000], Loss: 223131.1406\n",
      "Epoch [7040/30000], Loss: 222315.5625\n",
      "Epoch [7050/30000], Loss: 221503.3750\n",
      "Epoch [7060/30000], Loss: 220694.3438\n",
      "Epoch [7070/30000], Loss: 219888.8750\n",
      "Epoch [7080/30000], Loss: 219086.6719\n",
      "Epoch [7090/30000], Loss: 218287.5781\n",
      "Epoch [7100/30000], Loss: 217491.9219\n",
      "Epoch [7110/30000], Loss: 216699.4844\n",
      "Epoch [7120/30000], Loss: 215910.3594\n",
      "Epoch [7130/30000], Loss: 215124.5781\n",
      "Epoch [7140/30000], Loss: 214341.9531\n",
      "Epoch [7150/30000], Loss: 213562.5938\n",
      "Epoch [7160/30000], Loss: 212786.4688\n",
      "Epoch [7170/30000], Loss: 212013.7656\n",
      "Epoch [7180/30000], Loss: 211244.2031\n",
      "Epoch [7190/30000], Loss: 210477.8594\n",
      "Epoch [7200/30000], Loss: 209714.7969\n",
      "Epoch [7210/30000], Loss: 208954.8438\n",
      "Epoch [7220/30000], Loss: 208198.2031\n",
      "Epoch [7230/30000], Loss: 207444.6719\n",
      "Epoch [7240/30000], Loss: 206694.2344\n",
      "Epoch [7250/30000], Loss: 205947.1094\n",
      "Epoch [7260/30000], Loss: 205203.1094\n",
      "Epoch [7270/30000], Loss: 204462.3281\n",
      "Epoch [7280/30000], Loss: 203724.6719\n",
      "Epoch [7290/30000], Loss: 202990.2031\n",
      "Epoch [7300/30000], Loss: 202258.8906\n",
      "Epoch [7310/30000], Loss: 201530.7656\n",
      "Epoch [7320/30000], Loss: 200805.6562\n",
      "Epoch [7330/30000], Loss: 200083.7188\n",
      "Epoch [7340/30000], Loss: 199364.8594\n",
      "Epoch [7350/30000], Loss: 198649.3750\n",
      "Epoch [7360/30000], Loss: 197936.6562\n",
      "Epoch [7370/30000], Loss: 197227.2031\n",
      "Epoch [7380/30000], Loss: 196520.8594\n",
      "Epoch [7390/30000], Loss: 195817.4844\n",
      "Epoch [7400/30000], Loss: 195117.1719\n",
      "Epoch [7410/30000], Loss: 194419.9844\n",
      "Epoch [7420/30000], Loss: 193725.7031\n",
      "Epoch [7430/30000], Loss: 193034.6719\n",
      "Epoch [7440/30000], Loss: 192346.4844\n",
      "Epoch [7450/30000], Loss: 191661.4844\n",
      "Epoch [7460/30000], Loss: 190979.2500\n",
      "Epoch [7470/30000], Loss: 190300.2188\n",
      "Epoch [7480/30000], Loss: 189624.1562\n",
      "Epoch [7490/30000], Loss: 188951.0469\n",
      "Epoch [7500/30000], Loss: 188280.8594\n",
      "Epoch [7510/30000], Loss: 187613.8125\n",
      "Epoch [7520/30000], Loss: 186949.6250\n",
      "Epoch [7530/30000], Loss: 186288.3438\n",
      "Epoch [7540/30000], Loss: 185630.0469\n",
      "Epoch [7550/30000], Loss: 184974.7500\n",
      "Epoch [7560/30000], Loss: 184322.1719\n",
      "Epoch [7570/30000], Loss: 183672.6406\n",
      "Epoch [7580/30000], Loss: 183025.9844\n",
      "Epoch [7590/30000], Loss: 182382.2500\n",
      "Epoch [7600/30000], Loss: 181741.3594\n",
      "Epoch [7610/30000], Loss: 181103.3125\n",
      "Epoch [7620/30000], Loss: 180468.2031\n",
      "Epoch [7630/30000], Loss: 179835.9688\n",
      "Epoch [7640/30000], Loss: 179206.5000\n",
      "Epoch [7650/30000], Loss: 178579.8594\n",
      "Epoch [7660/30000], Loss: 177956.1094\n",
      "Epoch [7670/30000], Loss: 177335.0781\n",
      "Epoch [7680/30000], Loss: 176716.9062\n",
      "Epoch [7690/30000], Loss: 176101.5781\n",
      "Epoch [7700/30000], Loss: 175488.9844\n",
      "Epoch [7710/30000], Loss: 174879.1562\n",
      "Epoch [7720/30000], Loss: 174272.0000\n",
      "Epoch [7730/30000], Loss: 173667.7969\n",
      "Epoch [7740/30000], Loss: 173066.2812\n",
      "Epoch [7750/30000], Loss: 172467.4375\n",
      "Epoch [7760/30000], Loss: 171871.3281\n",
      "Epoch [7770/30000], Loss: 171277.9688\n",
      "Epoch [7780/30000], Loss: 170687.2656\n",
      "Epoch [7790/30000], Loss: 170099.3281\n",
      "Epoch [7800/30000], Loss: 169513.9844\n",
      "Epoch [7810/30000], Loss: 168931.3125\n",
      "Epoch [7820/30000], Loss: 168351.3906\n",
      "Epoch [7830/30000], Loss: 167774.0469\n",
      "Epoch [7840/30000], Loss: 167199.4219\n",
      "Epoch [7850/30000], Loss: 166627.3438\n",
      "Epoch [7860/30000], Loss: 166057.9688\n",
      "Epoch [7870/30000], Loss: 165491.2031\n",
      "Epoch [7880/30000], Loss: 164926.9062\n",
      "Epoch [7890/30000], Loss: 164365.3594\n",
      "Epoch [7900/30000], Loss: 163806.3125\n",
      "Epoch [7910/30000], Loss: 163249.7812\n",
      "Epoch [7920/30000], Loss: 162695.8281\n",
      "Epoch [7930/30000], Loss: 162144.5156\n",
      "Epoch [7940/30000], Loss: 161595.6562\n",
      "Epoch [7950/30000], Loss: 161049.3906\n",
      "Epoch [7960/30000], Loss: 160505.5312\n",
      "Epoch [7970/30000], Loss: 159964.3125\n",
      "Epoch [7980/30000], Loss: 159425.4688\n",
      "Epoch [7990/30000], Loss: 158889.1875\n",
      "Epoch [8000/30000], Loss: 158355.3906\n",
      "Epoch [8010/30000], Loss: 157824.0469\n",
      "Epoch [8020/30000], Loss: 157295.0781\n",
      "Epoch [8030/30000], Loss: 156768.6250\n",
      "Epoch [8040/30000], Loss: 156244.4375\n",
      "Epoch [8050/30000], Loss: 155722.8438\n",
      "Epoch [8060/30000], Loss: 155203.6406\n",
      "Epoch [8070/30000], Loss: 154686.7969\n",
      "Epoch [8080/30000], Loss: 154172.3594\n",
      "Epoch [8090/30000], Loss: 153660.3125\n",
      "Epoch [8100/30000], Loss: 153150.5781\n",
      "Epoch [8110/30000], Loss: 152643.2500\n",
      "Epoch [8120/30000], Loss: 152138.2812\n",
      "Epoch [8130/30000], Loss: 151635.5938\n",
      "Epoch [8140/30000], Loss: 151135.2188\n",
      "Epoch [8150/30000], Loss: 150637.1250\n",
      "Epoch [8160/30000], Loss: 150141.3438\n",
      "Epoch [8170/30000], Loss: 149647.8906\n",
      "Epoch [8180/30000], Loss: 149156.7656\n",
      "Epoch [8190/30000], Loss: 148667.8125\n",
      "Epoch [8200/30000], Loss: 148181.2031\n",
      "Epoch [8210/30000], Loss: 147696.7344\n",
      "Epoch [8220/30000], Loss: 147214.5469\n",
      "Epoch [8230/30000], Loss: 146734.6406\n",
      "Epoch [8240/30000], Loss: 146256.8594\n",
      "Epoch [8250/30000], Loss: 145781.2969\n",
      "Epoch [8260/30000], Loss: 145307.9375\n",
      "Epoch [8270/30000], Loss: 144836.7656\n",
      "Epoch [8280/30000], Loss: 144367.6406\n",
      "Epoch [8290/30000], Loss: 143900.8438\n",
      "Epoch [8300/30000], Loss: 143436.1719\n",
      "Epoch [8310/30000], Loss: 142973.5312\n",
      "Epoch [8320/30000], Loss: 142513.0312\n",
      "Epoch [8330/30000], Loss: 142054.7031\n",
      "Epoch [8340/30000], Loss: 141598.4688\n",
      "Epoch [8350/30000], Loss: 141144.3125\n",
      "Epoch [8360/30000], Loss: 140692.1875\n",
      "Epoch [8370/30000], Loss: 140242.2031\n",
      "Epoch [8380/30000], Loss: 139794.2031\n",
      "Epoch [8390/30000], Loss: 139348.2500\n",
      "Epoch [8400/30000], Loss: 138904.4062\n",
      "Epoch [8410/30000], Loss: 138462.5312\n",
      "Epoch [8420/30000], Loss: 138022.6875\n",
      "Epoch [8430/30000], Loss: 137584.8438\n",
      "Epoch [8440/30000], Loss: 137148.9844\n",
      "Epoch [8450/30000], Loss: 136715.0781\n",
      "Epoch [8460/30000], Loss: 136283.1562\n",
      "Epoch [8470/30000], Loss: 135853.2031\n",
      "Epoch [8480/30000], Loss: 135425.1406\n",
      "Epoch [8490/30000], Loss: 134999.1406\n",
      "Epoch [8500/30000], Loss: 134574.9688\n",
      "Epoch [8510/30000], Loss: 134152.7031\n",
      "Epoch [8520/30000], Loss: 133732.3438\n",
      "Epoch [8530/30000], Loss: 133313.9531\n",
      "Epoch [8540/30000], Loss: 132897.4062\n",
      "Epoch [8550/30000], Loss: 132482.7188\n",
      "Epoch [8560/30000], Loss: 132069.8438\n",
      "Epoch [8570/30000], Loss: 131658.8125\n",
      "Epoch [8580/30000], Loss: 131249.6250\n",
      "Epoch [8590/30000], Loss: 130842.2891\n",
      "Epoch [8600/30000], Loss: 130436.7656\n",
      "Epoch [8610/30000], Loss: 130033.0703\n",
      "Epoch [8620/30000], Loss: 129631.1562\n",
      "Epoch [8630/30000], Loss: 129230.9922\n",
      "Epoch [8640/30000], Loss: 128832.6641\n",
      "Epoch [8650/30000], Loss: 128435.9688\n",
      "Epoch [8660/30000], Loss: 128041.1016\n",
      "Epoch [8670/30000], Loss: 127647.9688\n",
      "Epoch [8680/30000], Loss: 127256.5859\n",
      "Epoch [8690/30000], Loss: 126866.8203\n",
      "Epoch [8700/30000], Loss: 126478.8438\n",
      "Epoch [8710/30000], Loss: 126092.5234\n",
      "Epoch [8720/30000], Loss: 125707.9219\n",
      "Epoch [8730/30000], Loss: 125324.9922\n",
      "Epoch [8740/30000], Loss: 124943.7422\n",
      "Epoch [8750/30000], Loss: 124564.0625\n",
      "Epoch [8760/30000], Loss: 124186.0391\n",
      "Epoch [8770/30000], Loss: 123809.7656\n",
      "Epoch [8780/30000], Loss: 123434.9453\n",
      "Epoch [8790/30000], Loss: 123061.7656\n",
      "Epoch [8800/30000], Loss: 122690.2422\n",
      "Epoch [8810/30000], Loss: 122320.3438\n",
      "Epoch [8820/30000], Loss: 121951.9844\n",
      "Epoch [8830/30000], Loss: 121585.1406\n",
      "Epoch [8840/30000], Loss: 121219.8750\n",
      "Epoch [8850/30000], Loss: 120856.1016\n",
      "Epoch [8860/30000], Loss: 120493.9453\n",
      "Epoch [8870/30000], Loss: 120133.2812\n",
      "Epoch [8880/30000], Loss: 119774.1250\n",
      "Epoch [8890/30000], Loss: 119416.4766\n",
      "Epoch [8900/30000], Loss: 119060.2500\n",
      "Epoch [8910/30000], Loss: 118705.6094\n",
      "Epoch [8920/30000], Loss: 118352.4375\n",
      "Epoch [8930/30000], Loss: 118000.7031\n",
      "Epoch [8940/30000], Loss: 117650.3594\n",
      "Epoch [8950/30000], Loss: 117301.4609\n",
      "Epoch [8960/30000], Loss: 116954.0625\n",
      "Epoch [8970/30000], Loss: 116607.9531\n",
      "Epoch [8980/30000], Loss: 116263.3906\n",
      "Epoch [8990/30000], Loss: 115920.1562\n",
      "Epoch [9000/30000], Loss: 115578.3359\n",
      "Epoch [9010/30000], Loss: 115237.9141\n",
      "Epoch [9020/30000], Loss: 114898.7812\n",
      "Epoch [9030/30000], Loss: 114561.0547\n",
      "Epoch [9040/30000], Loss: 114224.7109\n",
      "Epoch [9050/30000], Loss: 113889.6250\n",
      "Epoch [9060/30000], Loss: 113555.8594\n",
      "Epoch [9070/30000], Loss: 113223.4766\n",
      "Epoch [9080/30000], Loss: 112892.2656\n",
      "Epoch [9090/30000], Loss: 112562.5000\n",
      "Epoch [9100/30000], Loss: 112233.9375\n",
      "Epoch [9110/30000], Loss: 111906.7031\n",
      "Epoch [9120/30000], Loss: 111580.7500\n",
      "Epoch [9130/30000], Loss: 111255.9375\n",
      "Epoch [9140/30000], Loss: 110932.4609\n",
      "Epoch [9150/30000], Loss: 110610.1953\n",
      "Epoch [9160/30000], Loss: 110289.1797\n",
      "Epoch [9170/30000], Loss: 109969.3750\n",
      "Epoch [9180/30000], Loss: 109650.7188\n",
      "Epoch [9190/30000], Loss: 109333.2734\n",
      "Epoch [9200/30000], Loss: 109017.0547\n",
      "Epoch [9210/30000], Loss: 108701.9375\n",
      "Epoch [9220/30000], Loss: 108388.0625\n",
      "Epoch [9230/30000], Loss: 108075.3359\n",
      "Epoch [9240/30000], Loss: 107763.7109\n",
      "Epoch [9250/30000], Loss: 107453.2891\n",
      "Epoch [9260/30000], Loss: 107143.9844\n",
      "Epoch [9270/30000], Loss: 106835.7344\n",
      "Epoch [9280/30000], Loss: 106528.6094\n",
      "Epoch [9290/30000], Loss: 106222.6406\n",
      "Epoch [9300/30000], Loss: 105917.6875\n",
      "Epoch [9310/30000], Loss: 105613.9531\n",
      "Epoch [9320/30000], Loss: 105311.1016\n",
      "Epoch [9330/30000], Loss: 105009.4219\n",
      "Epoch [9340/30000], Loss: 104708.7500\n",
      "Epoch [9350/30000], Loss: 104409.0781\n",
      "Epoch [9360/30000], Loss: 104110.5781\n",
      "Epoch [9370/30000], Loss: 103813.0234\n",
      "Epoch [9380/30000], Loss: 103516.4219\n",
      "Epoch [9390/30000], Loss: 103220.9766\n",
      "Epoch [9400/30000], Loss: 102926.3984\n",
      "Epoch [9410/30000], Loss: 102632.9062\n",
      "Epoch [9420/30000], Loss: 102340.2656\n",
      "Epoch [9430/30000], Loss: 102048.6641\n",
      "Epoch [9440/30000], Loss: 101758.0391\n",
      "Epoch [9450/30000], Loss: 101468.3984\n",
      "Epoch [9460/30000], Loss: 101179.6328\n",
      "Epoch [9470/30000], Loss: 100891.8047\n",
      "Epoch [9480/30000], Loss: 100604.9062\n",
      "Epoch [9490/30000], Loss: 100319.0000\n",
      "Epoch [9500/30000], Loss: 100033.9453\n",
      "Epoch [9510/30000], Loss: 99749.8359\n",
      "Epoch [9520/30000], Loss: 99466.5703\n",
      "Epoch [9530/30000], Loss: 99184.1484\n",
      "Epoch [9540/30000], Loss: 98902.7266\n",
      "Epoch [9550/30000], Loss: 98622.0312\n",
      "Epoch [9560/30000], Loss: 98342.2266\n",
      "Epoch [9570/30000], Loss: 98063.3594\n",
      "Epoch [9580/30000], Loss: 97785.2656\n",
      "Epoch [9590/30000], Loss: 97507.9922\n",
      "Epoch [9600/30000], Loss: 97231.5859\n",
      "Epoch [9610/30000], Loss: 96956.0078\n",
      "Epoch [9620/30000], Loss: 96681.1250\n",
      "Epoch [9630/30000], Loss: 96407.1016\n",
      "Epoch [9640/30000], Loss: 96133.9453\n",
      "Epoch [9650/30000], Loss: 95861.4766\n",
      "Epoch [9660/30000], Loss: 95589.8516\n",
      "Epoch [9670/30000], Loss: 95319.0234\n",
      "Epoch [9680/30000], Loss: 95048.8828\n",
      "Epoch [9690/30000], Loss: 94779.5156\n",
      "Epoch [9700/30000], Loss: 94510.8906\n",
      "Epoch [9710/30000], Loss: 94242.9844\n",
      "Epoch [9720/30000], Loss: 93975.8906\n",
      "Epoch [9730/30000], Loss: 93709.4297\n",
      "Epoch [9740/30000], Loss: 93443.7500\n",
      "Epoch [9750/30000], Loss: 93178.7188\n",
      "Epoch [9760/30000], Loss: 92914.4219\n",
      "Epoch [9770/30000], Loss: 92650.8438\n",
      "Epoch [9780/30000], Loss: 92387.9297\n",
      "Epoch [9790/30000], Loss: 92125.6406\n",
      "Epoch [9800/30000], Loss: 91864.0859\n",
      "Epoch [9810/30000], Loss: 91603.1328\n",
      "Epoch [9820/30000], Loss: 91342.9219\n",
      "Epoch [9830/30000], Loss: 91083.3047\n",
      "Epoch [9840/30000], Loss: 90824.4062\n",
      "Epoch [9850/30000], Loss: 90566.0781\n",
      "Epoch [9860/30000], Loss: 90308.3594\n",
      "Epoch [9870/30000], Loss: 90051.3281\n",
      "Epoch [9880/30000], Loss: 89794.8203\n",
      "Epoch [9890/30000], Loss: 89539.0234\n",
      "Epoch [9900/30000], Loss: 89283.7500\n",
      "Epoch [9910/30000], Loss: 89029.0625\n",
      "Epoch [9920/30000], Loss: 88775.0625\n",
      "Epoch [9930/30000], Loss: 88521.5781\n",
      "Epoch [9940/30000], Loss: 88268.6484\n",
      "Epoch [9950/30000], Loss: 88016.3359\n",
      "Epoch [9960/30000], Loss: 87764.5859\n",
      "Epoch [9970/30000], Loss: 87513.3750\n",
      "Epoch [9980/30000], Loss: 87262.7578\n",
      "Epoch [9990/30000], Loss: 87012.6328\n",
      "Epoch [10000/30000], Loss: 86763.0781\n",
      "Epoch [10010/30000], Loss: 86514.0391\n",
      "Epoch [10020/30000], Loss: 86265.5234\n",
      "Epoch [10030/30000], Loss: 86017.5625\n",
      "Epoch [10040/30000], Loss: 85770.0781\n",
      "Epoch [10050/30000], Loss: 85523.1328\n",
      "Epoch [10060/30000], Loss: 85276.6719\n",
      "Epoch [10070/30000], Loss: 85030.6562\n",
      "Epoch [10080/30000], Loss: 84785.2188\n",
      "Epoch [10090/30000], Loss: 84540.2656\n",
      "Epoch [10100/30000], Loss: 84295.7188\n",
      "Epoch [10110/30000], Loss: 84051.7188\n",
      "Epoch [10120/30000], Loss: 83808.2344\n",
      "Epoch [10130/30000], Loss: 83565.1250\n",
      "Epoch [10140/30000], Loss: 83322.4844\n",
      "Epoch [10150/30000], Loss: 83080.3594\n",
      "Epoch [10160/30000], Loss: 82838.6719\n",
      "Epoch [10170/30000], Loss: 82597.3906\n",
      "Epoch [10180/30000], Loss: 82356.5781\n",
      "Epoch [10190/30000], Loss: 82116.1562\n",
      "Epoch [10200/30000], Loss: 81876.2266\n",
      "Epoch [10210/30000], Loss: 81636.6484\n",
      "Epoch [10220/30000], Loss: 81397.6172\n",
      "Epoch [10230/30000], Loss: 81158.8984\n",
      "Epoch [10240/30000], Loss: 80920.6328\n",
      "Epoch [10250/30000], Loss: 80682.7969\n",
      "Epoch [10260/30000], Loss: 80445.2500\n",
      "Epoch [10270/30000], Loss: 80208.2656\n",
      "Epoch [10280/30000], Loss: 79971.6250\n",
      "Epoch [10290/30000], Loss: 79735.3281\n",
      "Epoch [10300/30000], Loss: 79499.4062\n",
      "Epoch [10310/30000], Loss: 79263.9141\n",
      "Epoch [10320/30000], Loss: 79028.8281\n",
      "Epoch [10330/30000], Loss: 78794.0391\n",
      "Epoch [10340/30000], Loss: 78559.6016\n",
      "Epoch [10350/30000], Loss: 78325.5938\n",
      "Epoch [10360/30000], Loss: 78091.9375\n",
      "Epoch [10370/30000], Loss: 77858.6797\n",
      "Epoch [10380/30000], Loss: 77625.6641\n",
      "Epoch [10390/30000], Loss: 77393.0938\n",
      "Epoch [10400/30000], Loss: 77160.8984\n",
      "Epoch [10410/30000], Loss: 76929.0000\n",
      "Epoch [10420/30000], Loss: 76697.4531\n",
      "Epoch [10430/30000], Loss: 76466.1875\n",
      "Epoch [10440/30000], Loss: 76235.3438\n",
      "Epoch [10450/30000], Loss: 76004.7734\n",
      "Epoch [10460/30000], Loss: 75774.5547\n",
      "Epoch [10470/30000], Loss: 75544.6250\n",
      "Epoch [10480/30000], Loss: 75315.0703\n",
      "Epoch [10490/30000], Loss: 75085.7969\n",
      "Epoch [10500/30000], Loss: 74856.9141\n",
      "Epoch [10510/30000], Loss: 74628.2422\n",
      "Epoch [10520/30000], Loss: 74399.8906\n",
      "Epoch [10530/30000], Loss: 74171.9297\n",
      "Epoch [10540/30000], Loss: 73944.2109\n",
      "Epoch [10550/30000], Loss: 73716.8203\n",
      "Epoch [10560/30000], Loss: 73489.7500\n",
      "Epoch [10570/30000], Loss: 73262.8984\n",
      "Epoch [10580/30000], Loss: 73036.3359\n",
      "Epoch [10590/30000], Loss: 72810.1484\n",
      "Epoch [10600/30000], Loss: 72584.2031\n",
      "Epoch [10610/30000], Loss: 72358.6016\n",
      "Epoch [10620/30000], Loss: 72133.1953\n",
      "Epoch [10630/30000], Loss: 71908.1797\n",
      "Epoch [10640/30000], Loss: 71683.3594\n",
      "Epoch [10650/30000], Loss: 71458.8594\n",
      "Epoch [10660/30000], Loss: 71234.5781\n",
      "Epoch [10670/30000], Loss: 71010.6328\n",
      "Epoch [10680/30000], Loss: 70786.9453\n",
      "Epoch [10690/30000], Loss: 70563.5625\n",
      "Epoch [10700/30000], Loss: 70340.4375\n",
      "Epoch [10710/30000], Loss: 70117.5469\n",
      "Epoch [10720/30000], Loss: 69894.9297\n",
      "Epoch [10730/30000], Loss: 69672.5703\n",
      "Epoch [10740/30000], Loss: 69450.4922\n",
      "Epoch [10750/30000], Loss: 69228.6797\n",
      "Epoch [10760/30000], Loss: 69007.1094\n",
      "Epoch [10770/30000], Loss: 68785.8359\n",
      "Epoch [10780/30000], Loss: 68564.7891\n",
      "Epoch [10790/30000], Loss: 68343.9844\n",
      "Epoch [10800/30000], Loss: 68123.4688\n",
      "Epoch [10810/30000], Loss: 67903.2188\n",
      "Epoch [10820/30000], Loss: 67683.2109\n",
      "Epoch [10830/30000], Loss: 67463.4375\n",
      "Epoch [10840/30000], Loss: 67243.9219\n",
      "Epoch [10850/30000], Loss: 67024.6562\n",
      "Epoch [10860/30000], Loss: 66805.6250\n",
      "Epoch [10870/30000], Loss: 66586.8672\n",
      "Epoch [10880/30000], Loss: 66368.3438\n",
      "Epoch [10890/30000], Loss: 66150.0859\n",
      "Epoch [10900/30000], Loss: 65932.0625\n",
      "Epoch [10910/30000], Loss: 65714.2969\n",
      "Epoch [10920/30000], Loss: 65496.7578\n",
      "Epoch [10930/30000], Loss: 65279.4531\n",
      "Epoch [10940/30000], Loss: 65062.3984\n",
      "Epoch [10950/30000], Loss: 64845.6250\n",
      "Epoch [10960/30000], Loss: 64629.0820\n",
      "Epoch [10970/30000], Loss: 64412.7461\n",
      "Epoch [10980/30000], Loss: 64196.6875\n",
      "Epoch [10990/30000], Loss: 63980.8594\n",
      "Epoch [11000/30000], Loss: 63765.2969\n",
      "Epoch [11010/30000], Loss: 63549.9219\n",
      "Epoch [11020/30000], Loss: 63334.8242\n",
      "Epoch [11030/30000], Loss: 63120.0156\n",
      "Epoch [11040/30000], Loss: 62905.3789\n",
      "Epoch [11050/30000], Loss: 62691.0273\n",
      "Epoch [11060/30000], Loss: 62476.9062\n",
      "Epoch [11070/30000], Loss: 62263.0039\n",
      "Epoch [11080/30000], Loss: 62049.3398\n",
      "Epoch [11090/30000], Loss: 61835.9258\n",
      "Epoch [11100/30000], Loss: 61622.8281\n",
      "Epoch [11110/30000], Loss: 61409.9102\n",
      "Epoch [11120/30000], Loss: 61197.2461\n",
      "Epoch [11130/30000], Loss: 60984.8203\n",
      "Epoch [11140/30000], Loss: 60772.6250\n",
      "Epoch [11150/30000], Loss: 60560.7148\n",
      "Epoch [11160/30000], Loss: 60348.9727\n",
      "Epoch [11170/30000], Loss: 60137.5469\n",
      "Epoch [11180/30000], Loss: 59926.4062\n",
      "Epoch [11190/30000], Loss: 59715.4258\n",
      "Epoch [11200/30000], Loss: 59504.7305\n",
      "Epoch [11210/30000], Loss: 59294.2852\n",
      "Epoch [11220/30000], Loss: 59084.0664\n",
      "Epoch [11230/30000], Loss: 58874.1016\n",
      "Epoch [11240/30000], Loss: 58664.4023\n",
      "Epoch [11250/30000], Loss: 58454.9688\n",
      "Epoch [11260/30000], Loss: 58245.7461\n",
      "Epoch [11270/30000], Loss: 58036.8008\n",
      "Epoch [11280/30000], Loss: 57828.1328\n",
      "Epoch [11290/30000], Loss: 57619.7031\n",
      "Epoch [11300/30000], Loss: 57411.5039\n",
      "Epoch [11310/30000], Loss: 57203.5586\n",
      "Epoch [11320/30000], Loss: 56995.9219\n",
      "Epoch [11330/30000], Loss: 56788.4844\n",
      "Epoch [11340/30000], Loss: 56581.3594\n",
      "Epoch [11350/30000], Loss: 56374.4414\n",
      "Epoch [11360/30000], Loss: 56167.8203\n",
      "Epoch [11370/30000], Loss: 55961.4336\n",
      "Epoch [11380/30000], Loss: 55755.3633\n",
      "Epoch [11390/30000], Loss: 55549.4961\n",
      "Epoch [11400/30000], Loss: 55343.9414\n",
      "Epoch [11410/30000], Loss: 55138.6523\n",
      "Epoch [11420/30000], Loss: 54933.6445\n",
      "Epoch [11430/30000], Loss: 54728.8867\n",
      "Epoch [11440/30000], Loss: 54524.3242\n",
      "Epoch [11450/30000], Loss: 54320.1328\n",
      "Epoch [11460/30000], Loss: 54116.1719\n",
      "Epoch [11470/30000], Loss: 53912.5312\n",
      "Epoch [11480/30000], Loss: 53709.1250\n",
      "Epoch [11490/30000], Loss: 53506.0156\n",
      "Epoch [11500/30000], Loss: 53303.1680\n",
      "Epoch [11510/30000], Loss: 53100.6406\n",
      "Epoch [11520/30000], Loss: 52898.3359\n",
      "Epoch [11530/30000], Loss: 52696.3789\n",
      "Epoch [11540/30000], Loss: 52494.6992\n",
      "Epoch [11550/30000], Loss: 52293.2422\n",
      "Epoch [11560/30000], Loss: 52092.1211\n",
      "Epoch [11570/30000], Loss: 51891.2930\n",
      "Epoch [11580/30000], Loss: 51690.7305\n",
      "Epoch [11590/30000], Loss: 51490.4922\n",
      "Epoch [11600/30000], Loss: 51290.5469\n",
      "Epoch [11610/30000], Loss: 51090.8438\n",
      "Epoch [11620/30000], Loss: 50891.4805\n",
      "Epoch [11630/30000], Loss: 50692.4375\n",
      "Epoch [11640/30000], Loss: 50493.7031\n",
      "Epoch [11650/30000], Loss: 50295.2344\n",
      "Epoch [11660/30000], Loss: 50097.0625\n",
      "Epoch [11670/30000], Loss: 49899.1719\n",
      "Epoch [11680/30000], Loss: 49701.6328\n",
      "Epoch [11690/30000], Loss: 49504.4023\n",
      "Epoch [11700/30000], Loss: 49307.4844\n",
      "Epoch [11710/30000], Loss: 49110.8906\n",
      "Epoch [11720/30000], Loss: 48914.5469\n",
      "Epoch [11730/30000], Loss: 48718.5781\n",
      "Epoch [11740/30000], Loss: 48522.8945\n",
      "Epoch [11750/30000], Loss: 48327.5859\n",
      "Epoch [11760/30000], Loss: 48132.5664\n",
      "Epoch [11770/30000], Loss: 47937.8398\n",
      "Epoch [11780/30000], Loss: 47743.4648\n",
      "Epoch [11790/30000], Loss: 47549.4258\n",
      "Epoch [11800/30000], Loss: 47355.7109\n",
      "Epoch [11810/30000], Loss: 47162.3125\n",
      "Epoch [11820/30000], Loss: 46969.2383\n",
      "Epoch [11830/30000], Loss: 46776.5352\n",
      "Epoch [11840/30000], Loss: 46584.1172\n",
      "Epoch [11850/30000], Loss: 46392.1016\n",
      "Epoch [11860/30000], Loss: 46200.3672\n",
      "Epoch [11870/30000], Loss: 46008.9609\n",
      "Epoch [11880/30000], Loss: 45817.9336\n",
      "Epoch [11890/30000], Loss: 45627.2305\n",
      "Epoch [11900/30000], Loss: 45436.9062\n",
      "Epoch [11910/30000], Loss: 45246.9102\n",
      "Epoch [11920/30000], Loss: 45057.2148\n",
      "Epoch [11930/30000], Loss: 44867.9609\n",
      "Epoch [11940/30000], Loss: 44679.0156\n",
      "Epoch [11950/30000], Loss: 44490.3867\n",
      "Epoch [11960/30000], Loss: 44302.1836\n",
      "Epoch [11970/30000], Loss: 44114.3242\n",
      "Epoch [11980/30000], Loss: 43926.7578\n",
      "Epoch [11990/30000], Loss: 43739.6172\n",
      "Epoch [12000/30000], Loss: 43552.8047\n",
      "Epoch [12010/30000], Loss: 43366.4375\n",
      "Epoch [12020/30000], Loss: 43180.3711\n",
      "Epoch [12030/30000], Loss: 42994.7070\n",
      "Epoch [12040/30000], Loss: 42809.3555\n",
      "Epoch [12050/30000], Loss: 42624.3555\n",
      "Epoch [12060/30000], Loss: 42439.8242\n",
      "Epoch [12070/30000], Loss: 42255.6211\n",
      "Epoch [12080/30000], Loss: 42071.7969\n",
      "Epoch [12090/30000], Loss: 41888.3672\n",
      "Epoch [12100/30000], Loss: 41705.2734\n",
      "Epoch [12110/30000], Loss: 41522.6172\n",
      "Epoch [12120/30000], Loss: 41340.3086\n",
      "Epoch [12130/30000], Loss: 41158.4023\n",
      "Epoch [12140/30000], Loss: 40976.8828\n",
      "Epoch [12150/30000], Loss: 40795.7461\n",
      "Epoch [12160/30000], Loss: 40614.9922\n",
      "Epoch [12170/30000], Loss: 40434.6602\n",
      "Epoch [12180/30000], Loss: 40254.6953\n",
      "Epoch [12190/30000], Loss: 40075.1836\n",
      "Epoch [12200/30000], Loss: 39895.9297\n",
      "Epoch [12210/30000], Loss: 39717.1992\n",
      "Epoch [12220/30000], Loss: 39538.8477\n",
      "Epoch [12230/30000], Loss: 39360.8750\n",
      "Epoch [12240/30000], Loss: 39183.3086\n",
      "Epoch [12250/30000], Loss: 39006.1602\n",
      "Epoch [12260/30000], Loss: 38829.4062\n",
      "Epoch [12270/30000], Loss: 38653.0820\n",
      "Epoch [12280/30000], Loss: 38477.1172\n",
      "Epoch [12290/30000], Loss: 38301.6055\n",
      "Epoch [12300/30000], Loss: 38126.5312\n",
      "Epoch [12310/30000], Loss: 37951.8398\n",
      "Epoch [12320/30000], Loss: 37777.5469\n",
      "Epoch [12330/30000], Loss: 37603.7070\n",
      "Epoch [12340/30000], Loss: 37430.2734\n",
      "Epoch [12350/30000], Loss: 37257.2344\n",
      "Epoch [12360/30000], Loss: 37084.6484\n",
      "Epoch [12370/30000], Loss: 36912.4961\n",
      "Epoch [12380/30000], Loss: 36740.7500\n",
      "Epoch [12390/30000], Loss: 36569.4727\n",
      "Epoch [12400/30000], Loss: 36398.5664\n",
      "Epoch [12410/30000], Loss: 36228.1055\n",
      "Epoch [12420/30000], Loss: 36058.0508\n",
      "Epoch [12430/30000], Loss: 35888.4570\n",
      "Epoch [12440/30000], Loss: 35719.2812\n",
      "Epoch [12450/30000], Loss: 35550.5781\n",
      "Epoch [12460/30000], Loss: 35382.2734\n",
      "Epoch [12470/30000], Loss: 35214.4219\n",
      "Epoch [12480/30000], Loss: 35047.0078\n",
      "Epoch [12490/30000], Loss: 34880.0195\n",
      "Epoch [12500/30000], Loss: 34713.4727\n",
      "Epoch [12510/30000], Loss: 34547.3711\n",
      "Epoch [12520/30000], Loss: 34381.7148\n",
      "Epoch [12530/30000], Loss: 34216.5664\n",
      "Epoch [12540/30000], Loss: 34051.7930\n",
      "Epoch [12550/30000], Loss: 33887.4844\n",
      "Epoch [12560/30000], Loss: 33723.5977\n",
      "Epoch [12570/30000], Loss: 33560.2148\n",
      "Epoch [12580/30000], Loss: 33397.2266\n",
      "Epoch [12590/30000], Loss: 33234.7305\n",
      "Epoch [12600/30000], Loss: 33072.6758\n",
      "Epoch [12610/30000], Loss: 32911.0742\n",
      "Epoch [12620/30000], Loss: 32749.9199\n",
      "Epoch [12630/30000], Loss: 32589.2461\n",
      "Epoch [12640/30000], Loss: 32429.0547\n",
      "Epoch [12650/30000], Loss: 32269.2305\n",
      "Epoch [12660/30000], Loss: 32109.9492\n",
      "Epoch [12670/30000], Loss: 31951.0820\n",
      "Epoch [12680/30000], Loss: 31792.6875\n",
      "Epoch [12690/30000], Loss: 31634.7832\n",
      "Epoch [12700/30000], Loss: 31477.3242\n",
      "Epoch [12710/30000], Loss: 31320.3418\n",
      "Epoch [12720/30000], Loss: 31163.7832\n",
      "Epoch [12730/30000], Loss: 31007.7090\n",
      "Epoch [12740/30000], Loss: 30852.1484\n",
      "Epoch [12750/30000], Loss: 30697.0449\n",
      "Epoch [12760/30000], Loss: 30542.3633\n",
      "Epoch [12770/30000], Loss: 30388.2051\n",
      "Epoch [12780/30000], Loss: 30234.4570\n",
      "Epoch [12790/30000], Loss: 30081.2168\n",
      "Epoch [12800/30000], Loss: 29928.4453\n",
      "Epoch [12810/30000], Loss: 29776.1680\n",
      "Epoch [12820/30000], Loss: 29624.3320\n",
      "Epoch [12830/30000], Loss: 29472.9824\n",
      "Epoch [12840/30000], Loss: 29322.0996\n",
      "Epoch [12850/30000], Loss: 29171.7188\n",
      "Epoch [12860/30000], Loss: 29021.7871\n",
      "Epoch [12870/30000], Loss: 28872.3496\n",
      "Epoch [12880/30000], Loss: 28723.3965\n",
      "Epoch [12890/30000], Loss: 28574.9160\n",
      "Epoch [12900/30000], Loss: 28426.9141\n",
      "Epoch [12910/30000], Loss: 28279.3672\n",
      "Epoch [12920/30000], Loss: 28132.3535\n",
      "Epoch [12930/30000], Loss: 27985.8047\n",
      "Epoch [12940/30000], Loss: 27839.7246\n",
      "Epoch [12950/30000], Loss: 27694.1641\n",
      "Epoch [12960/30000], Loss: 27549.0605\n",
      "Epoch [12970/30000], Loss: 27404.4238\n",
      "Epoch [12980/30000], Loss: 27260.2988\n",
      "Epoch [12990/30000], Loss: 27116.6406\n",
      "Epoch [13000/30000], Loss: 26973.4766\n",
      "Epoch [13010/30000], Loss: 26830.7754\n",
      "Epoch [13020/30000], Loss: 26688.5840\n",
      "Epoch [13030/30000], Loss: 26546.8984\n",
      "Epoch [13040/30000], Loss: 26405.6797\n",
      "Epoch [13050/30000], Loss: 26264.9414\n",
      "Epoch [13060/30000], Loss: 26124.7031\n",
      "Epoch [13070/30000], Loss: 25984.9414\n",
      "Epoch [13080/30000], Loss: 25845.6836\n",
      "Epoch [13090/30000], Loss: 25706.9238\n",
      "Epoch [13100/30000], Loss: 25568.6523\n",
      "Epoch [13110/30000], Loss: 25430.8516\n",
      "Epoch [13120/30000], Loss: 25293.5645\n",
      "Epoch [13130/30000], Loss: 25156.7207\n",
      "Epoch [13140/30000], Loss: 25020.4180\n",
      "Epoch [13150/30000], Loss: 24884.5762\n",
      "Epoch [13160/30000], Loss: 24749.2441\n",
      "Epoch [13170/30000], Loss: 24614.4102\n",
      "Epoch [13180/30000], Loss: 24480.0352\n",
      "Epoch [13190/30000], Loss: 24346.1465\n",
      "Epoch [13200/30000], Loss: 24212.7871\n",
      "Epoch [13210/30000], Loss: 24079.9395\n",
      "Epoch [13220/30000], Loss: 23947.5156\n",
      "Epoch [13230/30000], Loss: 23815.6113\n",
      "Epoch [13240/30000], Loss: 23684.2227\n",
      "Epoch [13250/30000], Loss: 23553.3359\n",
      "Epoch [13260/30000], Loss: 23422.8887\n",
      "Epoch [13270/30000], Loss: 23292.9746\n",
      "Epoch [13280/30000], Loss: 23163.5566\n",
      "Epoch [13290/30000], Loss: 23034.5977\n",
      "Epoch [13300/30000], Loss: 22906.1504\n",
      "Epoch [13310/30000], Loss: 22778.2285\n",
      "Epoch [13320/30000], Loss: 22650.7363\n",
      "Epoch [13330/30000], Loss: 22523.7461\n",
      "Epoch [13340/30000], Loss: 22397.3184\n",
      "Epoch [13350/30000], Loss: 22271.3457\n",
      "Epoch [13360/30000], Loss: 22145.8496\n",
      "Epoch [13370/30000], Loss: 22020.8281\n",
      "Epoch [13380/30000], Loss: 21896.3262\n",
      "Epoch [13390/30000], Loss: 21772.3359\n",
      "Epoch [13400/30000], Loss: 21648.7910\n",
      "Epoch [13410/30000], Loss: 21525.7734\n",
      "Epoch [13420/30000], Loss: 21403.2305\n",
      "Epoch [13430/30000], Loss: 21281.1680\n",
      "Epoch [13440/30000], Loss: 21159.6348\n",
      "Epoch [13450/30000], Loss: 21038.5508\n",
      "Epoch [13460/30000], Loss: 20917.9805\n",
      "Epoch [13470/30000], Loss: 20797.8945\n",
      "Epoch [13480/30000], Loss: 20678.3047\n",
      "Epoch [13490/30000], Loss: 20559.1992\n",
      "Epoch [13500/30000], Loss: 20440.5957\n",
      "Epoch [13510/30000], Loss: 20322.4785\n",
      "Epoch [13520/30000], Loss: 20204.8164\n",
      "Epoch [13530/30000], Loss: 20087.6816\n",
      "Epoch [13540/30000], Loss: 19971.0293\n",
      "Epoch [13550/30000], Loss: 19854.8340\n",
      "Epoch [13560/30000], Loss: 19739.1602\n",
      "Epoch [13570/30000], Loss: 19623.9453\n",
      "Epoch [13580/30000], Loss: 19509.2559\n",
      "Epoch [13590/30000], Loss: 19395.0117\n",
      "Epoch [13600/30000], Loss: 19281.2949\n",
      "Epoch [13610/30000], Loss: 19168.0293\n",
      "Epoch [13620/30000], Loss: 19055.2793\n",
      "Epoch [13630/30000], Loss: 18942.9844\n",
      "Epoch [13640/30000], Loss: 18831.1973\n",
      "Epoch [13650/30000], Loss: 18719.8652\n",
      "Epoch [13660/30000], Loss: 18609.0293\n",
      "Epoch [13670/30000], Loss: 18498.6816\n",
      "Epoch [13680/30000], Loss: 18388.7930\n",
      "Epoch [13690/30000], Loss: 18279.4102\n",
      "Epoch [13700/30000], Loss: 18170.5176\n",
      "Epoch [13710/30000], Loss: 18062.0742\n",
      "Epoch [13720/30000], Loss: 17954.1270\n",
      "Epoch [13730/30000], Loss: 17846.6582\n",
      "Epoch [13740/30000], Loss: 17739.6660\n",
      "Epoch [13750/30000], Loss: 17633.1387\n",
      "Epoch [13760/30000], Loss: 17527.0879\n",
      "Epoch [13770/30000], Loss: 17421.5234\n",
      "Epoch [13780/30000], Loss: 17316.4375\n",
      "Epoch [13790/30000], Loss: 17211.8301\n",
      "Epoch [13800/30000], Loss: 17107.6699\n",
      "Epoch [13810/30000], Loss: 17004.0078\n",
      "Epoch [13820/30000], Loss: 16900.7969\n",
      "Epoch [13830/30000], Loss: 16798.0840\n",
      "Epoch [13840/30000], Loss: 16695.8457\n",
      "Epoch [13850/30000], Loss: 16594.0527\n",
      "Epoch [13860/30000], Loss: 16492.7305\n",
      "Epoch [13870/30000], Loss: 16391.8711\n",
      "Epoch [13880/30000], Loss: 16291.5176\n",
      "Epoch [13890/30000], Loss: 16191.6113\n",
      "Epoch [13900/30000], Loss: 16092.1768\n",
      "Epoch [13910/30000], Loss: 15993.1865\n",
      "Epoch [13920/30000], Loss: 15894.6719\n",
      "Epoch [13930/30000], Loss: 15796.6309\n",
      "Epoch [13940/30000], Loss: 15699.0410\n",
      "Epoch [13950/30000], Loss: 15601.9180\n",
      "Epoch [13960/30000], Loss: 15505.2451\n",
      "Epoch [13970/30000], Loss: 15409.0518\n",
      "Epoch [13980/30000], Loss: 15313.3223\n",
      "Epoch [13990/30000], Loss: 15218.0361\n",
      "Epoch [14000/30000], Loss: 15123.2012\n",
      "Epoch [14010/30000], Loss: 15028.8271\n",
      "Epoch [14020/30000], Loss: 14934.9258\n",
      "Epoch [14030/30000], Loss: 14841.4561\n",
      "Epoch [14040/30000], Loss: 14748.4561\n",
      "Epoch [14050/30000], Loss: 14655.9160\n",
      "Epoch [14060/30000], Loss: 14563.8223\n",
      "Epoch [14070/30000], Loss: 14472.1602\n",
      "Epoch [14080/30000], Loss: 14380.9746\n",
      "Epoch [14090/30000], Loss: 14290.2148\n",
      "Epoch [14100/30000], Loss: 14199.9170\n",
      "Epoch [14110/30000], Loss: 14110.0479\n",
      "Epoch [14120/30000], Loss: 14020.6562\n",
      "Epoch [14130/30000], Loss: 13931.6895\n",
      "Epoch [14140/30000], Loss: 13843.1602\n",
      "Epoch [14150/30000], Loss: 13755.0918\n",
      "Epoch [14160/30000], Loss: 13667.4512\n",
      "Epoch [14170/30000], Loss: 13580.2490\n",
      "Epoch [14180/30000], Loss: 13493.4883\n",
      "Epoch [14190/30000], Loss: 13407.1670\n",
      "Epoch [14200/30000], Loss: 13321.2930\n",
      "Epoch [14210/30000], Loss: 13235.8574\n",
      "Epoch [14220/30000], Loss: 13150.8320\n",
      "Epoch [14230/30000], Loss: 13066.2510\n",
      "Epoch [14240/30000], Loss: 12982.1191\n",
      "Epoch [14250/30000], Loss: 12898.4150\n",
      "Epoch [14260/30000], Loss: 12815.1406\n",
      "Epoch [14270/30000], Loss: 12732.2686\n",
      "Epoch [14280/30000], Loss: 12649.8408\n",
      "Epoch [14290/30000], Loss: 12567.8555\n",
      "Epoch [14300/30000], Loss: 12486.2705\n",
      "Epoch [14310/30000], Loss: 12405.1465\n",
      "Epoch [14320/30000], Loss: 12324.4365\n",
      "Epoch [14330/30000], Loss: 12244.1201\n",
      "Epoch [14340/30000], Loss: 12164.2393\n",
      "Epoch [14350/30000], Loss: 12084.7832\n",
      "Epoch [14360/30000], Loss: 12005.7471\n",
      "Epoch [14370/30000], Loss: 11927.1221\n",
      "Epoch [14380/30000], Loss: 11848.9150\n",
      "Epoch [14390/30000], Loss: 11771.1455\n",
      "Epoch [14400/30000], Loss: 11693.7451\n",
      "Epoch [14410/30000], Loss: 11616.7852\n",
      "Epoch [14420/30000], Loss: 11540.2383\n",
      "Epoch [14430/30000], Loss: 11464.0879\n",
      "Epoch [14440/30000], Loss: 11388.3525\n",
      "Epoch [14450/30000], Loss: 11313.0391\n",
      "Epoch [14460/30000], Loss: 11238.1094\n",
      "Epoch [14470/30000], Loss: 11163.5918\n",
      "Epoch [14480/30000], Loss: 11089.4854\n",
      "Epoch [14490/30000], Loss: 11015.7812\n",
      "Epoch [14500/30000], Loss: 10942.4697\n",
      "Epoch [14510/30000], Loss: 10869.5596\n",
      "Epoch [14520/30000], Loss: 10797.0508\n",
      "Epoch [14530/30000], Loss: 10724.9434\n",
      "Epoch [14540/30000], Loss: 10653.2285\n",
      "Epoch [14550/30000], Loss: 10581.9189\n",
      "Epoch [14560/30000], Loss: 10510.9805\n",
      "Epoch [14570/30000], Loss: 10440.4512\n",
      "Epoch [14580/30000], Loss: 10370.3096\n",
      "Epoch [14590/30000], Loss: 10300.5684\n",
      "Epoch [14600/30000], Loss: 10231.1865\n",
      "Epoch [14610/30000], Loss: 10162.2139\n",
      "Epoch [14620/30000], Loss: 10093.6260\n",
      "Epoch [14630/30000], Loss: 10025.4102\n",
      "Epoch [14640/30000], Loss: 9957.5840\n",
      "Epoch [14650/30000], Loss: 9890.1455\n",
      "Epoch [14660/30000], Loss: 9823.0752\n",
      "Epoch [14670/30000], Loss: 9756.3926\n",
      "Epoch [14680/30000], Loss: 9690.0732\n",
      "Epoch [14690/30000], Loss: 9624.1406\n",
      "Epoch [14700/30000], Loss: 9558.5889\n",
      "Epoch [14710/30000], Loss: 9493.4053\n",
      "Epoch [14720/30000], Loss: 9428.5947\n",
      "Epoch [14730/30000], Loss: 9364.1602\n",
      "Epoch [14740/30000], Loss: 9300.0889\n",
      "Epoch [14750/30000], Loss: 9236.3799\n",
      "Epoch [14760/30000], Loss: 9173.0449\n",
      "Epoch [14770/30000], Loss: 9110.0684\n",
      "Epoch [14780/30000], Loss: 9047.4639\n",
      "Epoch [14790/30000], Loss: 8985.2148\n",
      "Epoch [14800/30000], Loss: 8923.3418\n",
      "Epoch [14810/30000], Loss: 8861.8105\n",
      "Epoch [14820/30000], Loss: 8800.6338\n",
      "Epoch [14830/30000], Loss: 8739.8320\n",
      "Epoch [14840/30000], Loss: 8679.3789\n",
      "Epoch [14850/30000], Loss: 8619.2822\n",
      "Epoch [14860/30000], Loss: 8559.5273\n",
      "Epoch [14870/30000], Loss: 8500.1348\n",
      "Epoch [14880/30000], Loss: 8441.0801\n",
      "Epoch [14890/30000], Loss: 8382.3877\n",
      "Epoch [14900/30000], Loss: 8324.0312\n",
      "Epoch [14910/30000], Loss: 8266.0303\n",
      "Epoch [14920/30000], Loss: 8208.3682\n",
      "Epoch [14930/30000], Loss: 8151.0542\n",
      "Epoch [14940/30000], Loss: 8094.0752\n",
      "Epoch [14950/30000], Loss: 8037.4399\n",
      "Epoch [14960/30000], Loss: 7981.1431\n",
      "Epoch [14970/30000], Loss: 7925.1860\n",
      "Epoch [14980/30000], Loss: 7869.5547\n",
      "Epoch [14990/30000], Loss: 7814.2593\n",
      "Epoch [15000/30000], Loss: 7759.3042\n",
      "Epoch [15010/30000], Loss: 7704.6851\n",
      "Epoch [15020/30000], Loss: 7650.3794\n",
      "Epoch [15030/30000], Loss: 7596.4165\n",
      "Epoch [15040/30000], Loss: 7542.7793\n",
      "Epoch [15050/30000], Loss: 7489.4766\n",
      "Epoch [15060/30000], Loss: 7436.4858\n",
      "Epoch [15070/30000], Loss: 7383.8301\n",
      "Epoch [15080/30000], Loss: 7331.4746\n",
      "Epoch [15090/30000], Loss: 7279.4478\n",
      "Epoch [15100/30000], Loss: 7227.7437\n",
      "Epoch [15110/30000], Loss: 7176.3643\n",
      "Epoch [15120/30000], Loss: 7125.2974\n",
      "Epoch [15130/30000], Loss: 7074.5347\n",
      "Epoch [15140/30000], Loss: 7024.0938\n",
      "Epoch [15150/30000], Loss: 6973.9688\n",
      "Epoch [15160/30000], Loss: 6924.1553\n",
      "Epoch [15170/30000], Loss: 6874.6406\n",
      "Epoch [15180/30000], Loss: 6825.4385\n",
      "Epoch [15190/30000], Loss: 6776.5356\n",
      "Epoch [15200/30000], Loss: 6727.9556\n",
      "Epoch [15210/30000], Loss: 6679.6699\n",
      "Epoch [15220/30000], Loss: 6631.6787\n",
      "Epoch [15230/30000], Loss: 6583.9941\n",
      "Epoch [15240/30000], Loss: 6536.6172\n",
      "Epoch [15250/30000], Loss: 6489.5259\n",
      "Epoch [15260/30000], Loss: 6442.7397\n",
      "Epoch [15270/30000], Loss: 6396.2334\n",
      "Epoch [15280/30000], Loss: 6350.0356\n",
      "Epoch [15290/30000], Loss: 6304.1362\n",
      "Epoch [15300/30000], Loss: 6258.5151\n",
      "Epoch [15310/30000], Loss: 6213.1841\n",
      "Epoch [15320/30000], Loss: 6168.1421\n",
      "Epoch [15330/30000], Loss: 6123.3896\n",
      "Epoch [15340/30000], Loss: 6078.9297\n",
      "Epoch [15350/30000], Loss: 6034.7383\n",
      "Epoch [15360/30000], Loss: 5990.8394\n",
      "Epoch [15370/30000], Loss: 5947.2207\n",
      "Epoch [15380/30000], Loss: 5903.8818\n",
      "Epoch [15390/30000], Loss: 5860.8213\n",
      "Epoch [15400/30000], Loss: 5818.0410\n",
      "Epoch [15410/30000], Loss: 5775.5386\n",
      "Epoch [15420/30000], Loss: 5733.2969\n",
      "Epoch [15430/30000], Loss: 5691.3472\n",
      "Epoch [15440/30000], Loss: 5649.6567\n",
      "Epoch [15450/30000], Loss: 5608.2378\n",
      "Epoch [15460/30000], Loss: 5567.0894\n",
      "Epoch [15470/30000], Loss: 5526.2095\n",
      "Epoch [15480/30000], Loss: 5485.5996\n",
      "Epoch [15490/30000], Loss: 5445.2490\n",
      "Epoch [15500/30000], Loss: 5405.1641\n",
      "Epoch [15510/30000], Loss: 5365.3457\n",
      "Epoch [15520/30000], Loss: 5325.7837\n",
      "Epoch [15530/30000], Loss: 5286.4878\n",
      "Epoch [15540/30000], Loss: 5247.4380\n",
      "Epoch [15550/30000], Loss: 5208.6538\n",
      "Epoch [15560/30000], Loss: 5170.1162\n",
      "Epoch [15570/30000], Loss: 5131.8477\n",
      "Epoch [15580/30000], Loss: 5093.8223\n",
      "Epoch [15590/30000], Loss: 5056.0635\n",
      "Epoch [15600/30000], Loss: 5018.5366\n",
      "Epoch [15610/30000], Loss: 4981.2686\n",
      "Epoch [15620/30000], Loss: 4944.2437\n",
      "Epoch [15630/30000], Loss: 4907.4561\n",
      "Epoch [15640/30000], Loss: 4870.9297\n",
      "Epoch [15650/30000], Loss: 4834.6489\n",
      "Epoch [15660/30000], Loss: 4798.5986\n",
      "Epoch [15670/30000], Loss: 4762.7983\n",
      "Epoch [15680/30000], Loss: 4727.2339\n",
      "Epoch [15690/30000], Loss: 4691.9092\n",
      "Epoch [15700/30000], Loss: 4656.8223\n",
      "Epoch [15710/30000], Loss: 4621.9707\n",
      "Epoch [15720/30000], Loss: 4587.3511\n",
      "Epoch [15730/30000], Loss: 4552.9731\n",
      "Epoch [15740/30000], Loss: 4518.8169\n",
      "Epoch [15750/30000], Loss: 4484.9062\n",
      "Epoch [15760/30000], Loss: 4451.2144\n",
      "Epoch [15770/30000], Loss: 4417.7480\n",
      "Epoch [15780/30000], Loss: 4384.5269\n",
      "Epoch [15790/30000], Loss: 4351.5127\n",
      "Epoch [15800/30000], Loss: 4318.7236\n",
      "Epoch [15810/30000], Loss: 4286.1685\n",
      "Epoch [15820/30000], Loss: 4253.8354\n",
      "Epoch [15830/30000], Loss: 4221.7217\n",
      "Epoch [15840/30000], Loss: 4189.8213\n",
      "Epoch [15850/30000], Loss: 4158.1489\n",
      "Epoch [15860/30000], Loss: 4126.6812\n",
      "Epoch [15870/30000], Loss: 4095.4429\n",
      "Epoch [15880/30000], Loss: 4064.4175\n",
      "Epoch [15890/30000], Loss: 4033.5999\n",
      "Epoch [15900/30000], Loss: 4002.9949\n",
      "Epoch [15910/30000], Loss: 3972.6062\n",
      "Epoch [15920/30000], Loss: 3942.4265\n",
      "Epoch [15930/30000], Loss: 3912.4541\n",
      "Epoch [15940/30000], Loss: 3882.6851\n",
      "Epoch [15950/30000], Loss: 3853.1343\n",
      "Epoch [15960/30000], Loss: 3823.7803\n",
      "Epoch [15970/30000], Loss: 3794.6321\n",
      "Epoch [15980/30000], Loss: 3765.6846\n",
      "Epoch [15990/30000], Loss: 3736.9446\n",
      "Epoch [16000/30000], Loss: 3708.4019\n",
      "Epoch [16010/30000], Loss: 3680.0571\n",
      "Epoch [16020/30000], Loss: 3651.9094\n",
      "Epoch [16030/30000], Loss: 3623.9619\n",
      "Epoch [16040/30000], Loss: 3596.2117\n",
      "Epoch [16050/30000], Loss: 3568.6536\n",
      "Epoch [16060/30000], Loss: 3541.2922\n",
      "Epoch [16070/30000], Loss: 3514.1218\n",
      "Epoch [16080/30000], Loss: 3487.1377\n",
      "Epoch [16090/30000], Loss: 3460.3511\n",
      "Epoch [16100/30000], Loss: 3433.7532\n",
      "Epoch [16110/30000], Loss: 3407.3457\n",
      "Epoch [16120/30000], Loss: 3381.1143\n",
      "Epoch [16130/30000], Loss: 3355.0771\n",
      "Epoch [16140/30000], Loss: 3329.2268\n",
      "Epoch [16150/30000], Loss: 3303.5518\n",
      "Epoch [16160/30000], Loss: 3278.0635\n",
      "Epoch [16170/30000], Loss: 3252.7563\n",
      "Epoch [16180/30000], Loss: 3227.6267\n",
      "Epoch [16190/30000], Loss: 3202.6785\n",
      "Epoch [16200/30000], Loss: 3177.9170\n",
      "Epoch [16210/30000], Loss: 3153.3196\n",
      "Epoch [16220/30000], Loss: 3128.9048\n",
      "Epoch [16230/30000], Loss: 3104.6621\n",
      "Epoch [16240/30000], Loss: 3080.5955\n",
      "Epoch [16250/30000], Loss: 3056.6990\n",
      "Epoch [16260/30000], Loss: 3032.9758\n",
      "Epoch [16270/30000], Loss: 3009.4250\n",
      "Epoch [16280/30000], Loss: 2986.0435\n",
      "Epoch [16290/30000], Loss: 2962.8235\n",
      "Epoch [16300/30000], Loss: 2939.7766\n",
      "Epoch [16310/30000], Loss: 2916.8965\n",
      "Epoch [16320/30000], Loss: 2894.1768\n",
      "Epoch [16330/30000], Loss: 2871.6313\n",
      "Epoch [16340/30000], Loss: 2849.2400\n",
      "Epoch [16350/30000], Loss: 2827.0134\n",
      "Epoch [16360/30000], Loss: 2804.9495\n",
      "Epoch [16370/30000], Loss: 2783.0442\n",
      "Epoch [16380/30000], Loss: 2761.3057\n",
      "Epoch [16390/30000], Loss: 2739.7148\n",
      "Epoch [16400/30000], Loss: 2718.2876\n",
      "Epoch [16410/30000], Loss: 2697.0149\n",
      "Epoch [16420/30000], Loss: 2675.9014\n",
      "Epoch [16430/30000], Loss: 2654.9319\n",
      "Epoch [16440/30000], Loss: 2634.1260\n",
      "Epoch [16450/30000], Loss: 2613.4673\n",
      "Epoch [16460/30000], Loss: 2592.9590\n",
      "Epoch [16470/30000], Loss: 2572.6001\n",
      "Epoch [16480/30000], Loss: 2552.3918\n",
      "Epoch [16490/30000], Loss: 2532.3376\n",
      "Epoch [16500/30000], Loss: 2512.4258\n",
      "Epoch [16510/30000], Loss: 2492.6636\n",
      "Epoch [16520/30000], Loss: 2473.0466\n",
      "Epoch [16530/30000], Loss: 2453.5696\n",
      "Epoch [16540/30000], Loss: 2434.2410\n",
      "Epoch [16550/30000], Loss: 2415.0544\n",
      "Epoch [16560/30000], Loss: 2396.0085\n",
      "Epoch [16570/30000], Loss: 2377.1003\n",
      "Epoch [16580/30000], Loss: 2358.3479\n",
      "Epoch [16590/30000], Loss: 2339.7166\n",
      "Epoch [16600/30000], Loss: 2321.2283\n",
      "Epoch [16610/30000], Loss: 2302.8826\n",
      "Epoch [16620/30000], Loss: 2284.6667\n",
      "Epoch [16630/30000], Loss: 2266.5923\n",
      "Epoch [16640/30000], Loss: 2248.6489\n",
      "Epoch [16650/30000], Loss: 2230.8376\n",
      "Epoch [16660/30000], Loss: 2213.1658\n",
      "Epoch [16670/30000], Loss: 2195.6157\n",
      "Epoch [16680/30000], Loss: 2178.2087\n",
      "Epoch [16690/30000], Loss: 2160.9231\n",
      "Epoch [16700/30000], Loss: 2143.7729\n",
      "Epoch [16710/30000], Loss: 2126.7507\n",
      "Epoch [16720/30000], Loss: 2109.8538\n",
      "Epoch [16730/30000], Loss: 2093.0781\n",
      "Epoch [16740/30000], Loss: 2076.4329\n",
      "Epoch [16750/30000], Loss: 2059.9143\n",
      "Epoch [16760/30000], Loss: 2043.5253\n",
      "Epoch [16770/30000], Loss: 2027.2484\n",
      "Epoch [16780/30000], Loss: 2011.1022\n",
      "Epoch [16790/30000], Loss: 1995.0796\n",
      "Epoch [16800/30000], Loss: 1979.1729\n",
      "Epoch [16810/30000], Loss: 1963.3850\n",
      "Epoch [16820/30000], Loss: 1947.7179\n",
      "Epoch [16830/30000], Loss: 1932.1737\n",
      "Epoch [16840/30000], Loss: 1916.7434\n",
      "Epoch [16850/30000], Loss: 1901.4316\n",
      "Epoch [16860/30000], Loss: 1886.2352\n",
      "Epoch [16870/30000], Loss: 1871.1519\n",
      "Epoch [16880/30000], Loss: 1856.1830\n",
      "Epoch [16890/30000], Loss: 1841.3323\n",
      "Epoch [16900/30000], Loss: 1826.5913\n",
      "Epoch [16910/30000], Loss: 1811.9636\n",
      "Epoch [16920/30000], Loss: 1797.4515\n",
      "Epoch [16930/30000], Loss: 1783.0424\n",
      "Epoch [16940/30000], Loss: 1768.7460\n",
      "Epoch [16950/30000], Loss: 1754.5612\n",
      "Epoch [16960/30000], Loss: 1740.4828\n",
      "Epoch [16970/30000], Loss: 1726.5099\n",
      "Epoch [16980/30000], Loss: 1712.6464\n",
      "Epoch [16990/30000], Loss: 1698.8892\n",
      "Epoch [17000/30000], Loss: 1685.2430\n",
      "Epoch [17010/30000], Loss: 1671.6917\n",
      "Epoch [17020/30000], Loss: 1658.2509\n",
      "Epoch [17030/30000], Loss: 1644.9122\n",
      "Epoch [17040/30000], Loss: 1631.6744\n",
      "Epoch [17050/30000], Loss: 1618.5391\n",
      "Epoch [17060/30000], Loss: 1605.5059\n",
      "Epoch [17070/30000], Loss: 1592.5691\n",
      "Epoch [17080/30000], Loss: 1579.7344\n",
      "Epoch [17090/30000], Loss: 1567.0009\n",
      "Epoch [17100/30000], Loss: 1554.3645\n",
      "Epoch [17110/30000], Loss: 1541.8264\n",
      "Epoch [17120/30000], Loss: 1529.3827\n",
      "Epoch [17130/30000], Loss: 1517.0393\n",
      "Epoch [17140/30000], Loss: 1504.7872\n",
      "Epoch [17150/30000], Loss: 1492.6333\n",
      "Epoch [17160/30000], Loss: 1480.5671\n",
      "Epoch [17170/30000], Loss: 1468.6049\n",
      "Epoch [17180/30000], Loss: 1456.7297\n",
      "Epoch [17190/30000], Loss: 1444.9456\n",
      "Epoch [17200/30000], Loss: 1433.2551\n",
      "Epoch [17210/30000], Loss: 1421.6559\n",
      "Epoch [17220/30000], Loss: 1410.1426\n",
      "Epoch [17230/30000], Loss: 1398.7267\n",
      "Epoch [17240/30000], Loss: 1387.3939\n",
      "Epoch [17250/30000], Loss: 1376.1487\n",
      "Epoch [17260/30000], Loss: 1364.9976\n",
      "Epoch [17270/30000], Loss: 1353.9297\n",
      "Epoch [17280/30000], Loss: 1342.9486\n",
      "Epoch [17290/30000], Loss: 1332.0509\n",
      "Epoch [17300/30000], Loss: 1321.2441\n",
      "Epoch [17310/30000], Loss: 1310.5133\n",
      "Epoch [17320/30000], Loss: 1299.8737\n",
      "Epoch [17330/30000], Loss: 1289.3193\n",
      "Epoch [17340/30000], Loss: 1278.8378\n",
      "Epoch [17350/30000], Loss: 1268.4490\n",
      "Epoch [17360/30000], Loss: 1258.1354\n",
      "Epoch [17370/30000], Loss: 1247.9069\n",
      "Epoch [17380/30000], Loss: 1237.7556\n",
      "Epoch [17390/30000], Loss: 1227.6862\n",
      "Epoch [17400/30000], Loss: 1217.6927\n",
      "Epoch [17410/30000], Loss: 1207.7844\n",
      "Epoch [17420/30000], Loss: 1197.9462\n",
      "Epoch [17430/30000], Loss: 1188.1907\n",
      "Epoch [17440/30000], Loss: 1178.5116\n",
      "Epoch [17450/30000], Loss: 1168.9061\n",
      "Epoch [17460/30000], Loss: 1159.3790\n",
      "Epoch [17470/30000], Loss: 1149.9271\n",
      "Epoch [17480/30000], Loss: 1140.5510\n",
      "Epoch [17490/30000], Loss: 1131.2479\n",
      "Epoch [17500/30000], Loss: 1122.0164\n",
      "Epoch [17510/30000], Loss: 1112.8604\n",
      "Epoch [17520/30000], Loss: 1103.7748\n",
      "Epoch [17530/30000], Loss: 1094.7666\n",
      "Epoch [17540/30000], Loss: 1085.8247\n",
      "Epoch [17550/30000], Loss: 1076.9539\n",
      "Epoch [17560/30000], Loss: 1068.1547\n",
      "Epoch [17570/30000], Loss: 1059.4257\n",
      "Epoch [17580/30000], Loss: 1050.7651\n",
      "Epoch [17590/30000], Loss: 1042.1742\n",
      "Epoch [17600/30000], Loss: 1033.6530\n",
      "Epoch [17610/30000], Loss: 1025.1986\n",
      "Epoch [17620/30000], Loss: 1016.8118\n",
      "Epoch [17630/30000], Loss: 1008.4924\n",
      "Epoch [17640/30000], Loss: 1000.2356\n",
      "Epoch [17650/30000], Loss: 992.0488\n",
      "Epoch [17660/30000], Loss: 983.9266\n",
      "Epoch [17670/30000], Loss: 975.8683\n",
      "Epoch [17680/30000], Loss: 967.8760\n",
      "Epoch [17690/30000], Loss: 959.9449\n",
      "Epoch [17700/30000], Loss: 952.0814\n",
      "Epoch [17710/30000], Loss: 944.2779\n",
      "Epoch [17720/30000], Loss: 936.5322\n",
      "Epoch [17730/30000], Loss: 928.8549\n",
      "Epoch [17740/30000], Loss: 921.2393\n",
      "Epoch [17750/30000], Loss: 913.6816\n",
      "Epoch [17760/30000], Loss: 906.1862\n",
      "Epoch [17770/30000], Loss: 898.7519\n",
      "Epoch [17780/30000], Loss: 891.3760\n",
      "Epoch [17790/30000], Loss: 884.0598\n",
      "Epoch [17800/30000], Loss: 876.8022\n",
      "Epoch [17810/30000], Loss: 869.5999\n",
      "Epoch [17820/30000], Loss: 862.4613\n",
      "Epoch [17830/30000], Loss: 855.3754\n",
      "Epoch [17840/30000], Loss: 848.3510\n",
      "Epoch [17850/30000], Loss: 841.3799\n",
      "Epoch [17860/30000], Loss: 834.4645\n",
      "Epoch [17870/30000], Loss: 827.6066\n",
      "Epoch [17880/30000], Loss: 820.8062\n",
      "Epoch [17890/30000], Loss: 814.0546\n",
      "Epoch [17900/30000], Loss: 807.3624\n",
      "Epoch [17910/30000], Loss: 800.7245\n",
      "Epoch [17920/30000], Loss: 794.1373\n",
      "Epoch [17930/30000], Loss: 787.6056\n",
      "Epoch [17940/30000], Loss: 781.1249\n",
      "Epoch [17950/30000], Loss: 774.6970\n",
      "Epoch [17960/30000], Loss: 768.3212\n",
      "Epoch [17970/30000], Loss: 761.9999\n",
      "Epoch [17980/30000], Loss: 755.7275\n",
      "Epoch [17990/30000], Loss: 749.5057\n",
      "Epoch [18000/30000], Loss: 743.3342\n",
      "Epoch [18010/30000], Loss: 737.2119\n",
      "Epoch [18020/30000], Loss: 731.1425\n",
      "Epoch [18030/30000], Loss: 725.1193\n",
      "Epoch [18040/30000], Loss: 719.1444\n",
      "Epoch [18050/30000], Loss: 713.2209\n",
      "Epoch [18060/30000], Loss: 707.3450\n",
      "Epoch [18070/30000], Loss: 701.5154\n",
      "Epoch [18080/30000], Loss: 695.7346\n",
      "Epoch [18090/30000], Loss: 689.9990\n",
      "Epoch [18100/30000], Loss: 684.3141\n",
      "Epoch [18110/30000], Loss: 678.6705\n",
      "Epoch [18120/30000], Loss: 673.0766\n",
      "Epoch [18130/30000], Loss: 667.5264\n",
      "Epoch [18140/30000], Loss: 662.0206\n",
      "Epoch [18150/30000], Loss: 656.5620\n",
      "Epoch [18160/30000], Loss: 651.1469\n",
      "Epoch [18170/30000], Loss: 645.7751\n",
      "Epoch [18180/30000], Loss: 640.4492\n",
      "Epoch [18190/30000], Loss: 635.1661\n",
      "Epoch [18200/30000], Loss: 629.9253\n",
      "Epoch [18210/30000], Loss: 624.7267\n",
      "Epoch [18220/30000], Loss: 619.5726\n",
      "Epoch [18230/30000], Loss: 614.4587\n",
      "Epoch [18240/30000], Loss: 609.3900\n",
      "Epoch [18250/30000], Loss: 604.3601\n",
      "Epoch [18260/30000], Loss: 599.3718\n",
      "Epoch [18270/30000], Loss: 594.4230\n",
      "Epoch [18280/30000], Loss: 589.5170\n",
      "Epoch [18290/30000], Loss: 584.6495\n",
      "Epoch [18300/30000], Loss: 579.8228\n",
      "Epoch [18310/30000], Loss: 575.0356\n",
      "Epoch [18320/30000], Loss: 570.2875\n",
      "Epoch [18330/30000], Loss: 565.5773\n",
      "Epoch [18340/30000], Loss: 560.9073\n",
      "Epoch [18350/30000], Loss: 556.2753\n",
      "Epoch [18360/30000], Loss: 551.6805\n",
      "Epoch [18370/30000], Loss: 547.1257\n",
      "Epoch [18380/30000], Loss: 542.6066\n",
      "Epoch [18390/30000], Loss: 538.1250\n",
      "Epoch [18400/30000], Loss: 533.6805\n",
      "Epoch [18410/30000], Loss: 529.2719\n",
      "Epoch [18420/30000], Loss: 524.9005\n",
      "Epoch [18430/30000], Loss: 520.5637\n",
      "Epoch [18440/30000], Loss: 516.2639\n",
      "Epoch [18450/30000], Loss: 511.9995\n",
      "Epoch [18460/30000], Loss: 507.7691\n",
      "Epoch [18470/30000], Loss: 503.5745\n",
      "Epoch [18480/30000], Loss: 499.4142\n",
      "Epoch [18490/30000], Loss: 495.2891\n",
      "Epoch [18500/30000], Loss: 491.1958\n",
      "Epoch [18510/30000], Loss: 487.1387\n",
      "Epoch [18520/30000], Loss: 483.1132\n",
      "Epoch [18530/30000], Loss: 479.1225\n",
      "Epoch [18540/30000], Loss: 475.1630\n",
      "Epoch [18550/30000], Loss: 471.2391\n",
      "Epoch [18560/30000], Loss: 467.3446\n",
      "Epoch [18570/30000], Loss: 463.4847\n",
      "Epoch [18580/30000], Loss: 459.6534\n",
      "Epoch [18590/30000], Loss: 455.8562\n",
      "Epoch [18600/30000], Loss: 452.0909\n",
      "Epoch [18610/30000], Loss: 448.3544\n",
      "Epoch [18620/30000], Loss: 444.6507\n",
      "Epoch [18630/30000], Loss: 440.9769\n",
      "Epoch [18640/30000], Loss: 437.3363\n",
      "Epoch [18650/30000], Loss: 433.7217\n",
      "Epoch [18660/30000], Loss: 430.1391\n",
      "Epoch [18670/30000], Loss: 426.5856\n",
      "Epoch [18680/30000], Loss: 423.0611\n",
      "Epoch [18690/30000], Loss: 419.5667\n",
      "Epoch [18700/30000], Loss: 416.1012\n",
      "Epoch [18710/30000], Loss: 412.6639\n",
      "Epoch [18720/30000], Loss: 409.2566\n",
      "Epoch [18730/30000], Loss: 405.8755\n",
      "Epoch [18740/30000], Loss: 402.5227\n",
      "Epoch [18750/30000], Loss: 399.1989\n",
      "Epoch [18760/30000], Loss: 395.9018\n",
      "Epoch [18770/30000], Loss: 392.6328\n",
      "Epoch [18780/30000], Loss: 389.3909\n",
      "Epoch [18790/30000], Loss: 386.1755\n",
      "Epoch [18800/30000], Loss: 382.9876\n",
      "Epoch [18810/30000], Loss: 379.8246\n",
      "Epoch [18820/30000], Loss: 376.6893\n",
      "Epoch [18830/30000], Loss: 373.5806\n",
      "Epoch [18840/30000], Loss: 370.4964\n",
      "Epoch [18850/30000], Loss: 367.4368\n",
      "Epoch [18860/30000], Loss: 364.4050\n",
      "Epoch [18870/30000], Loss: 361.3979\n",
      "Epoch [18880/30000], Loss: 358.4147\n",
      "Epoch [18890/30000], Loss: 355.4569\n",
      "Epoch [18900/30000], Loss: 352.5240\n",
      "Epoch [18910/30000], Loss: 349.6167\n",
      "Epoch [18920/30000], Loss: 346.7297\n",
      "Epoch [18930/30000], Loss: 343.8713\n",
      "Epoch [18940/30000], Loss: 341.0346\n",
      "Epoch [18950/30000], Loss: 338.2209\n",
      "Epoch [18960/30000], Loss: 335.4315\n",
      "Epoch [18970/30000], Loss: 332.6655\n",
      "Epoch [18980/30000], Loss: 329.9229\n",
      "Epoch [18990/30000], Loss: 327.2025\n",
      "Epoch [19000/30000], Loss: 324.5047\n",
      "Epoch [19010/30000], Loss: 321.8288\n",
      "Epoch [19020/30000], Loss: 319.1765\n",
      "Epoch [19030/30000], Loss: 316.5468\n",
      "Epoch [19040/30000], Loss: 313.9377\n",
      "Epoch [19050/30000], Loss: 311.3506\n",
      "Epoch [19060/30000], Loss: 308.7849\n",
      "Epoch [19070/30000], Loss: 306.2412\n",
      "Epoch [19080/30000], Loss: 303.7178\n",
      "Epoch [19090/30000], Loss: 301.2175\n",
      "Epoch [19100/30000], Loss: 298.7359\n",
      "Epoch [19110/30000], Loss: 296.2762\n",
      "Epoch [19120/30000], Loss: 293.8366\n",
      "Epoch [19130/30000], Loss: 291.4182\n",
      "Epoch [19140/30000], Loss: 289.0187\n",
      "Epoch [19150/30000], Loss: 286.6389\n",
      "Epoch [19160/30000], Loss: 284.2809\n",
      "Epoch [19170/30000], Loss: 281.9417\n",
      "Epoch [19180/30000], Loss: 279.6219\n",
      "Epoch [19190/30000], Loss: 277.3213\n",
      "Epoch [19200/30000], Loss: 275.0400\n",
      "Epoch [19210/30000], Loss: 272.7786\n",
      "Epoch [19220/30000], Loss: 270.5352\n",
      "Epoch [19230/30000], Loss: 268.3114\n",
      "Epoch [19240/30000], Loss: 266.1049\n",
      "Epoch [19250/30000], Loss: 263.9173\n",
      "Epoch [19260/30000], Loss: 261.7488\n",
      "Epoch [19270/30000], Loss: 259.5988\n",
      "Epoch [19280/30000], Loss: 257.4666\n",
      "Epoch [19290/30000], Loss: 255.3507\n",
      "Epoch [19300/30000], Loss: 253.2534\n",
      "Epoch [19310/30000], Loss: 251.1735\n",
      "Epoch [19320/30000], Loss: 249.1113\n",
      "Epoch [19330/30000], Loss: 247.0657\n",
      "Epoch [19340/30000], Loss: 245.0387\n",
      "Epoch [19350/30000], Loss: 243.0285\n",
      "Epoch [19360/30000], Loss: 241.0333\n",
      "Epoch [19370/30000], Loss: 239.0561\n",
      "Epoch [19380/30000], Loss: 237.0954\n",
      "Epoch [19390/30000], Loss: 235.1510\n",
      "Epoch [19400/30000], Loss: 233.2239\n",
      "Epoch [19410/30000], Loss: 231.3114\n",
      "Epoch [19420/30000], Loss: 229.4148\n",
      "Epoch [19430/30000], Loss: 227.5355\n",
      "Epoch [19440/30000], Loss: 225.6721\n",
      "Epoch [19450/30000], Loss: 223.8226\n",
      "Epoch [19460/30000], Loss: 221.9891\n",
      "Epoch [19470/30000], Loss: 220.1713\n",
      "Epoch [19480/30000], Loss: 218.3688\n",
      "Epoch [19490/30000], Loss: 216.5819\n",
      "Epoch [19500/30000], Loss: 214.8090\n",
      "Epoch [19510/30000], Loss: 213.0520\n",
      "Epoch [19520/30000], Loss: 211.3085\n",
      "Epoch [19530/30000], Loss: 209.5805\n",
      "Epoch [19540/30000], Loss: 207.8665\n",
      "Epoch [19550/30000], Loss: 206.1671\n",
      "Epoch [19560/30000], Loss: 204.4819\n",
      "Epoch [19570/30000], Loss: 202.8113\n",
      "Epoch [19580/30000], Loss: 201.1541\n",
      "Epoch [19590/30000], Loss: 199.5107\n",
      "Epoch [19600/30000], Loss: 197.8813\n",
      "Epoch [19610/30000], Loss: 196.2665\n",
      "Epoch [19620/30000], Loss: 194.6641\n",
      "Epoch [19630/30000], Loss: 193.0752\n",
      "Epoch [19640/30000], Loss: 191.4998\n",
      "Epoch [19650/30000], Loss: 189.9378\n",
      "Epoch [19660/30000], Loss: 188.3890\n",
      "Epoch [19670/30000], Loss: 186.8525\n",
      "Epoch [19680/30000], Loss: 185.3296\n",
      "Epoch [19690/30000], Loss: 183.8190\n",
      "Epoch [19700/30000], Loss: 182.3214\n",
      "Epoch [19710/30000], Loss: 180.8361\n",
      "Epoch [19720/30000], Loss: 179.3636\n",
      "Epoch [19730/30000], Loss: 177.9034\n",
      "Epoch [19740/30000], Loss: 176.4552\n",
      "Epoch [19750/30000], Loss: 175.0194\n",
      "Epoch [19760/30000], Loss: 173.5951\n",
      "Epoch [19770/30000], Loss: 172.1837\n",
      "Epoch [19780/30000], Loss: 170.7830\n",
      "Epoch [19790/30000], Loss: 169.3950\n",
      "Epoch [19800/30000], Loss: 168.0186\n",
      "Epoch [19810/30000], Loss: 166.6536\n",
      "Epoch [19820/30000], Loss: 165.2994\n",
      "Epoch [19830/30000], Loss: 163.9570\n",
      "Epoch [19840/30000], Loss: 162.6257\n",
      "Epoch [19850/30000], Loss: 161.3061\n",
      "Epoch [19860/30000], Loss: 159.9976\n",
      "Epoch [19870/30000], Loss: 158.6998\n",
      "Epoch [19880/30000], Loss: 157.4127\n",
      "Epoch [19890/30000], Loss: 156.1362\n",
      "Epoch [19900/30000], Loss: 154.8707\n",
      "Epoch [19910/30000], Loss: 153.6158\n",
      "Epoch [19920/30000], Loss: 152.3718\n",
      "Epoch [19930/30000], Loss: 151.1374\n",
      "Epoch [19940/30000], Loss: 149.9143\n",
      "Epoch [19950/30000], Loss: 148.7007\n",
      "Epoch [19960/30000], Loss: 147.4974\n",
      "Epoch [19970/30000], Loss: 146.3042\n",
      "Epoch [19980/30000], Loss: 145.1209\n",
      "Epoch [19990/30000], Loss: 143.9486\n",
      "Epoch [20000/30000], Loss: 142.7850\n",
      "Epoch [20010/30000], Loss: 141.6314\n",
      "Epoch [20020/30000], Loss: 140.4873\n",
      "Epoch [20030/30000], Loss: 139.3534\n",
      "Epoch [20040/30000], Loss: 138.2282\n",
      "Epoch [20050/30000], Loss: 137.1129\n",
      "Epoch [20060/30000], Loss: 136.0070\n",
      "Epoch [20070/30000], Loss: 134.9098\n",
      "Epoch [20080/30000], Loss: 133.8227\n",
      "Epoch [20090/30000], Loss: 132.7442\n",
      "Epoch [20100/30000], Loss: 131.6749\n",
      "Epoch [20110/30000], Loss: 130.6140\n",
      "Epoch [20120/30000], Loss: 129.5624\n",
      "Epoch [20130/30000], Loss: 128.5201\n",
      "Epoch [20140/30000], Loss: 127.4856\n",
      "Epoch [20150/30000], Loss: 126.4605\n",
      "Epoch [20160/30000], Loss: 125.4438\n",
      "Epoch [20170/30000], Loss: 124.4353\n",
      "Epoch [20180/30000], Loss: 123.4354\n",
      "Epoch [20190/30000], Loss: 122.4438\n",
      "Epoch [20200/30000], Loss: 121.4604\n",
      "Epoch [20210/30000], Loss: 120.4862\n",
      "Epoch [20220/30000], Loss: 119.5190\n",
      "Epoch [20230/30000], Loss: 118.5604\n",
      "Epoch [20240/30000], Loss: 117.6099\n",
      "Epoch [20250/30000], Loss: 116.6671\n",
      "Epoch [20260/30000], Loss: 115.7320\n",
      "Epoch [20270/30000], Loss: 114.8056\n",
      "Epoch [20280/30000], Loss: 113.8856\n",
      "Epoch [20290/30000], Loss: 112.9744\n",
      "Epoch [20300/30000], Loss: 112.0707\n",
      "Epoch [20310/30000], Loss: 111.1738\n",
      "Epoch [20320/30000], Loss: 110.2849\n",
      "Epoch [20330/30000], Loss: 109.4038\n",
      "Epoch [20340/30000], Loss: 108.5287\n",
      "Epoch [20350/30000], Loss: 107.6626\n",
      "Epoch [20360/30000], Loss: 106.8031\n",
      "Epoch [20370/30000], Loss: 105.9505\n",
      "Epoch [20380/30000], Loss: 105.1054\n",
      "Epoch [20390/30000], Loss: 104.2671\n",
      "Epoch [20400/30000], Loss: 103.4362\n",
      "Epoch [20410/30000], Loss: 102.6121\n",
      "Epoch [20420/30000], Loss: 101.7946\n",
      "Epoch [20430/30000], Loss: 100.9839\n",
      "Epoch [20440/30000], Loss: 100.1799\n",
      "Epoch [20450/30000], Loss: 99.3831\n",
      "Epoch [20460/30000], Loss: 98.5928\n",
      "Epoch [20470/30000], Loss: 97.8090\n",
      "Epoch [20480/30000], Loss: 97.0314\n",
      "Epoch [20490/30000], Loss: 96.2605\n",
      "Epoch [20500/30000], Loss: 95.4963\n",
      "Epoch [20510/30000], Loss: 94.7383\n",
      "Epoch [20520/30000], Loss: 93.9868\n",
      "Epoch [20530/30000], Loss: 93.2411\n",
      "Epoch [20540/30000], Loss: 92.5023\n",
      "Epoch [20550/30000], Loss: 91.7692\n",
      "Epoch [20560/30000], Loss: 91.0422\n",
      "Epoch [20570/30000], Loss: 90.3214\n",
      "Epoch [20580/30000], Loss: 89.6062\n",
      "Epoch [20590/30000], Loss: 88.8977\n",
      "Epoch [20600/30000], Loss: 88.1946\n",
      "Epoch [20610/30000], Loss: 87.4969\n",
      "Epoch [20620/30000], Loss: 86.8060\n",
      "Epoch [20630/30000], Loss: 86.1201\n",
      "Epoch [20640/30000], Loss: 85.4403\n",
      "Epoch [20650/30000], Loss: 84.7663\n",
      "Epoch [20660/30000], Loss: 84.0977\n",
      "Epoch [20670/30000], Loss: 83.4347\n",
      "Epoch [20680/30000], Loss: 82.7772\n",
      "Epoch [20690/30000], Loss: 82.1247\n",
      "Epoch [20700/30000], Loss: 81.4781\n",
      "Epoch [20710/30000], Loss: 80.8370\n",
      "Epoch [20720/30000], Loss: 80.2007\n",
      "Epoch [20730/30000], Loss: 79.5702\n",
      "Epoch [20740/30000], Loss: 78.9448\n",
      "Epoch [20750/30000], Loss: 78.3244\n",
      "Epoch [20760/30000], Loss: 77.7094\n",
      "Epoch [20770/30000], Loss: 77.0990\n",
      "Epoch [20780/30000], Loss: 76.4943\n",
      "Epoch [20790/30000], Loss: 75.8945\n",
      "Epoch [20800/30000], Loss: 75.2994\n",
      "Epoch [20810/30000], Loss: 74.7097\n",
      "Epoch [20820/30000], Loss: 74.1241\n",
      "Epoch [20830/30000], Loss: 73.5440\n",
      "Epoch [20840/30000], Loss: 72.9685\n",
      "Epoch [20850/30000], Loss: 72.3977\n",
      "Epoch [20860/30000], Loss: 71.8318\n",
      "Epoch [20870/30000], Loss: 71.2706\n",
      "Epoch [20880/30000], Loss: 70.7139\n",
      "Epoch [20890/30000], Loss: 70.1620\n",
      "Epoch [20900/30000], Loss: 69.6142\n",
      "Epoch [20910/30000], Loss: 69.0714\n",
      "Epoch [20920/30000], Loss: 68.5330\n",
      "Epoch [20930/30000], Loss: 67.9993\n",
      "Epoch [20940/30000], Loss: 67.4694\n",
      "Epoch [20950/30000], Loss: 66.9444\n",
      "Epoch [20960/30000], Loss: 66.4234\n",
      "Epoch [20970/30000], Loss: 65.9068\n",
      "Epoch [20980/30000], Loss: 65.3946\n",
      "Epoch [20990/30000], Loss: 64.8867\n",
      "Epoch [21000/30000], Loss: 64.3830\n",
      "Epoch [21010/30000], Loss: 63.8828\n",
      "Epoch [21020/30000], Loss: 63.3873\n",
      "Epoch [21030/30000], Loss: 62.8959\n",
      "Epoch [21040/30000], Loss: 62.4084\n",
      "Epoch [21050/30000], Loss: 61.9250\n",
      "Epoch [21060/30000], Loss: 61.4456\n",
      "Epoch [21070/30000], Loss: 60.9700\n",
      "Epoch [21080/30000], Loss: 60.4985\n",
      "Epoch [21090/30000], Loss: 60.0306\n",
      "Epoch [21100/30000], Loss: 59.5671\n",
      "Epoch [21110/30000], Loss: 59.1069\n",
      "Epoch [21120/30000], Loss: 58.6510\n",
      "Epoch [21130/30000], Loss: 58.1980\n",
      "Epoch [21140/30000], Loss: 57.7494\n",
      "Epoch [21150/30000], Loss: 57.3043\n",
      "Epoch [21160/30000], Loss: 56.8630\n",
      "Epoch [21170/30000], Loss: 56.4252\n",
      "Epoch [21180/30000], Loss: 55.9910\n",
      "Epoch [21190/30000], Loss: 55.5604\n",
      "Epoch [21200/30000], Loss: 55.1331\n",
      "Epoch [21210/30000], Loss: 54.7097\n",
      "Epoch [21220/30000], Loss: 54.2895\n",
      "Epoch [21230/30000], Loss: 53.8731\n",
      "Epoch [21240/30000], Loss: 53.4598\n",
      "Epoch [21250/30000], Loss: 53.0496\n",
      "Epoch [21260/30000], Loss: 52.6431\n",
      "Epoch [21270/30000], Loss: 52.2402\n",
      "Epoch [21280/30000], Loss: 51.8403\n",
      "Epoch [21290/30000], Loss: 51.4436\n",
      "Epoch [21300/30000], Loss: 51.0503\n",
      "Epoch [21310/30000], Loss: 50.6600\n",
      "Epoch [21320/30000], Loss: 50.2733\n",
      "Epoch [21330/30000], Loss: 49.8896\n",
      "Epoch [21340/30000], Loss: 49.5089\n",
      "Epoch [21350/30000], Loss: 49.1313\n",
      "Epoch [21360/30000], Loss: 48.7569\n",
      "Epoch [21370/30000], Loss: 48.3858\n",
      "Epoch [21380/30000], Loss: 48.0169\n",
      "Epoch [21390/30000], Loss: 47.6517\n",
      "Epoch [21400/30000], Loss: 47.2893\n",
      "Epoch [21410/30000], Loss: 46.9302\n",
      "Epoch [21420/30000], Loss: 46.5736\n",
      "Epoch [21430/30000], Loss: 46.2199\n",
      "Epoch [21440/30000], Loss: 45.8695\n",
      "Epoch [21450/30000], Loss: 45.5213\n",
      "Epoch [21460/30000], Loss: 45.1763\n",
      "Epoch [21470/30000], Loss: 44.8341\n",
      "Epoch [21480/30000], Loss: 44.4945\n",
      "Epoch [21490/30000], Loss: 44.1580\n",
      "Epoch [21500/30000], Loss: 43.8240\n",
      "Epoch [21510/30000], Loss: 43.4929\n",
      "Epoch [21520/30000], Loss: 43.1643\n",
      "Epoch [21530/30000], Loss: 42.8385\n",
      "Epoch [21540/30000], Loss: 42.5153\n",
      "Epoch [21550/30000], Loss: 42.1947\n",
      "Epoch [21560/30000], Loss: 41.8768\n",
      "Epoch [21570/30000], Loss: 41.5613\n",
      "Epoch [21580/30000], Loss: 41.2484\n",
      "Epoch [21590/30000], Loss: 40.9379\n",
      "Epoch [21600/30000], Loss: 40.6304\n",
      "Epoch [21610/30000], Loss: 40.3251\n",
      "Epoch [21620/30000], Loss: 40.0221\n",
      "Epoch [21630/30000], Loss: 39.7218\n",
      "Epoch [21640/30000], Loss: 39.4239\n",
      "Epoch [21650/30000], Loss: 39.1283\n",
      "Epoch [21660/30000], Loss: 38.8353\n",
      "Epoch [21670/30000], Loss: 38.5442\n",
      "Epoch [21680/30000], Loss: 38.2558\n",
      "Epoch [21690/30000], Loss: 37.9696\n",
      "Epoch [21700/30000], Loss: 37.6859\n",
      "Epoch [21710/30000], Loss: 37.4044\n",
      "Epoch [21720/30000], Loss: 37.1250\n",
      "Epoch [21730/30000], Loss: 36.8481\n",
      "Epoch [21740/30000], Loss: 36.5734\n",
      "Epoch [21750/30000], Loss: 36.3008\n",
      "Epoch [21760/30000], Loss: 36.0305\n",
      "Epoch [21770/30000], Loss: 35.7621\n",
      "Epoch [21780/30000], Loss: 35.4961\n",
      "Epoch [21790/30000], Loss: 35.2321\n",
      "Epoch [21800/30000], Loss: 34.9705\n",
      "Epoch [21810/30000], Loss: 34.7107\n",
      "Epoch [21820/30000], Loss: 34.4532\n",
      "Epoch [21830/30000], Loss: 34.1977\n",
      "Epoch [21840/30000], Loss: 33.9441\n",
      "Epoch [21850/30000], Loss: 33.6926\n",
      "Epoch [21860/30000], Loss: 33.4433\n",
      "Epoch [21870/30000], Loss: 33.1957\n",
      "Epoch [21880/30000], Loss: 32.9502\n",
      "Epoch [21890/30000], Loss: 32.7070\n",
      "Epoch [21900/30000], Loss: 32.4651\n",
      "Epoch [21910/30000], Loss: 32.2258\n",
      "Epoch [21920/30000], Loss: 31.9879\n",
      "Epoch [21930/30000], Loss: 31.7519\n",
      "Epoch [21940/30000], Loss: 31.5182\n",
      "Epoch [21950/30000], Loss: 31.2860\n",
      "Epoch [21960/30000], Loss: 31.0559\n",
      "Epoch [21970/30000], Loss: 30.8274\n",
      "Epoch [21980/30000], Loss: 30.6009\n",
      "Epoch [21990/30000], Loss: 30.3761\n",
      "Epoch [22000/30000], Loss: 30.1532\n",
      "Epoch [22010/30000], Loss: 29.9321\n",
      "Epoch [22020/30000], Loss: 29.7126\n",
      "Epoch [22030/30000], Loss: 29.4949\n",
      "Epoch [22040/30000], Loss: 29.2790\n",
      "Epoch [22050/30000], Loss: 29.0645\n",
      "Epoch [22060/30000], Loss: 28.8523\n",
      "Epoch [22070/30000], Loss: 28.6414\n",
      "Epoch [22080/30000], Loss: 28.4321\n",
      "Epoch [22090/30000], Loss: 28.2247\n",
      "Epoch [22100/30000], Loss: 28.0186\n",
      "Epoch [22110/30000], Loss: 27.8145\n",
      "Epoch [22120/30000], Loss: 27.6119\n",
      "Epoch [22130/30000], Loss: 27.4109\n",
      "Epoch [22140/30000], Loss: 27.2114\n",
      "Epoch [22150/30000], Loss: 27.0135\n",
      "Epoch [22160/30000], Loss: 26.8173\n",
      "Epoch [22170/30000], Loss: 26.6225\n",
      "Epoch [22180/30000], Loss: 26.4293\n",
      "Epoch [22190/30000], Loss: 26.2375\n",
      "Epoch [22200/30000], Loss: 26.0474\n",
      "Epoch [22210/30000], Loss: 25.8586\n",
      "Epoch [22220/30000], Loss: 25.6715\n",
      "Epoch [22230/30000], Loss: 25.4857\n",
      "Epoch [22240/30000], Loss: 25.3014\n",
      "Epoch [22250/30000], Loss: 25.1186\n",
      "Epoch [22260/30000], Loss: 24.9371\n",
      "Epoch [22270/30000], Loss: 24.7573\n",
      "Epoch [22280/30000], Loss: 24.5789\n",
      "Epoch [22290/30000], Loss: 24.4015\n",
      "Epoch [22300/30000], Loss: 24.2257\n",
      "Epoch [22310/30000], Loss: 24.0515\n",
      "Epoch [22320/30000], Loss: 23.8783\n",
      "Epoch [22330/30000], Loss: 23.7066\n",
      "Epoch [22340/30000], Loss: 23.5364\n",
      "Epoch [22350/30000], Loss: 23.3674\n",
      "Epoch [22360/30000], Loss: 23.1997\n",
      "Epoch [22370/30000], Loss: 23.0333\n",
      "Epoch [22380/30000], Loss: 22.8682\n",
      "Epoch [22390/30000], Loss: 22.7046\n",
      "Epoch [22400/30000], Loss: 22.5419\n",
      "Epoch [22410/30000], Loss: 22.3808\n",
      "Epoch [22420/30000], Loss: 22.2207\n",
      "Epoch [22430/30000], Loss: 22.0620\n",
      "Epoch [22440/30000], Loss: 21.9045\n",
      "Epoch [22450/30000], Loss: 21.7482\n",
      "Epoch [22460/30000], Loss: 21.5931\n",
      "Epoch [22470/30000], Loss: 21.4394\n",
      "Epoch [22480/30000], Loss: 21.2867\n",
      "Epoch [22490/30000], Loss: 21.1352\n",
      "Epoch [22500/30000], Loss: 20.9848\n",
      "Epoch [22510/30000], Loss: 20.8356\n",
      "Epoch [22520/30000], Loss: 20.6876\n",
      "Epoch [22530/30000], Loss: 20.5409\n",
      "Epoch [22540/30000], Loss: 20.3951\n",
      "Epoch [22550/30000], Loss: 20.2505\n",
      "Epoch [22560/30000], Loss: 20.1069\n",
      "Epoch [22570/30000], Loss: 19.9646\n",
      "Epoch [22580/30000], Loss: 19.8234\n",
      "Epoch [22590/30000], Loss: 19.6831\n",
      "Epoch [22600/30000], Loss: 19.5440\n",
      "Epoch [22610/30000], Loss: 19.4059\n",
      "Epoch [22620/30000], Loss: 19.2691\n",
      "Epoch [22630/30000], Loss: 19.1332\n",
      "Epoch [22640/30000], Loss: 18.9982\n",
      "Epoch [22650/30000], Loss: 18.8645\n",
      "Epoch [22660/30000], Loss: 18.7316\n",
      "Epoch [22670/30000], Loss: 18.5998\n",
      "Epoch [22680/30000], Loss: 18.4690\n",
      "Epoch [22690/30000], Loss: 18.3392\n",
      "Epoch [22700/30000], Loss: 18.2103\n",
      "Epoch [22710/30000], Loss: 18.0827\n",
      "Epoch [22720/30000], Loss: 17.9558\n",
      "Epoch [22730/30000], Loss: 17.8299\n",
      "Epoch [22740/30000], Loss: 17.7049\n",
      "Epoch [22750/30000], Loss: 17.5811\n",
      "Epoch [22760/30000], Loss: 17.4581\n",
      "Epoch [22770/30000], Loss: 17.3360\n",
      "Epoch [22780/30000], Loss: 17.2148\n",
      "Epoch [22790/30000], Loss: 17.0946\n",
      "Epoch [22800/30000], Loss: 16.9753\n",
      "Epoch [22810/30000], Loss: 16.8568\n",
      "Epoch [22820/30000], Loss: 16.7393\n",
      "Epoch [22830/30000], Loss: 16.6228\n",
      "Epoch [22840/30000], Loss: 16.5070\n",
      "Epoch [22850/30000], Loss: 16.3922\n",
      "Epoch [22860/30000], Loss: 16.2781\n",
      "Epoch [22870/30000], Loss: 16.1652\n",
      "Epoch [22880/30000], Loss: 16.0528\n",
      "Epoch [22890/30000], Loss: 15.9414\n",
      "Epoch [22900/30000], Loss: 15.8308\n",
      "Epoch [22910/30000], Loss: 15.7211\n",
      "Epoch [22920/30000], Loss: 15.6121\n",
      "Epoch [22930/30000], Loss: 15.5040\n",
      "Epoch [22940/30000], Loss: 15.3967\n",
      "Epoch [22950/30000], Loss: 15.2903\n",
      "Epoch [22960/30000], Loss: 15.1846\n",
      "Epoch [22970/30000], Loss: 15.0797\n",
      "Epoch [22980/30000], Loss: 14.9756\n",
      "Epoch [22990/30000], Loss: 14.8723\n",
      "Epoch [23000/30000], Loss: 14.7698\n",
      "Epoch [23010/30000], Loss: 14.6679\n",
      "Epoch [23020/30000], Loss: 14.5669\n",
      "Epoch [23030/30000], Loss: 14.4667\n",
      "Epoch [23040/30000], Loss: 14.3673\n",
      "Epoch [23050/30000], Loss: 14.2684\n",
      "Epoch [23060/30000], Loss: 14.1704\n",
      "Epoch [23070/30000], Loss: 14.0731\n",
      "Epoch [23080/30000], Loss: 13.9765\n",
      "Epoch [23090/30000], Loss: 13.8807\n",
      "Epoch [23100/30000], Loss: 13.7855\n",
      "Epoch [23110/30000], Loss: 13.6910\n",
      "Epoch [23120/30000], Loss: 13.5973\n",
      "Epoch [23130/30000], Loss: 13.5043\n",
      "Epoch [23140/30000], Loss: 13.4119\n",
      "Epoch [23150/30000], Loss: 13.3202\n",
      "Epoch [23160/30000], Loss: 13.2293\n",
      "Epoch [23170/30000], Loss: 13.1390\n",
      "Epoch [23180/30000], Loss: 13.0493\n",
      "Epoch [23190/30000], Loss: 12.9604\n",
      "Epoch [23200/30000], Loss: 12.8720\n",
      "Epoch [23210/30000], Loss: 12.7843\n",
      "Epoch [23220/30000], Loss: 12.6972\n",
      "Epoch [23230/30000], Loss: 12.6108\n",
      "Epoch [23240/30000], Loss: 12.5250\n",
      "Epoch [23250/30000], Loss: 12.4400\n",
      "Epoch [23260/30000], Loss: 12.3555\n",
      "Epoch [23270/30000], Loss: 12.2715\n",
      "Epoch [23280/30000], Loss: 12.1883\n",
      "Epoch [23290/30000], Loss: 12.1056\n",
      "Epoch [23300/30000], Loss: 12.0236\n",
      "Epoch [23310/30000], Loss: 11.9421\n",
      "Epoch [23320/30000], Loss: 11.8612\n",
      "Epoch [23330/30000], Loss: 11.7810\n",
      "Epoch [23340/30000], Loss: 11.7013\n",
      "Epoch [23350/30000], Loss: 11.6222\n",
      "Epoch [23360/30000], Loss: 11.5437\n",
      "Epoch [23370/30000], Loss: 11.4657\n",
      "Epoch [23380/30000], Loss: 11.3883\n",
      "Epoch [23390/30000], Loss: 11.3115\n",
      "Epoch [23400/30000], Loss: 11.2353\n",
      "Epoch [23410/30000], Loss: 11.1595\n",
      "Epoch [23420/30000], Loss: 11.0844\n",
      "Epoch [23430/30000], Loss: 11.0098\n",
      "Epoch [23440/30000], Loss: 10.9357\n",
      "Epoch [23450/30000], Loss: 10.8621\n",
      "Epoch [23460/30000], Loss: 10.7891\n",
      "Epoch [23470/30000], Loss: 10.7166\n",
      "Epoch [23480/30000], Loss: 10.6446\n",
      "Epoch [23490/30000], Loss: 10.5732\n",
      "Epoch [23500/30000], Loss: 10.5022\n",
      "Epoch [23510/30000], Loss: 10.4318\n",
      "Epoch [23520/30000], Loss: 10.3619\n",
      "Epoch [23530/30000], Loss: 10.2925\n",
      "Epoch [23540/30000], Loss: 10.2236\n",
      "Epoch [23550/30000], Loss: 10.1551\n",
      "Epoch [23560/30000], Loss: 10.0872\n",
      "Epoch [23570/30000], Loss: 10.0197\n",
      "Epoch [23580/30000], Loss: 9.9528\n",
      "Epoch [23590/30000], Loss: 9.8863\n",
      "Epoch [23600/30000], Loss: 9.8202\n",
      "Epoch [23610/30000], Loss: 9.7548\n",
      "Epoch [23620/30000], Loss: 9.6897\n",
      "Epoch [23630/30000], Loss: 9.6251\n",
      "Epoch [23640/30000], Loss: 9.5610\n",
      "Epoch [23650/30000], Loss: 9.4972\n",
      "Epoch [23660/30000], Loss: 9.4339\n",
      "Epoch [23670/30000], Loss: 9.3712\n",
      "Epoch [23680/30000], Loss: 9.3088\n",
      "Epoch [23690/30000], Loss: 9.2469\n",
      "Epoch [23700/30000], Loss: 9.1854\n",
      "Epoch [23710/30000], Loss: 9.1243\n",
      "Epoch [23720/30000], Loss: 9.0638\n",
      "Epoch [23730/30000], Loss: 9.0036\n",
      "Epoch [23740/30000], Loss: 8.9438\n",
      "Epoch [23750/30000], Loss: 8.8845\n",
      "Epoch [23760/30000], Loss: 8.8256\n",
      "Epoch [23770/30000], Loss: 8.7671\n",
      "Epoch [23780/30000], Loss: 8.7090\n",
      "Epoch [23790/30000], Loss: 8.6513\n",
      "Epoch [23800/30000], Loss: 8.5939\n",
      "Epoch [23810/30000], Loss: 8.5371\n",
      "Epoch [23820/30000], Loss: 8.4806\n",
      "Epoch [23830/30000], Loss: 8.4246\n",
      "Epoch [23840/30000], Loss: 8.3689\n",
      "Epoch [23850/30000], Loss: 8.3135\n",
      "Epoch [23860/30000], Loss: 8.2586\n",
      "Epoch [23870/30000], Loss: 8.2041\n",
      "Epoch [23880/30000], Loss: 8.1499\n",
      "Epoch [23890/30000], Loss: 8.0962\n",
      "Epoch [23900/30000], Loss: 8.0428\n",
      "Epoch [23910/30000], Loss: 7.9898\n",
      "Epoch [23920/30000], Loss: 7.9370\n",
      "Epoch [23930/30000], Loss: 7.8847\n",
      "Epoch [23940/30000], Loss: 7.8328\n",
      "Epoch [23950/30000], Loss: 7.7812\n",
      "Epoch [23960/30000], Loss: 7.7300\n",
      "Epoch [23970/30000], Loss: 7.6790\n",
      "Epoch [23980/30000], Loss: 7.6286\n",
      "Epoch [23990/30000], Loss: 7.5784\n",
      "Epoch [24000/30000], Loss: 7.5286\n",
      "Epoch [24010/30000], Loss: 7.4790\n",
      "Epoch [24020/30000], Loss: 7.4299\n",
      "Epoch [24030/30000], Loss: 7.3811\n",
      "Epoch [24040/30000], Loss: 7.3326\n",
      "Epoch [24050/30000], Loss: 7.2845\n",
      "Epoch [24060/30000], Loss: 7.2367\n",
      "Epoch [24070/30000], Loss: 7.1892\n",
      "Epoch [24080/30000], Loss: 7.1420\n",
      "Epoch [24090/30000], Loss: 7.0952\n",
      "Epoch [24100/30000], Loss: 7.0487\n",
      "Epoch [24110/30000], Loss: 7.0024\n",
      "Epoch [24120/30000], Loss: 6.9566\n",
      "Epoch [24130/30000], Loss: 6.9110\n",
      "Epoch [24140/30000], Loss: 6.8657\n",
      "Epoch [24150/30000], Loss: 6.8207\n",
      "Epoch [24160/30000], Loss: 6.7760\n",
      "Epoch [24170/30000], Loss: 6.7317\n",
      "Epoch [24180/30000], Loss: 6.6877\n",
      "Epoch [24190/30000], Loss: 6.6439\n",
      "Epoch [24200/30000], Loss: 6.6005\n",
      "Epoch [24210/30000], Loss: 6.5573\n",
      "Epoch [24220/30000], Loss: 6.5144\n",
      "Epoch [24230/30000], Loss: 6.4718\n",
      "Epoch [24240/30000], Loss: 6.4295\n",
      "Epoch [24250/30000], Loss: 6.3875\n",
      "Epoch [24260/30000], Loss: 6.3458\n",
      "Epoch [24270/30000], Loss: 6.3043\n",
      "Epoch [24280/30000], Loss: 6.2631\n",
      "Epoch [24290/30000], Loss: 6.2222\n",
      "Epoch [24300/30000], Loss: 6.1816\n",
      "Epoch [24310/30000], Loss: 6.1412\n",
      "Epoch [24320/30000], Loss: 6.1012\n",
      "Epoch [24330/30000], Loss: 6.0613\n",
      "Epoch [24340/30000], Loss: 6.0218\n",
      "Epoch [24350/30000], Loss: 5.9825\n",
      "Epoch [24360/30000], Loss: 5.9435\n",
      "Epoch [24370/30000], Loss: 5.9047\n",
      "Epoch [24380/30000], Loss: 5.8662\n",
      "Epoch [24390/30000], Loss: 5.8279\n",
      "Epoch [24400/30000], Loss: 5.7899\n",
      "Epoch [24410/30000], Loss: 5.7522\n",
      "Epoch [24420/30000], Loss: 5.7147\n",
      "Epoch [24430/30000], Loss: 5.6774\n",
      "Epoch [24440/30000], Loss: 5.6404\n",
      "Epoch [24450/30000], Loss: 5.6037\n",
      "Epoch [24460/30000], Loss: 5.5671\n",
      "Epoch [24470/30000], Loss: 5.5309\n",
      "Epoch [24480/30000], Loss: 5.4948\n",
      "Epoch [24490/30000], Loss: 5.4590\n",
      "Epoch [24500/30000], Loss: 5.4234\n",
      "Epoch [24510/30000], Loss: 5.3881\n",
      "Epoch [24520/30000], Loss: 5.3530\n",
      "Epoch [24530/30000], Loss: 5.3181\n",
      "Epoch [24540/30000], Loss: 5.2835\n",
      "Epoch [24550/30000], Loss: 5.2491\n",
      "Epoch [24560/30000], Loss: 5.2149\n",
      "Epoch [24570/30000], Loss: 5.1809\n",
      "Epoch [24580/30000], Loss: 5.1472\n",
      "Epoch [24590/30000], Loss: 5.1137\n",
      "Epoch [24600/30000], Loss: 5.0804\n",
      "Epoch [24610/30000], Loss: 5.0473\n",
      "Epoch [24620/30000], Loss: 5.0144\n",
      "Epoch [24630/30000], Loss: 4.9817\n",
      "Epoch [24640/30000], Loss: 4.9493\n",
      "Epoch [24650/30000], Loss: 4.9170\n",
      "Epoch [24660/30000], Loss: 4.8850\n",
      "Epoch [24670/30000], Loss: 4.8532\n",
      "Epoch [24680/30000], Loss: 4.8216\n",
      "Epoch [24690/30000], Loss: 4.7902\n",
      "Epoch [24700/30000], Loss: 4.7589\n",
      "Epoch [24710/30000], Loss: 4.7279\n",
      "Epoch [24720/30000], Loss: 4.6971\n",
      "Epoch [24730/30000], Loss: 4.6665\n",
      "Epoch [24740/30000], Loss: 4.6361\n",
      "Epoch [24750/30000], Loss: 4.6059\n",
      "Epoch [24760/30000], Loss: 4.5759\n",
      "Epoch [24770/30000], Loss: 4.5460\n",
      "Epoch [24780/30000], Loss: 4.5164\n",
      "Epoch [24790/30000], Loss: 4.4870\n",
      "Epoch [24800/30000], Loss: 4.4577\n",
      "Epoch [24810/30000], Loss: 4.4286\n",
      "Epoch [24820/30000], Loss: 4.3997\n",
      "Epoch [24830/30000], Loss: 4.3710\n",
      "Epoch [24840/30000], Loss: 4.3425\n",
      "Epoch [24850/30000], Loss: 4.3142\n",
      "Epoch [24860/30000], Loss: 4.2860\n",
      "Epoch [24870/30000], Loss: 4.2581\n",
      "Epoch [24880/30000], Loss: 4.2303\n",
      "Epoch [24890/30000], Loss: 4.2027\n",
      "Epoch [24900/30000], Loss: 4.1752\n",
      "Epoch [24910/30000], Loss: 4.1479\n",
      "Epoch [24920/30000], Loss: 4.1208\n",
      "Epoch [24930/30000], Loss: 4.0939\n",
      "Epoch [24940/30000], Loss: 4.0672\n",
      "Epoch [24950/30000], Loss: 4.0406\n",
      "Epoch [24960/30000], Loss: 4.0142\n",
      "Epoch [24970/30000], Loss: 3.9879\n",
      "Epoch [24980/30000], Loss: 3.9618\n",
      "Epoch [24990/30000], Loss: 3.9359\n",
      "Epoch [25000/30000], Loss: 3.9102\n",
      "Epoch [25010/30000], Loss: 3.8846\n",
      "Epoch [25020/30000], Loss: 3.8591\n",
      "Epoch [25030/30000], Loss: 3.8339\n",
      "Epoch [25040/30000], Loss: 3.8088\n",
      "Epoch [25050/30000], Loss: 3.7838\n",
      "Epoch [25060/30000], Loss: 3.7590\n",
      "Epoch [25070/30000], Loss: 3.7344\n",
      "Epoch [25080/30000], Loss: 3.7099\n",
      "Epoch [25090/30000], Loss: 3.6856\n",
      "Epoch [25100/30000], Loss: 3.6614\n",
      "Epoch [25110/30000], Loss: 3.6374\n",
      "Epoch [25120/30000], Loss: 3.6135\n",
      "Epoch [25130/30000], Loss: 3.5898\n",
      "Epoch [25140/30000], Loss: 3.5662\n",
      "Epoch [25150/30000], Loss: 3.5428\n",
      "Epoch [25160/30000], Loss: 3.5195\n",
      "Epoch [25170/30000], Loss: 3.4964\n",
      "Epoch [25180/30000], Loss: 3.4733\n",
      "Epoch [25190/30000], Loss: 3.4505\n",
      "Epoch [25200/30000], Loss: 3.4278\n",
      "Epoch [25210/30000], Loss: 3.4052\n",
      "Epoch [25220/30000], Loss: 3.3828\n",
      "Epoch [25230/30000], Loss: 3.3605\n",
      "Epoch [25240/30000], Loss: 3.3384\n",
      "Epoch [25250/30000], Loss: 3.3164\n",
      "Epoch [25260/30000], Loss: 3.2945\n",
      "Epoch [25270/30000], Loss: 3.2728\n",
      "Epoch [25280/30000], Loss: 3.2512\n",
      "Epoch [25290/30000], Loss: 3.2297\n",
      "Epoch [25300/30000], Loss: 3.2084\n",
      "Epoch [25310/30000], Loss: 3.1872\n",
      "Epoch [25320/30000], Loss: 3.1661\n",
      "Epoch [25330/30000], Loss: 3.1452\n",
      "Epoch [25340/30000], Loss: 3.1244\n",
      "Epoch [25350/30000], Loss: 3.1037\n",
      "Epoch [25360/30000], Loss: 3.0832\n",
      "Epoch [25370/30000], Loss: 3.0627\n",
      "Epoch [25380/30000], Loss: 3.0424\n",
      "Epoch [25390/30000], Loss: 3.0223\n",
      "Epoch [25400/30000], Loss: 3.0022\n",
      "Epoch [25410/30000], Loss: 2.9823\n",
      "Epoch [25420/30000], Loss: 2.9625\n",
      "Epoch [25430/30000], Loss: 2.9428\n",
      "Epoch [25440/30000], Loss: 2.9233\n",
      "Epoch [25450/30000], Loss: 2.9038\n",
      "Epoch [25460/30000], Loss: 2.8845\n",
      "Epoch [25470/30000], Loss: 2.8653\n",
      "Epoch [25480/30000], Loss: 2.8463\n",
      "Epoch [25490/30000], Loss: 2.8273\n",
      "Epoch [25500/30000], Loss: 2.8085\n",
      "Epoch [25510/30000], Loss: 2.7897\n",
      "Epoch [25520/30000], Loss: 2.7711\n",
      "Epoch [25530/30000], Loss: 2.7526\n",
      "Epoch [25540/30000], Loss: 2.7342\n",
      "Epoch [25550/30000], Loss: 2.7160\n",
      "Epoch [25560/30000], Loss: 2.6978\n",
      "Epoch [25570/30000], Loss: 2.6798\n",
      "Epoch [25580/30000], Loss: 2.6618\n",
      "Epoch [25590/30000], Loss: 2.6440\n",
      "Epoch [25600/30000], Loss: 2.6263\n",
      "Epoch [25610/30000], Loss: 2.6087\n",
      "Epoch [25620/30000], Loss: 2.5912\n",
      "Epoch [25630/30000], Loss: 2.5738\n",
      "Epoch [25640/30000], Loss: 2.5565\n",
      "Epoch [25650/30000], Loss: 2.5393\n",
      "Epoch [25660/30000], Loss: 2.5223\n",
      "Epoch [25670/30000], Loss: 2.5053\n",
      "Epoch [25680/30000], Loss: 2.4884\n",
      "Epoch [25690/30000], Loss: 2.4717\n",
      "Epoch [25700/30000], Loss: 2.4550\n",
      "Epoch [25710/30000], Loss: 2.4385\n",
      "Epoch [25720/30000], Loss: 2.4220\n",
      "Epoch [25730/30000], Loss: 2.4057\n",
      "Epoch [25740/30000], Loss: 2.3894\n",
      "Epoch [25750/30000], Loss: 2.3733\n",
      "Epoch [25760/30000], Loss: 2.3572\n",
      "Epoch [25770/30000], Loss: 2.3413\n",
      "Epoch [25780/30000], Loss: 2.3254\n",
      "Epoch [25790/30000], Loss: 2.3097\n",
      "Epoch [25800/30000], Loss: 2.2941\n",
      "Epoch [25810/30000], Loss: 2.2785\n",
      "Epoch [25820/30000], Loss: 2.2630\n",
      "Epoch [25830/30000], Loss: 2.2476\n",
      "Epoch [25840/30000], Loss: 2.2324\n",
      "Epoch [25850/30000], Loss: 2.2172\n",
      "Epoch [25860/30000], Loss: 2.2021\n",
      "Epoch [25870/30000], Loss: 2.1871\n",
      "Epoch [25880/30000], Loss: 2.1722\n",
      "Epoch [25890/30000], Loss: 2.1573\n",
      "Epoch [25900/30000], Loss: 2.1426\n",
      "Epoch [25910/30000], Loss: 2.1280\n",
      "Epoch [25920/30000], Loss: 2.1134\n",
      "Epoch [25930/30000], Loss: 2.0990\n",
      "Epoch [25940/30000], Loss: 2.0846\n",
      "Epoch [25950/30000], Loss: 2.0704\n",
      "Epoch [25960/30000], Loss: 2.0562\n",
      "Epoch [25970/30000], Loss: 2.0421\n",
      "Epoch [25980/30000], Loss: 2.0280\n",
      "Epoch [25990/30000], Loss: 2.0141\n",
      "Epoch [26000/30000], Loss: 2.0003\n",
      "Epoch [26010/30000], Loss: 1.9865\n",
      "Epoch [26020/30000], Loss: 1.9729\n",
      "Epoch [26030/30000], Loss: 1.9593\n",
      "Epoch [26040/30000], Loss: 1.9458\n",
      "Epoch [26050/30000], Loss: 1.9324\n",
      "Epoch [26060/30000], Loss: 1.9190\n",
      "Epoch [26070/30000], Loss: 1.9058\n",
      "Epoch [26080/30000], Loss: 1.8926\n",
      "Epoch [26090/30000], Loss: 1.8795\n",
      "Epoch [26100/30000], Loss: 1.8665\n",
      "Epoch [26110/30000], Loss: 1.8535\n",
      "Epoch [26120/30000], Loss: 1.8407\n",
      "Epoch [26130/30000], Loss: 1.8279\n",
      "Epoch [26140/30000], Loss: 1.8152\n",
      "Epoch [26150/30000], Loss: 1.8026\n",
      "Epoch [26160/30000], Loss: 1.7901\n",
      "Epoch [26170/30000], Loss: 1.7776\n",
      "Epoch [26180/30000], Loss: 1.7652\n",
      "Epoch [26190/30000], Loss: 1.7529\n",
      "Epoch [26200/30000], Loss: 1.7407\n",
      "Epoch [26210/30000], Loss: 1.7285\n",
      "Epoch [26220/30000], Loss: 1.7165\n",
      "Epoch [26230/30000], Loss: 1.7045\n",
      "Epoch [26240/30000], Loss: 1.6925\n",
      "Epoch [26250/30000], Loss: 1.6807\n",
      "Epoch [26260/30000], Loss: 1.6689\n",
      "Epoch [26270/30000], Loss: 1.6572\n",
      "Epoch [26280/30000], Loss: 1.6456\n",
      "Epoch [26290/30000], Loss: 1.6340\n",
      "Epoch [26300/30000], Loss: 1.6225\n",
      "Epoch [26310/30000], Loss: 1.6111\n",
      "Epoch [26320/30000], Loss: 1.5998\n",
      "Epoch [26330/30000], Loss: 1.5885\n",
      "Epoch [26340/30000], Loss: 1.5773\n",
      "Epoch [26350/30000], Loss: 1.5661\n",
      "Epoch [26360/30000], Loss: 1.5551\n",
      "Epoch [26370/30000], Loss: 1.5441\n",
      "Epoch [26380/30000], Loss: 1.5331\n",
      "Epoch [26390/30000], Loss: 1.5223\n",
      "Epoch [26400/30000], Loss: 1.5115\n",
      "Epoch [26410/30000], Loss: 1.5008\n",
      "Epoch [26420/30000], Loss: 1.4901\n",
      "Epoch [26430/30000], Loss: 1.4795\n",
      "Epoch [26440/30000], Loss: 1.4690\n",
      "Epoch [26450/30000], Loss: 1.4585\n",
      "Epoch [26460/30000], Loss: 1.4481\n",
      "Epoch [26470/30000], Loss: 1.4378\n",
      "Epoch [26480/30000], Loss: 1.4275\n",
      "Epoch [26490/30000], Loss: 1.4173\n",
      "Epoch [26500/30000], Loss: 1.4072\n",
      "Epoch [26510/30000], Loss: 1.3971\n",
      "Epoch [26520/30000], Loss: 1.3871\n",
      "Epoch [26530/30000], Loss: 1.3772\n",
      "Epoch [26540/30000], Loss: 1.3673\n",
      "Epoch [26550/30000], Loss: 1.3575\n",
      "Epoch [26560/30000], Loss: 1.3477\n",
      "Epoch [26570/30000], Loss: 1.3381\n",
      "Epoch [26580/30000], Loss: 1.3284\n",
      "Epoch [26590/30000], Loss: 1.3188\n",
      "Epoch [26600/30000], Loss: 1.3093\n",
      "Epoch [26610/30000], Loss: 1.2999\n",
      "Epoch [26620/30000], Loss: 1.2905\n",
      "Epoch [26630/30000], Loss: 1.2812\n",
      "Epoch [26640/30000], Loss: 1.2719\n",
      "Epoch [26650/30000], Loss: 1.2627\n",
      "Epoch [26660/30000], Loss: 1.2535\n",
      "Epoch [26670/30000], Loss: 1.2444\n",
      "Epoch [26680/30000], Loss: 1.2354\n",
      "Epoch [26690/30000], Loss: 1.2264\n",
      "Epoch [26700/30000], Loss: 1.2175\n",
      "Epoch [26710/30000], Loss: 1.2086\n",
      "Epoch [26720/30000], Loss: 1.1998\n",
      "Epoch [26730/30000], Loss: 1.1911\n",
      "Epoch [26740/30000], Loss: 1.1824\n",
      "Epoch [26750/30000], Loss: 1.1737\n",
      "Epoch [26760/30000], Loss: 1.1651\n",
      "Epoch [26770/30000], Loss: 1.1566\n",
      "Epoch [26780/30000], Loss: 1.1482\n",
      "Epoch [26790/30000], Loss: 1.1397\n",
      "Epoch [26800/30000], Loss: 1.1314\n",
      "Epoch [26810/30000], Loss: 1.1231\n",
      "Epoch [26820/30000], Loss: 1.1148\n",
      "Epoch [26830/30000], Loss: 1.1066\n",
      "Epoch [26840/30000], Loss: 1.0984\n",
      "Epoch [26850/30000], Loss: 1.0904\n",
      "Epoch [26860/30000], Loss: 1.0823\n",
      "Epoch [26870/30000], Loss: 1.0743\n",
      "Epoch [26880/30000], Loss: 1.0664\n",
      "Epoch [26890/30000], Loss: 1.0585\n",
      "Epoch [26900/30000], Loss: 1.0506\n",
      "Epoch [26910/30000], Loss: 1.0429\n",
      "Epoch [26920/30000], Loss: 1.0351\n",
      "Epoch [26930/30000], Loss: 1.0274\n",
      "Epoch [26940/30000], Loss: 1.0198\n",
      "Epoch [26950/30000], Loss: 1.0122\n",
      "Epoch [26960/30000], Loss: 1.0047\n",
      "Epoch [26970/30000], Loss: 0.9972\n",
      "Epoch [26980/30000], Loss: 0.9898\n",
      "Epoch [26990/30000], Loss: 0.9824\n",
      "Epoch [27000/30000], Loss: 0.9750\n",
      "Epoch [27010/30000], Loss: 0.9678\n",
      "Epoch [27020/30000], Loss: 0.9605\n",
      "Epoch [27030/30000], Loss: 0.9533\n",
      "Epoch [27040/30000], Loss: 0.9462\n",
      "Epoch [27050/30000], Loss: 0.9391\n",
      "Epoch [27060/30000], Loss: 0.9320\n",
      "Epoch [27070/30000], Loss: 0.9250\n",
      "Epoch [27080/30000], Loss: 0.9180\n",
      "Epoch [27090/30000], Loss: 0.9111\n",
      "Epoch [27100/30000], Loss: 0.9043\n",
      "Epoch [27110/30000], Loss: 0.8974\n",
      "Epoch [27120/30000], Loss: 0.8907\n",
      "Epoch [27130/30000], Loss: 0.8839\n",
      "Epoch [27140/30000], Loss: 0.8773\n",
      "Epoch [27150/30000], Loss: 0.8706\n",
      "Epoch [27160/30000], Loss: 0.8640\n",
      "Epoch [27170/30000], Loss: 0.8575\n",
      "Epoch [27180/30000], Loss: 0.8509\n",
      "Epoch [27190/30000], Loss: 0.8445\n",
      "Epoch [27200/30000], Loss: 0.8381\n",
      "Epoch [27210/30000], Loss: 0.8317\n",
      "Epoch [27220/30000], Loss: 0.8253\n",
      "Epoch [27230/30000], Loss: 0.8191\n",
      "Epoch [27240/30000], Loss: 0.8128\n",
      "Epoch [27250/30000], Loss: 0.8066\n",
      "Epoch [27260/30000], Loss: 0.8004\n",
      "Epoch [27270/30000], Loss: 0.7943\n",
      "Epoch [27280/30000], Loss: 0.7882\n",
      "Epoch [27290/30000], Loss: 0.7822\n",
      "Epoch [27300/30000], Loss: 0.7762\n",
      "Epoch [27310/30000], Loss: 0.7702\n",
      "Epoch [27320/30000], Loss: 0.7643\n",
      "Epoch [27330/30000], Loss: 0.7584\n",
      "Epoch [27340/30000], Loss: 0.7526\n",
      "Epoch [27350/30000], Loss: 0.7468\n",
      "Epoch [27360/30000], Loss: 0.7410\n",
      "Epoch [27370/30000], Loss: 0.7353\n",
      "Epoch [27380/30000], Loss: 0.7296\n",
      "Epoch [27390/30000], Loss: 0.7240\n",
      "Epoch [27400/30000], Loss: 0.7184\n",
      "Epoch [27410/30000], Loss: 0.7128\n",
      "Epoch [27420/30000], Loss: 0.7073\n",
      "Epoch [27430/30000], Loss: 0.7018\n",
      "Epoch [27440/30000], Loss: 0.6963\n",
      "Epoch [27450/30000], Loss: 0.6909\n",
      "Epoch [27460/30000], Loss: 0.6855\n",
      "Epoch [27470/30000], Loss: 0.6802\n",
      "Epoch [27480/30000], Loss: 0.6749\n",
      "Epoch [27490/30000], Loss: 0.6696\n",
      "Epoch [27500/30000], Loss: 0.6644\n",
      "Epoch [27510/30000], Loss: 0.6592\n",
      "Epoch [27520/30000], Loss: 0.6540\n",
      "Epoch [27530/30000], Loss: 0.6489\n",
      "Epoch [27540/30000], Loss: 0.6438\n",
      "Epoch [27550/30000], Loss: 0.6388\n",
      "Epoch [27560/30000], Loss: 0.6338\n",
      "Epoch [27570/30000], Loss: 0.6288\n",
      "Epoch [27580/30000], Loss: 0.6238\n",
      "Epoch [27590/30000], Loss: 0.6189\n",
      "Epoch [27600/30000], Loss: 0.6141\n",
      "Epoch [27610/30000], Loss: 0.6092\n",
      "Epoch [27620/30000], Loss: 0.6044\n",
      "Epoch [27630/30000], Loss: 0.5997\n",
      "Epoch [27640/30000], Loss: 0.5949\n",
      "Epoch [27650/30000], Loss: 0.5902\n",
      "Epoch [27660/30000], Loss: 0.5855\n",
      "Epoch [27670/30000], Loss: 0.5809\n",
      "Epoch [27680/30000], Loss: 0.5763\n",
      "Epoch [27690/30000], Loss: 0.5717\n",
      "Epoch [27700/30000], Loss: 0.5672\n",
      "Epoch [27710/30000], Loss: 0.5627\n",
      "Epoch [27720/30000], Loss: 0.5582\n",
      "Epoch [27730/30000], Loss: 0.5537\n",
      "Epoch [27740/30000], Loss: 0.5493\n",
      "Epoch [27750/30000], Loss: 0.5450\n",
      "Epoch [27760/30000], Loss: 0.5406\n",
      "Epoch [27770/30000], Loss: 0.5363\n",
      "Epoch [27780/30000], Loss: 0.5320\n",
      "Epoch [27790/30000], Loss: 0.5277\n",
      "Epoch [27800/30000], Loss: 0.5235\n",
      "Epoch [27810/30000], Loss: 0.5193\n",
      "Epoch [27820/30000], Loss: 0.5152\n",
      "Epoch [27830/30000], Loss: 0.5110\n",
      "Epoch [27840/30000], Loss: 0.5069\n",
      "Epoch [27850/30000], Loss: 0.5028\n",
      "Epoch [27860/30000], Loss: 0.4988\n",
      "Epoch [27870/30000], Loss: 0.4948\n",
      "Epoch [27880/30000], Loss: 0.4908\n",
      "Epoch [27890/30000], Loss: 0.4868\n",
      "Epoch [27900/30000], Loss: 0.4829\n",
      "Epoch [27910/30000], Loss: 0.4790\n",
      "Epoch [27920/30000], Loss: 0.4751\n",
      "Epoch [27930/30000], Loss: 0.4713\n",
      "Epoch [27940/30000], Loss: 0.4675\n",
      "Epoch [27950/30000], Loss: 0.4637\n",
      "Epoch [27960/30000], Loss: 0.4599\n",
      "Epoch [27970/30000], Loss: 0.4562\n",
      "Epoch [27980/30000], Loss: 0.4525\n",
      "Epoch [27990/30000], Loss: 0.4488\n",
      "Epoch [28000/30000], Loss: 0.4451\n",
      "Epoch [28010/30000], Loss: 0.4415\n",
      "Epoch [28020/30000], Loss: 0.4379\n",
      "Epoch [28030/30000], Loss: 0.4343\n",
      "Epoch [28040/30000], Loss: 0.4308\n",
      "Epoch [28050/30000], Loss: 0.4273\n",
      "Epoch [28060/30000], Loss: 0.4238\n",
      "Epoch [28070/30000], Loss: 0.4203\n",
      "Epoch [28080/30000], Loss: 0.4169\n",
      "Epoch [28090/30000], Loss: 0.4134\n",
      "Epoch [28100/30000], Loss: 0.4101\n",
      "Epoch [28110/30000], Loss: 0.4067\n",
      "Epoch [28120/30000], Loss: 0.4033\n",
      "Epoch [28130/30000], Loss: 0.4000\n",
      "Epoch [28140/30000], Loss: 0.3967\n",
      "Epoch [28150/30000], Loss: 0.3935\n",
      "Epoch [28160/30000], Loss: 0.3902\n",
      "Epoch [28170/30000], Loss: 0.3870\n",
      "Epoch [28180/30000], Loss: 0.3838\n",
      "Epoch [28190/30000], Loss: 0.3806\n",
      "Epoch [28200/30000], Loss: 0.3775\n",
      "Epoch [28210/30000], Loss: 0.3744\n",
      "Epoch [28220/30000], Loss: 0.3713\n",
      "Epoch [28230/30000], Loss: 0.3682\n",
      "Epoch [28240/30000], Loss: 0.3651\n",
      "Epoch [28250/30000], Loss: 0.3621\n",
      "Epoch [28260/30000], Loss: 0.3591\n",
      "Epoch [28270/30000], Loss: 0.3561\n",
      "Epoch [28280/30000], Loss: 0.3532\n",
      "Epoch [28290/30000], Loss: 0.3502\n",
      "Epoch [28300/30000], Loss: 0.3473\n",
      "Epoch [28310/30000], Loss: 0.3444\n",
      "Epoch [28320/30000], Loss: 0.3416\n",
      "Epoch [28330/30000], Loss: 0.3387\n",
      "Epoch [28340/30000], Loss: 0.3359\n",
      "Epoch [28350/30000], Loss: 0.3331\n",
      "Epoch [28360/30000], Loss: 0.3303\n",
      "Epoch [28370/30000], Loss: 0.3275\n",
      "Epoch [28380/30000], Loss: 0.3248\n",
      "Epoch [28390/30000], Loss: 0.3221\n",
      "Epoch [28400/30000], Loss: 0.3193\n",
      "Epoch [28410/30000], Loss: 0.3167\n",
      "Epoch [28420/30000], Loss: 0.3140\n",
      "Epoch [28430/30000], Loss: 0.3114\n",
      "Epoch [28440/30000], Loss: 0.3087\n",
      "Epoch [28450/30000], Loss: 0.3062\n",
      "Epoch [28460/30000], Loss: 0.3036\n",
      "Epoch [28470/30000], Loss: 0.3010\n",
      "Epoch [28480/30000], Loss: 0.2985\n",
      "Epoch [28490/30000], Loss: 0.2960\n",
      "Epoch [28500/30000], Loss: 0.2935\n",
      "Epoch [28510/30000], Loss: 0.2910\n",
      "Epoch [28520/30000], Loss: 0.2885\n",
      "Epoch [28530/30000], Loss: 0.2861\n",
      "Epoch [28540/30000], Loss: 0.2837\n",
      "Epoch [28550/30000], Loss: 0.2813\n",
      "Epoch [28560/30000], Loss: 0.2789\n",
      "Epoch [28570/30000], Loss: 0.2765\n",
      "Epoch [28580/30000], Loss: 0.2741\n",
      "Epoch [28590/30000], Loss: 0.2718\n",
      "Epoch [28600/30000], Loss: 0.2695\n",
      "Epoch [28610/30000], Loss: 0.2672\n",
      "Epoch [28620/30000], Loss: 0.2649\n",
      "Epoch [28630/30000], Loss: 0.2627\n",
      "Epoch [28640/30000], Loss: 0.2604\n",
      "Epoch [28650/30000], Loss: 0.2582\n",
      "Epoch [28660/30000], Loss: 0.2560\n",
      "Epoch [28670/30000], Loss: 0.2538\n",
      "Epoch [28680/30000], Loss: 0.2517\n",
      "Epoch [28690/30000], Loss: 0.2495\n",
      "Epoch [28700/30000], Loss: 0.2474\n",
      "Epoch [28710/30000], Loss: 0.2453\n",
      "Epoch [28720/30000], Loss: 0.2431\n",
      "Epoch [28730/30000], Loss: 0.2411\n",
      "Epoch [28740/30000], Loss: 0.2390\n",
      "Epoch [28750/30000], Loss: 0.2369\n",
      "Epoch [28760/30000], Loss: 0.2349\n",
      "Epoch [28770/30000], Loss: 0.2329\n",
      "Epoch [28780/30000], Loss: 0.2309\n",
      "Epoch [28790/30000], Loss: 0.2289\n",
      "Epoch [28800/30000], Loss: 0.2269\n",
      "Epoch [28810/30000], Loss: 0.2250\n",
      "Epoch [28820/30000], Loss: 0.2230\n",
      "Epoch [28830/30000], Loss: 0.2211\n",
      "Epoch [28840/30000], Loss: 0.2192\n",
      "Epoch [28850/30000], Loss: 0.2173\n",
      "Epoch [28860/30000], Loss: 0.2154\n",
      "Epoch [28870/30000], Loss: 0.2136\n",
      "Epoch [28880/30000], Loss: 0.2117\n",
      "Epoch [28890/30000], Loss: 0.2099\n",
      "Epoch [28900/30000], Loss: 0.2081\n",
      "Epoch [28910/30000], Loss: 0.2063\n",
      "Epoch [28920/30000], Loss: 0.2045\n",
      "Epoch [28930/30000], Loss: 0.2027\n",
      "Epoch [28940/30000], Loss: 0.2009\n",
      "Epoch [28950/30000], Loss: 0.1992\n",
      "Epoch [28960/30000], Loss: 0.1975\n",
      "Epoch [28970/30000], Loss: 0.1957\n",
      "Epoch [28980/30000], Loss: 0.1940\n",
      "Epoch [28990/30000], Loss: 0.1923\n",
      "Epoch [29000/30000], Loss: 0.1907\n",
      "Epoch [29010/30000], Loss: 0.1890\n",
      "Epoch [29020/30000], Loss: 0.1874\n",
      "Epoch [29030/30000], Loss: 0.1857\n",
      "Epoch [29040/30000], Loss: 0.1841\n",
      "Epoch [29050/30000], Loss: 0.1825\n",
      "Epoch [29060/30000], Loss: 0.1809\n",
      "Epoch [29070/30000], Loss: 0.1793\n",
      "Epoch [29080/30000], Loss: 0.1777\n",
      "Epoch [29090/30000], Loss: 0.1762\n",
      "Epoch [29100/30000], Loss: 0.1746\n",
      "Epoch [29110/30000], Loss: 0.1731\n",
      "Epoch [29120/30000], Loss: 0.1716\n",
      "Epoch [29130/30000], Loss: 0.1701\n",
      "Epoch [29140/30000], Loss: 0.1686\n",
      "Epoch [29150/30000], Loss: 0.1671\n",
      "Epoch [29160/30000], Loss: 0.1656\n",
      "Epoch [29170/30000], Loss: 0.1642\n",
      "Epoch [29180/30000], Loss: 0.1627\n",
      "Epoch [29190/30000], Loss: 0.1613\n",
      "Epoch [29200/30000], Loss: 0.1599\n",
      "Epoch [29210/30000], Loss: 0.1585\n",
      "Epoch [29220/30000], Loss: 0.1571\n",
      "Epoch [29230/30000], Loss: 0.1557\n",
      "Epoch [29240/30000], Loss: 0.1543\n",
      "Epoch [29250/30000], Loss: 0.1529\n",
      "Epoch [29260/30000], Loss: 0.1516\n",
      "Epoch [29270/30000], Loss: 0.1502\n",
      "Epoch [29280/30000], Loss: 0.1489\n",
      "Epoch [29290/30000], Loss: 0.1476\n",
      "Epoch [29300/30000], Loss: 0.1463\n",
      "Epoch [29310/30000], Loss: 0.1450\n",
      "Epoch [29320/30000], Loss: 0.1437\n",
      "Epoch [29330/30000], Loss: 0.1424\n",
      "Epoch [29340/30000], Loss: 0.1412\n",
      "Epoch [29350/30000], Loss: 0.1399\n",
      "Epoch [29360/30000], Loss: 0.1387\n",
      "Epoch [29370/30000], Loss: 0.1374\n",
      "Epoch [29380/30000], Loss: 0.1362\n",
      "Epoch [29390/30000], Loss: 0.1350\n",
      "Epoch [29400/30000], Loss: 0.1338\n",
      "Epoch [29410/30000], Loss: 0.1326\n",
      "Epoch [29420/30000], Loss: 0.1314\n",
      "Epoch [29430/30000], Loss: 0.1302\n",
      "Epoch [29440/30000], Loss: 0.1291\n",
      "Epoch [29450/30000], Loss: 0.1279\n",
      "Epoch [29460/30000], Loss: 0.1268\n",
      "Epoch [29470/30000], Loss: 0.1256\n",
      "Epoch [29480/30000], Loss: 0.1245\n",
      "Epoch [29490/30000], Loss: 0.1234\n",
      "Epoch [29500/30000], Loss: 0.1223\n",
      "Epoch [29510/30000], Loss: 0.1212\n",
      "Epoch [29520/30000], Loss: 0.1201\n",
      "Epoch [29530/30000], Loss: 0.1190\n",
      "Epoch [29540/30000], Loss: 0.1180\n",
      "Epoch [29550/30000], Loss: 0.1169\n",
      "Epoch [29560/30000], Loss: 0.1159\n",
      "Epoch [29570/30000], Loss: 0.1148\n",
      "Epoch [29580/30000], Loss: 0.1138\n",
      "Epoch [29590/30000], Loss: 0.1128\n",
      "Epoch [29600/30000], Loss: 0.1118\n",
      "Epoch [29610/30000], Loss: 0.1108\n",
      "Epoch [29620/30000], Loss: 0.1098\n",
      "Epoch [29630/30000], Loss: 0.1088\n",
      "Epoch [29640/30000], Loss: 0.1078\n",
      "Epoch [29650/30000], Loss: 0.1068\n",
      "Epoch [29660/30000], Loss: 0.1059\n",
      "Epoch [29670/30000], Loss: 0.1049\n",
      "Epoch [29680/30000], Loss: 0.1039\n",
      "Epoch [29690/30000], Loss: 0.1030\n",
      "Epoch [29700/30000], Loss: 0.1021\n",
      "Epoch [29710/30000], Loss: 0.1012\n",
      "Epoch [29720/30000], Loss: 0.1002\n",
      "Epoch [29730/30000], Loss: 0.0993\n",
      "Epoch [29740/30000], Loss: 0.0984\n",
      "Epoch [29750/30000], Loss: 0.0975\n",
      "Epoch [29760/30000], Loss: 0.0967\n",
      "Epoch [29770/30000], Loss: 0.0958\n",
      "Epoch [29780/30000], Loss: 0.0949\n",
      "Epoch [29790/30000], Loss: 0.0941\n",
      "Epoch [29800/30000], Loss: 0.0932\n",
      "Epoch [29810/30000], Loss: 0.0924\n",
      "Epoch [29820/30000], Loss: 0.0915\n",
      "Epoch [29830/30000], Loss: 0.0907\n",
      "Epoch [29840/30000], Loss: 0.0899\n",
      "Epoch [29850/30000], Loss: 0.0890\n",
      "Epoch [29860/30000], Loss: 0.0882\n",
      "Epoch [29870/30000], Loss: 0.0874\n",
      "Epoch [29880/30000], Loss: 0.0866\n",
      "Epoch [29890/30000], Loss: 0.0858\n",
      "Epoch [29900/30000], Loss: 0.0851\n",
      "Epoch [29910/30000], Loss: 0.0843\n",
      "Epoch [29920/30000], Loss: 0.0835\n",
      "Epoch [29930/30000], Loss: 0.0828\n",
      "Epoch [29940/30000], Loss: 0.0820\n",
      "Epoch [29950/30000], Loss: 0.0813\n",
      "Epoch [29960/30000], Loss: 0.0805\n",
      "Epoch [29970/30000], Loss: 0.0798\n",
      "Epoch [29980/30000], Loss: 0.0790\n",
      "Epoch [29990/30000], Loss: 0.0783\n",
      "Epoch [30000/30000], Loss: 0.0776\n"
     ]
    }
   ],
   "source": [
    "lqm(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95051eee-85de-4269-b277-5f0b3cd9df2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([(2048, 2, 1, 32, 5, 1), (4, 2, 1, 4, 5, 2), (16, 6, 1, 16, 7, 2), (2, 12, 2, 2, 7, 3)], 61.41572189331055), ([(8, 2, 1, 8, 5, 2), (128, 6, 3, 2, 7, 4), (2, 12, 3, 2, 5, 3), (4, 2, 1, 4, 3, 3)], 47.95137023925781), ([(128, 2, 4, 1, 7, 2), (1, 12, 3, 1, 7, 1), (8, 6, 3, 8, 5, 2), (64, 12, 4, 1, 3, 4)], 37.088775634765625)]\n",
      "Configanynetx initial: out_planes=2048, inplane=64\n",
      "[(2048, 2, 1, 32, 5, 1), (4, 2, 1, 4, 5, 2), (16, 6, 1, 16, 7, 2), (2, 12, 2, 2, 7, 3)]\n",
      "Epoch 1, Loss: 2.26449989637078\n",
      "Epoch 2, Loss: 2.050007141602319\n"
     ]
    }
   ],
   "source": [
    "#num_cycles = 10\n",
    "target_accuracy = 0\n",
    "data= [] \n",
    "#for _ in range(num_cycles):\n",
    "while target_accuracy < 99.99:\n",
    "    top_k = 3# Choose the top k configurations to train and evaluate\n",
    "    bottom_k= 1\n",
    "    new_configs = [sample_configuration() for _ in range(top_k + bottom_k)] # Sample new configurations, top 3 and 1 bottom to vary\n",
    "    # Evaluate the predicted performance for each new configuration\n",
    "    performances = [(config, predict_performance(model, config)) for config in new_configs]\n",
    "    # Select the top performing configurations based on predicted performance\n",
    "    top_performing_configs = sorted(performances, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    print(top_performing_configs)\n",
    "    lower_ranking_configs = sorted(performances, key=lambda x: x[1], reverse=False)[:bottom_k]\n",
    "    configs_to_evaluate = top_performing_configs + lower_ranking_configs\n",
    "    \n",
    "    # Train and evaluate the top performing configurations using your RegNet-based model\n",
    "    for config, _ in configs_to_evaluate:\n",
    "        model = AnyNetX(config)  # Initialize the RegNet model with the configuration\n",
    "        accuracy = train_and_evaluate(model)  # Train and evaluate the RegNet mode\n",
    "        data.append((config, accuracy))\n",
    "        target_accuracy = accuracy\n",
    "    \n",
    "    X = torch.tensor([config_to_features(config) for config, _ in data], dtype=torch.float32)\n",
    "    y = torch.tensor([[performance] for _, performance in data], dtype=torch.float32)\n",
    "    lqm(model, X, y)  # This function encapsulates retraining logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebd4d4f-636b-4fa2-8554-d06fe0d41336",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d85579f-f659-4215-a358-74d2955d7e53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
